{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                   Version\n",
      "------------------------- --------------\n",
      "aiohappyeyeballs          2.4.4\n",
      "aiohttp                   3.11.9\n",
      "aiosignal                 1.3.1\n",
      "anyio                     4.6.2.post1\n",
      "argon2-cffi               23.1.0\n",
      "argon2-cffi-bindings      21.2.0\n",
      "arrow                     1.3.0\n",
      "asttokens                 2.4.1\n",
      "async-lru                 2.0.4\n",
      "async-timeout             5.0.1\n",
      "attrs                     24.2.0\n",
      "babel                     2.16.0\n",
      "beautifulsoup4            4.12.3\n",
      "bleach                    6.2.0\n",
      "certifi                   2024.8.30\n",
      "cffi                      1.17.1\n",
      "charset-normalizer        3.4.0\n",
      "comm                      0.2.2\n",
      "contourpy                 1.3.1\n",
      "cycler                    0.12.1\n",
      "debugpy                   1.8.9\n",
      "decorator                 5.1.1\n",
      "defusedxml                0.7.1\n",
      "einops                    0.8.0\n",
      "exceptiongroup            1.2.2\n",
      "executing                 2.1.0\n",
      "fastjsonschema            2.21.0\n",
      "filelock                  3.16.1\n",
      "fonttools                 4.55.0\n",
      "fqdn                      1.5.1\n",
      "frozenlist                1.5.0\n",
      "fsspec                    2024.10.0\n",
      "gdown                     5.2.0\n",
      "h11                       0.14.0\n",
      "httpcore                  1.0.7\n",
      "httpx                     0.28.0\n",
      "idna                      3.10\n",
      "ipykernel                 6.29.5\n",
      "ipython                   8.30.0\n",
      "ipywidgets                8.1.5\n",
      "isoduration               20.11.0\n",
      "jedi                      0.19.2\n",
      "Jinja2                    3.1.4\n",
      "joblib                    1.4.2\n",
      "json5                     0.10.0\n",
      "jsonpointer               3.0.0\n",
      "jsonschema                4.23.0\n",
      "jsonschema-specifications 2024.10.1\n",
      "jupyter                   1.1.1\n",
      "jupyter_client            8.6.3\n",
      "jupyter-console           6.6.3\n",
      "jupyter_core              5.7.2\n",
      "jupyter-events            0.10.0\n",
      "jupyter-lsp               2.2.5\n",
      "jupyter_server            2.14.2\n",
      "jupyter_server_terminals  0.5.3\n",
      "jupyterlab                4.2.6\n",
      "jupyterlab_pygments       0.3.0\n",
      "jupyterlab_server         2.27.3\n",
      "jupyterlab_widgets        3.0.13\n",
      "kiwisolver                1.4.7\n",
      "lightning-utilities       0.11.9\n",
      "MarkupSafe                3.0.2\n",
      "matplotlib                3.9.2\n",
      "matplotlib-inline         0.1.7\n",
      "mistune                   3.0.2\n",
      "monai                     1.4.0\n",
      "mpmath                    1.3.0\n",
      "multidict                 6.1.0\n",
      "nbclient                  0.10.1\n",
      "nbconvert                 7.16.4\n",
      "nbformat                  5.10.4\n",
      "nest-asyncio              1.6.0\n",
      "networkx                  3.4.2\n",
      "notebook                  7.2.2\n",
      "notebook_shim             0.2.4\n",
      "numpy                     1.26.4\n",
      "nvidia-cublas-cu12        12.4.5.8\n",
      "nvidia-cuda-cupti-cu12    12.4.127\n",
      "nvidia-cuda-nvrtc-cu12    12.4.127\n",
      "nvidia-cuda-runtime-cu12  12.4.127\n",
      "nvidia-cudnn-cu12         9.1.0.70\n",
      "nvidia-cufft-cu12         11.2.1.3\n",
      "nvidia-curand-cu12        10.3.5.147\n",
      "nvidia-cusolver-cu12      11.6.1.9\n",
      "nvidia-cusparse-cu12      12.3.1.170\n",
      "nvidia-nccl-cu12          2.21.5\n",
      "nvidia-nvjitlink-cu12     12.4.127\n",
      "nvidia-nvtx-cu12          12.4.127\n",
      "overrides                 7.7.0\n",
      "packaging                 24.2\n",
      "pandocfilters             1.5.1\n",
      "parso                     0.8.4\n",
      "pexpect                   4.9.0\n",
      "pillow                    11.0.0\n",
      "pip                       22.0.2\n",
      "platformdirs              4.3.6\n",
      "prometheus_client         0.21.0\n",
      "prompt_toolkit            3.0.48\n",
      "propcache                 0.2.1\n",
      "psutil                    6.1.0\n",
      "ptyprocess                0.7.0\n",
      "pure_eval                 0.2.3\n",
      "pycparser                 2.22\n",
      "Pygments                  2.18.0\n",
      "pyparsing                 3.2.0\n",
      "PySocks                   1.7.1\n",
      "python-dateutil           2.9.0.post0\n",
      "python-json-logger        2.0.7\n",
      "pytorch-lightning         2.4.0\n",
      "PyYAML                    6.0.2\n",
      "pyzmq                     26.2.0\n",
      "referencing               0.35.1\n",
      "requests                  2.32.3\n",
      "rfc3339-validator         0.1.4\n",
      "rfc3986-validator         0.1.1\n",
      "rpds-py                   0.21.0\n",
      "scikit-learn              1.5.2\n",
      "scipy                     1.14.1\n",
      "Send2Trash                1.8.3\n",
      "setuptools                59.6.0\n",
      "six                       1.16.0\n",
      "sniffio                   1.3.1\n",
      "soupsieve                 2.6\n",
      "stack-data                0.6.3\n",
      "sympy                     1.13.1\n",
      "terminado                 0.18.1\n",
      "threadpoolctl             3.5.0\n",
      "tinycss2                  1.4.0\n",
      "tomli                     2.2.1\n",
      "torch                     2.5.1\n",
      "torchaudio                2.5.1\n",
      "torchmetrics              1.6.0\n",
      "torchvision               0.20.1\n",
      "tornado                   6.4.2\n",
      "tqdm                      4.67.1\n",
      "traitlets                 5.14.3\n",
      "triton                    3.1.0\n",
      "types-python-dateutil     2.9.0.20241003\n",
      "typing_extensions         4.12.2\n",
      "uri-template              1.3.0\n",
      "urllib3                   2.2.3\n",
      "wcwidth                   0.2.13\n",
      "webcolors                 24.11.1\n",
      "webencodings              0.5.1\n",
      "websocket-client          1.8.0\n",
      "widgetsnbextension        4.0.13\n",
      "yarl                      1.18.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kicCnhfBzgmD"
   },
   "source": [
    "# DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "kicCnhfBzgmD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All imports work correctly!\n"
     ]
    }
   ],
   "source": [
    "# Test imports\n",
    "import os\n",
    "import json\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from torch.utils.data import DataLoader, random_split, Dataset\n",
    "from scipy import io\n",
    "import matplotlib.image as mpimg\n",
    "from scipy.ndimage import gaussian_filter\n",
    "import torch\n",
    "from matplotlib.path import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from monai.transforms import CropForeground, SpatialCrop\n",
    "import numpy as np\n",
    "import math\n",
    "from itertools import product\n",
    "\n",
    "print(\"All imports work correctly!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 37916,
     "status": "ok",
     "timestamp": 1732788649524,
     "user": {
      "displayName": "AdriÃ¡n De Miguel Palacio",
      "userId": "14927901167627228922"
     },
     "user_tz": -60
    },
    "id": "fVSVmLI3uuYC"
   },
   "outputs": [],
   "source": [
    "# Define a custom 3D dataset for testing purposes\n",
    "class SwinUnetr3DDataset(Dataset):\n",
    "    def __init__(self, is_inference_mode: bool, metadata_dict_with_files_selected: dict, data_dir: str, model_input_dim: tuple, overlap):\n",
    "        \"\"\"\n",
    "        Initializes the class instance with the provided parameters.\n",
    "\n",
    "        Parameters:\n",
    "        metadata_dict_with_files_selected (dict): Dictionary containing the selected files for the dataset with their metadata (e.g., 3D_thermal_sequence_filename, label_filename, ROI, stratified group, etc.).\n",
    "        data_dir (str): Directory where the data is located.\n",
    "        model_input_dim (tuple): 3D tuple indicating the dimensions of the model input (height, width, depth).\n",
    "        \"\"\"\n",
    "\n",
    "        self.metadata_dict_with_files_selected = metadata_dict_with_files_selected\n",
    "        self.data_dir = data_dir\n",
    "\n",
    "        self.is_inference_mode = is_inference_mode # e.g. False\n",
    "\n",
    "        self.model_input_dim = model_input_dim #e.g. (64,64,128)\n",
    "\n",
    "        self.overlap = overlap #e.g. [0.25,0.35]\n",
    "        formatted_overlap_dim_0 = f\"{self.overlap[0]:.2f}\"\n",
    "        formatted_overlap_dim_1 = f\"{self.overlap[0]:.2f}\"\n",
    "\n",
    "        self.patch_size = f\"{self.model_input_dim[0]}x{self.model_input_dim[1]}\" #e.g. 64x64\n",
    "        self.overlap_key = f\"{formatted_overlap_dim_0.replace('.', '_')}x{formatted_overlap_dim_1.replace('.', '_')}\" #e.g. 0_25x0_35\n",
    "\n",
    "        self.preprocessed_dir = os.path.join(os.getcwd(), self.data_dir, \"preprocessed_files\")\n",
    "        self.preprocessed_info_json_path = os.path.join(self.preprocessed_dir, \"preprocessed_info.json\")\n",
    "        self.preprocessed_info_dict = None\n",
    "        \n",
    "        self.preprocessed_patches_dataset = []\n",
    "\n",
    "        # ########################### PREPROCESSING (AND DISK SAVING) ###########################\n",
    "\n",
    "        if os.path.isdir(self.preprocessed_dir):\n",
    "            print(f\"The directory '{self.preprocessed_dir}' exists.\")\n",
    "        else:\n",
    "            os.makedirs(self.preprocessed_dir)\n",
    "            print(f\"The directory '{self.preprocessed_dir}' did not exist and has been created.\")\n",
    "\n",
    "        if os.path.isfile(self.preprocessed_info_json_path):\n",
    "            print(f\"The json file '{self.preprocessed_info_json_path}' exists.\")\n",
    "            self.preprocessed_info_dict = self.load_preprocessed_info()\n",
    "        else:\n",
    "            self.preprocessed_info_dict = {}\n",
    "\n",
    "\n",
    "        for sample_id, sample_metadata in self.metadata_dict_with_files_selected.items():\n",
    "            # Checks\n",
    "            if sample_id in self.preprocessed_info_dict:\n",
    "                print(f\"The label and measurement corresponding to '{sample_id}' sample have already been processed at least once.\")\n",
    "            else:\n",
    "                # Create directory for the sample\n",
    "                preprocessed_sample_dir = os.path.join(self.preprocessed_dir, f\"{sample_id}\")\n",
    "                os.makedirs(preprocessed_sample_dir)\n",
    "                print(f\"The directory '{preprocessed_sample_dir}' did not exist and has been created.\")\n",
    "                preprocessed_sample_label_dir = os.path.join(preprocessed_sample_dir, \"label\")\n",
    "                os.makedirs(preprocessed_sample_label_dir)\n",
    "                print(f\"The directory '{preprocessed_sample_label_dir}' did not exist and has been created.\")\n",
    "                preprocessed_sample_label_patches_dir = os.path.join(preprocessed_sample_dir, \"label\", \"patches\")\n",
    "                os.makedirs(preprocessed_sample_label_patches_dir)\n",
    "                print(f\"The directory '{preprocessed_sample_label_patches_dir}' did not exist and has been created.\")\n",
    "                preprocessed_sample_measurement_dir = os.path.join(preprocessed_sample_dir, \"measurement\")\n",
    "                os.makedirs(preprocessed_sample_measurement_dir)\n",
    "                print(f\"The directory '{preprocessed_sample_measurement_dir}' did not exist and has been created.\")\n",
    "                # Create and save the cropped label in the directory just created\n",
    "                cropped_label_tensor, cropped_bbox_info = self.load_and_crop_label(sample_id, sample_metadata)\n",
    "                cropped_label_tensor_filename = os.path.join(preprocessed_sample_label_dir,f\"{sample_id}_cropped_label.pt\")\n",
    "                torch.save(cropped_label_tensor, cropped_label_tensor_filename)\n",
    "                # Update the information in the preprocessed_info_json\n",
    "                self.update_dictionary(self.preprocessed_info_dict, sample_id, {})\n",
    "                self.update_dictionary(self.preprocessed_info_dict, sample_id, \"measurement\" ,{})\n",
    "                self.update_dictionary(self.preprocessed_info_dict, sample_id, \"label\" ,{})\n",
    "                self.update_dictionary(self.preprocessed_info_dict, sample_id, \"cropped_bbox_info\", cropped_bbox_info)\n",
    "                self.update_dictionary(self.preprocessed_info_dict, sample_id, \"label\" ,\"cropped_label_tensor_filename\", cropped_label_tensor_filename)\n",
    "                self.update_dictionary(self.preprocessed_info_dict, sample_id, \"label\" ,\"patches\", {})\n",
    "\n",
    "            #Check depth directory for measurement\n",
    "            if f\"depth[{self.model_input_dim[2]}]\" in self.preprocessed_info_dict[sample_id][\"measurement\"]:\n",
    "                print(f\"The measurement corresponding to '{sample_id}' sample have already been processed with depth [{self.model_input_dim[2]}] at least once.\")\n",
    "            else:\n",
    "                # Create directory for the depth we will apply inside the measurement directory\n",
    "                measurement_depth_dir = os.path.join(self.preprocessed_dir, f\"{sample_id}\", \"measurement\", f\"depth[{self.model_input_dim[2]}]\")\n",
    "                os.makedirs(measurement_depth_dir)\n",
    "                print(f\"The directory '{measurement_depth_dir}' did not exist and has been created.\")\n",
    "                measurement_depth_patches_dir = os.path.join(measurement_depth_dir, \"patches\")\n",
    "                os.makedirs(measurement_depth_patches_dir)\n",
    "                print(f\"The directory '{measurement_depth_patches_dir}' did not exist and has been created.\")\n",
    "                # Create and save the cropped, standarized & depth compressed 3D measurement in the directory just created\n",
    "                cropped_standarized_depth_compressed_measurement_tensor = self.load_crop_standardize_depth_compress_measurement(sample_id, sample_metadata)\n",
    "                cropped_standarized_depth_compressed_measurement_tensor_filename = os.path.join(measurement_depth_dir, f\"{sample_id}_cropped_standarized_depth[{self.model_input_dim[2]}].pt\")\n",
    "                torch.save(cropped_standarized_depth_compressed_measurement_tensor, cropped_standarized_depth_compressed_measurement_tensor_filename)\n",
    "                # Update the information in the preprocessed_info_json\n",
    "                self.update_dictionary(self.preprocessed_info_dict, sample_id, \"measurement\", f\"depth[{self.model_input_dim[2]}]\", {})\n",
    "                self.update_dictionary(self.preprocessed_info_dict, sample_id, \"measurement\", f\"depth[{self.model_input_dim[2]}]\", \"cropped_standarized_depth_compressed_measurement_tensor_filename\", cropped_standarized_depth_compressed_measurement_tensor_filename)\n",
    "                self.update_dictionary(self.preprocessed_info_dict, sample_id, \"measurement\", f\"depth[{self.model_input_dim[2]}]\", \"patches\", {})\n",
    "            \n",
    "            #Check patches_size directory for label\n",
    "            if  f\"patch_size[{self.patch_size}]\" in self.preprocessed_info_dict[sample_id][\"label\"][\"patches\"]:\n",
    "                print(f\"The label corresponding to '{sample_id}' sample have already been processed with patch size [{self.patch_size}] at least once.\")\n",
    "            else:\n",
    "                label_patches_size_dir = os.path.join(self.preprocessed_dir, f\"{sample_id}\", \"label\", \"patches\", f\"patch_size[{self.patch_size}]\")\n",
    "                os.makedirs(label_patches_size_dir)\n",
    "                print(f\"The directory '{label_patches_size_dir}' did not exist and has been created.\")\n",
    "                self.update_dictionary(self.preprocessed_info_dict, sample_id, \"label\", \"patches\", f\"patch_size[{self.patch_size}]\", {})\n",
    "\n",
    "            #Check patches_size directory for measurement\n",
    "            if  f\"patch_size[{self.patch_size}]\" in self.preprocessed_info_dict[sample_id][\"measurement\"][f\"depth[{self.model_input_dim[2]}]\"][\"patches\"]:\n",
    "                print(f\"The measurement corresponding to '{sample_id}' sample have already been processed with depth [{self.model_input_dim[2]}] and patch size [{self.patch_size}] at least once.\")\n",
    "            else:\n",
    "                measurement_depth_patches_size_dir = os.path.join(self.preprocessed_dir, f\"{sample_id}\", \"measurement\", f\"depth[{self.model_input_dim[2]}]\", \"patches\", f\"patch_size[{self.patch_size}]\")\n",
    "                os.makedirs(measurement_depth_patches_size_dir)\n",
    "                print(f\"The directory '{measurement_depth_patches_size_dir}' did not exist and has been created.\")\n",
    "                self.update_dictionary(self.preprocessed_info_dict, sample_id, \"measurement\", f\"depth[{self.model_input_dim[2]}]\", \"patches\", f\"patch_size[{self.patch_size}]\", {})\n",
    "\n",
    "            \n",
    "            #LABEL_PATCHES\n",
    "            if f\"overlap[{self.overlap_key}]\" in self.preprocessed_info_dict[sample_id][\"label\"][\"patches\"][f\"patch_size[{self.patch_size}]\"]:\n",
    "                print(f\"The label corresponding to '{sample_id}' sample have already been processed with patch size [{self.patch_size}] and overlap [{self.overlap_key}]\")\n",
    "            else:\n",
    "                label_overlap_dir = os.path.join(self.preprocessed_dir, f\"{sample_id}\", \"label\", \"patches\", f\"patch_size[{self.patch_size}]\", f\"overlap[{self.overlap_key}]\")\n",
    "                os.makedirs(label_overlap_dir)\n",
    "                print(f\"The directory '{label_overlap_dir}' did not exist and has been created.\")\n",
    "                cropped_bbox_info = self.preprocessed_info_dict[sample_id][\"cropped_bbox_info\"]\n",
    "                cropped_label_tensor_filename = self.preprocessed_info_dict[sample_id][\"label\"][\"cropped_label_tensor_filename\"]\n",
    "                cropped_label_tensor = torch.load(cropped_label_tensor_filename, weights_only=True)\n",
    "                label_patches = self.create_patches(cropped_label_tensor, sample_metadata[\"ROI\"], cropped_bbox_info)\n",
    "\n",
    "                label_patches_dictionary = {}\n",
    "                # Save patches\n",
    "                for i, label_patch in enumerate(label_patches):\n",
    "                    label_tensor_patch, label_tensor_patch_coord, roi_adjusted_patch = label_patch\n",
    "                    preprocessed_label_patch_filename = os.path.join(f\"label_{sample_id}_patch_size[{self.patch_size}]_overlap[{self.overlap_key}]_coords[{label_tensor_patch_coord[0]}x{label_tensor_patch_coord[1]}].pt\")\n",
    "                    patch_patch= os.path.join(label_overlap_dir, preprocessed_label_patch_filename)\n",
    "                    torch.save(label_tensor_patch, patch_patch)\n",
    "                    label_patches_dictionary[\"_\".join(map(str, label_tensor_patch_coord))] = {\n",
    "                        \"patch_coord\": label_tensor_patch_coord,\n",
    "                        \"patch_path\": patch_patch,\n",
    "                        \"roi_adjusted_patch\": roi_adjusted_patch\n",
    "                    }\n",
    "                self.update_dictionary(self.preprocessed_info_dict, sample_id, \"label\", \"patches\", f\"patch_size[{self.patch_size}]\", f\"overlap[{self.overlap_key}]\", label_patches_dictionary)\n",
    "\n",
    "            #MEASUREMENT_PATCHES\n",
    "            if f\"overlap[{self.overlap_key}]\" in self.preprocessed_info_dict[sample_id][\"measurement\"][f\"depth[{self.model_input_dim[2]}]\"][\"patches\"][f\"patch_size[{self.patch_size}]\"]:\n",
    "                print(f\"The measurement corresponding to '{sample_id}' sample have already been processed with depth [{self.model_input_dim[2]}], patch size [{self.patch_size}] and overlap [{self.overlap_key}]\")\n",
    "            else:\n",
    "                measurement_overlap_dir = os.path.join(self.preprocessed_dir, f\"{sample_id}\", \"measurement\", f\"depth[{self.model_input_dim[2]}]\",\"patches\", f\"patch_size[{self.patch_size}]\", f\"overlap[{self.overlap_key}]\")\n",
    "                os.makedirs(measurement_overlap_dir)\n",
    "                print(f\"The directory '{measurement_overlap_dir}' did not exist and has been created.\")\n",
    "                cropped_bbox_info = self.preprocessed_info_dict[sample_id][\"cropped_bbox_info\"]\n",
    "                cropped_standarized_depth_compressed_measurement_tensor_filename = self.preprocessed_info_dict[sample_id][\"measurement\"][f\"depth[{self.model_input_dim[2]}]\"][\"cropped_standarized_depth_compressed_measurement_tensor_filename\"]\n",
    "                cropped_standarized_depth_compressed_measurement_tensor = torch.load(cropped_standarized_depth_compressed_measurement_tensor_filename, weights_only=True)\n",
    "                measurement_patches = self.create_patches(cropped_standarized_depth_compressed_measurement_tensor, sample_metadata[\"ROI\"], cropped_bbox_info)\n",
    "\n",
    "                measurement_patches_dictionary = {}\n",
    "                # Save patches\n",
    "                for i, measurement_patch in enumerate(measurement_patches):\n",
    "                    measurement_tensor_patch, measurement_tensor_patch_coord, roi_adjusted_patch = measurement_patch\n",
    "                    preprocessed_measurement_patch_filename = os.path.join(f\"{sample_id}_depth[{self.model_input_dim[2]}]_patch_size[{self.patch_size}]_overlap[{self.overlap_key}]_coords[{measurement_tensor_patch_coord[0]}x{measurement_tensor_patch_coord[1]}].pt\")\n",
    "                    patch_patch = os.path.join(measurement_overlap_dir, preprocessed_measurement_patch_filename)\n",
    "                    torch.save(measurement_tensor_patch, patch_patch)\n",
    "                    measurement_patches_dictionary[\"_\".join(map(str, measurement_tensor_patch_coord))] = {\n",
    "                        \"patch_coord\": measurement_tensor_patch_coord,\n",
    "                        \"patch_path\": patch_patch,\n",
    "                        \"roi_adjusted_patch\": roi_adjusted_patch\n",
    "                    }\n",
    "                self.update_dictionary(self.preprocessed_info_dict, sample_id, \"measurement\", f\"depth[{self.model_input_dim[2]}]\", \"patches\", f\"patch_size[{self.patch_size}]\", f\"overlap[{self.overlap_key}]\", measurement_patches_dictionary)\n",
    "\n",
    "            label_patches_dict = self.preprocessed_info_dict[sample_id][\"label\"][\"patches\"][f\"patch_size[{self.patch_size}]\"][f\"overlap[{self.overlap_key}]\"]\n",
    "            measurement_patches_dict = self.preprocessed_info_dict[sample_id][\"measurement\"][f\"depth[{self.model_input_dim[2]}]\"][\"patches\"][f\"patch_size[{self.patch_size}]\"][f\"overlap[{self.overlap_key}]\"]\n",
    "            for (key1, value1), (key2, value2) in zip(label_patches_dict.items(), measurement_patches_dict.items()):\n",
    "                self.preprocessed_patches_dataset.append(\n",
    "                    {\n",
    "                        \"sample_id\": sample_id,\n",
    "                        \"coord\": key1, # Key 1 and Key 2 are equal\n",
    "                        \"patch_label_info\": value1,\n",
    "                        \"patch_measurement_info\": value2\n",
    "                    }\n",
    "                )\n",
    "\n",
    "        self.update_preprocessed_info() #Updates \"preprocessed_info.json\"\n",
    "\n",
    "\n",
    "    def load_preprocessed_info(self):\n",
    "        # If it exists, attempt to load existing data from the file\n",
    "        with open(self.preprocessed_info_json_path, 'r') as file:\n",
    "            try:\n",
    "                preprocessed_info_dict = json.load(file)  # Load JSON data into a dictionary\n",
    "            except json.JSONDecodeError:\n",
    "                raise ValueError(\n",
    "                    f\"The JSON file '{self.preprocessed_info_json_path}' is corrupted or not formatted correctly.\"\n",
    "                )\n",
    "        return preprocessed_info_dict\n",
    "        \n",
    "\n",
    "    def update_preprocessed_info(self):\n",
    "        ## Rewrite JSON in disk\n",
    "        with open(self.preprocessed_info_json_path, 'w') as file:\n",
    "            json.dump(self.preprocessed_info_dict, file, indent=4)  # The indent parameter makes it human-readable\n",
    "        \n",
    "        print(f\"The JSON file '{self.preprocessed_info_json_path}' has been updated.\")\n",
    "        \n",
    "\n",
    "    def update_dictionary(self, dictionary, *args):\n",
    "        # If it's just a key-value pair\n",
    "        if len(args) == 2:\n",
    "            if args[0] not in dictionary:\n",
    "                dictionary[args[0]] = args[1]\n",
    "            else:\n",
    "                dictionary[args[0]].update(args[1])\n",
    "        else:\n",
    "            current_level = dictionary\n",
    "            # Iterate through all but the last argument to handle nested keys\n",
    "            for key in args[:-2]:\n",
    "                current_level = current_level[key]  # Move deeper into the nested dictionary\n",
    "    \n",
    "            # The last two arguments are the final key-value pair to update\n",
    "            final_key, final_value = args[-2], args[-1]\n",
    "    \n",
    "            if final_key not in current_level:\n",
    "                current_level[final_key] = final_value\n",
    "            else:\n",
    "                current_level[final_key].update(final_value)\n",
    "                \n",
    "\n",
    "    def create_patches(self, cropped_tensor, ROI_coordinates, cropped_bbox_info): \n",
    "\n",
    "        for i in range(2):  # Check for i=0 (height) and i=1 (width)\n",
    "            cropped_dim = cropped_tensor.shape[i + 1]  # i+1 due to channel in dim 0\n",
    "            required_dim = self.model_input_dim[i]\n",
    "\n",
    "            if cropped_dim - required_dim == 0:\n",
    "                continue  # Dimension matches exactly; no issue\n",
    "            elif cropped_dim - required_dim > 0:\n",
    "                continue  # Cropped dimension is larger than required\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    f\"The cropped ROI area is too small for the predefined patch size. \"\n",
    "                    f\"Dimension {i} mismatch: cropped ROI area dimension {i} = {cropped_dim} \"\n",
    "                    f\"vs. patch dimension {i} = {required_dim}.\"\n",
    "                )\n",
    "\n",
    "        slices_per_dim = {}\n",
    "\n",
    "        for i in range(2):  # Check for i=0 (height) and i=1 (width)\n",
    "            stride = int(self.model_input_dim[i] * (1-self.overlap[i]))\n",
    "            print(f\"Dim {i} - Stride: {stride}\")\n",
    "            print(f\"Dim {i} - Model Input Dim: {self.model_input_dim[i]}\")\n",
    "            print(f\"Dim {i} - Cropped Tensor Shape: {cropped_tensor.shape[i + 1]}\")\n",
    "            slices = []\n",
    "            for j in range(0, cropped_tensor.shape[i + 1], stride):\n",
    "                slice_start = j\n",
    "                slice_end = j + self.model_input_dim[i]\n",
    "                if slice_end <= cropped_tensor.shape[i + 1]:\n",
    "                    slice_to_add = slice(slice_start, slice_end)\n",
    "                    print(f\"Dim {i} - Slice: {slice_to_add}\")\n",
    "                    slices.append(slice_to_add)\n",
    "                else:\n",
    "                    out_of_bounds = slice_end - cropped_tensor.shape[i + 1]\n",
    "                    slice_to_add = slice(slice_start - out_of_bounds, slice_end - out_of_bounds)\n",
    "                    if not slice_to_add in slices:\n",
    "                      print(f\"Dim {i} - Slice: {slice_to_add}\")\n",
    "                      slices.append(slice_to_add)\n",
    "                    else:\n",
    "                      break\n",
    "            slices_per_dim[i] = slices\n",
    "\n",
    "        # Get the lists of slices\n",
    "        slices_height_dim = slices_per_dim[0]\n",
    "        slices_width_dim = slices_per_dim[1]\n",
    "\n",
    "        # Generate all combinations (Cartesian product) of slices between both dimensions\n",
    "        patches_slices = list(product(slices_height_dim, slices_width_dim))\n",
    "\n",
    "        # List to store the resulting patches\n",
    "        patches_list = []\n",
    "\n",
    "        # Iterate over all the patches\n",
    "        for patch_slices in patches_slices:\n",
    "            patch_slice_height_dim = patch_slices[0]\n",
    "            patch_slice_width_dim = patch_slices[1]\n",
    "\n",
    "            roi_start = (patch_slice_height_dim.start, patch_slice_width_dim.start)\n",
    "\n",
    "            if len(cropped_tensor.shape) == 3: # Label tensor\n",
    "                patch_tensor = cropped_tensor[:, patch_slice_height_dim, patch_slice_width_dim]\n",
    "            else: # Measurement tensor\n",
    "                patch_tensor = cropped_tensor[:, patch_slice_height_dim, patch_slice_width_dim, :]\n",
    "\n",
    "            roi_coords_in_cropped_tensor = {\n",
    "                'all_points_x': [x - cropped_bbox_info[\"x_coord\"] for x in ROI_coordinates['all_points_x']],\n",
    "                'all_points_y': [y - cropped_bbox_info[\"y_coord\"] for y in ROI_coordinates['all_points_y']]\n",
    "            }\n",
    "\n",
    "            roi_adjusted_patch = {\n",
    "                'all_points_x': [x - roi_start[1] for x in roi_coords_in_cropped_tensor['all_points_x']],\n",
    "                'all_points_y': [y - roi_start[0] for y in roi_coords_in_cropped_tensor['all_points_y']]\n",
    "            }\n",
    "            # Append the cropped tensor and adjusted ROI to the list\n",
    "            patches_list.append((patch_tensor, (roi_start[0], roi_start[1]), roi_adjusted_patch))\n",
    "\n",
    "        if len(cropped_tensor.shape) == 3: # Label tensor\n",
    "                # Plotting Label Tensor Patches\n",
    "                self.plot_patches(\n",
    "                    patches_list,\n",
    "                    tensor_type='Label',\n",
    "                    channel=1,  # Specify the label channel (e.g., foreground)\n",
    "                    cmap='RdBu',\n",
    "                    number_slices_height_dim=len(slices_height_dim),\n",
    "                    number_slices_width_dim=len(slices_width_dim)\n",
    "                )\n",
    "        else: # Measurement tensor\n",
    "                # Plotting Measurement Tensor Patches\n",
    "                self.plot_patches(\n",
    "                    patches_list,\n",
    "                    tensor_type='Measurement',\n",
    "                    depth_frame=15,  # Specify the frame index\n",
    "                    cmap='RdBu',\n",
    "                    number_slices_height_dim=len(slices_height_dim),\n",
    "                    number_slices_width_dim=len(slices_width_dim)\n",
    "                )\n",
    "\n",
    "\n",
    "        return patches_list\n",
    "\n",
    "    def load_and_crop_label(self, measurement_id, measurement_data):\n",
    "        measurement_label_filename = measurement_data[\"label_filename\"]\n",
    "        measurement_ROI = measurement_data[\"ROI\"]\n",
    "\n",
    "        # ############# LABEL LOADING ###################\n",
    "\n",
    "        # Load label data\n",
    "        label_img_ndarray = mpimg.imread(os.path.join(os.getcwd(), self.data_dir, \"labels\", measurement_label_filename))\n",
    "        if len(label_img_ndarray.shape) == 3:\n",
    "            label_img_ndarray = label_img_ndarray[..., 0] # Shape (256, 320)\n",
    "\n",
    "        # ############# LABEL CONVERSION TO MULTI-CHANNEL (IT IS ADAPTED TO THE MODEL) ###################\n",
    "\n",
    "        # Convert label to multi-channel for the model\n",
    "        label_one_hot_encoded, label_mapping = self.one_hot_encode(label_img_ndarray)\n",
    "        label_tensor = label_one_hot_encoded # Shape (2, 256, 320)\n",
    "\n",
    "        self.plot_tensor_and_polygon(label_tensor, measurement_ROI, \"Label Tensor\")\n",
    "\n",
    "        # ############# CROPPING LABEL USING MANUALLY DEFINED ROI ###################\n",
    "\n",
    "        print(f\"(Before cropping) label_tensor.shape: {label_tensor.shape}\")\n",
    "\n",
    "        # 1. Extract bounding box coordinates from ROI polygon\n",
    "        x_coords = np.array(measurement_ROI['all_points_x'])\n",
    "        y_coords = np.array(measurement_ROI['all_points_y'])\n",
    "\n",
    "        x_min, x_max = int(x_coords.min()), int(x_coords.max())\n",
    "        y_min, y_max = int(y_coords.min()), int(y_coords.max())\n",
    "\n",
    "        # 2. Crop the tensor along height and width using the bounding box\n",
    "        # Slicing along height (y-axis) and width (x-axis)\n",
    "        cropped_label_tensor = label_tensor[:, y_min:y_max, x_min:x_max]\n",
    "\n",
    "        cropped_bbox_info = {\n",
    "            'y_coord': y_min,\n",
    "            'x_coord': x_min,\n",
    "            'height': y_max-y_min,\n",
    "            'width': x_max-x_min,\n",
    "        }\n",
    "\n",
    "        ################### LOGGING #############################\n",
    "\n",
    "        print(f\"(After cropping) cropped_label_tensor.shape: {cropped_label_tensor.shape}\")\n",
    "\n",
    "        roi_coords_in_cropped_tensor = {\n",
    "            'all_points_x': [x - x_min for x in measurement_ROI['all_points_x']],\n",
    "            'all_points_y': [y - y_min for y in measurement_ROI['all_points_y']]\n",
    "        }\n",
    "\n",
    "        self.plot_tensor_and_polygon(cropped_label_tensor, roi_coords_in_cropped_tensor,\"(Cropped) Label Tensor\")\n",
    "\n",
    "        return cropped_label_tensor, cropped_bbox_info\n",
    "\n",
    "    def load_crop_standardize_depth_compress_measurement(self, measurement_id, measurement_data):\n",
    "        measurement_3D_thermal_sequence_filename = measurement_data[\"3D_thermal_sequence_filename\"]\n",
    "        measurement_ROI = measurement_data[\"ROI\"]\n",
    "\n",
    "        # ############# 3D SEQUENCE DATA LOADING ###################\n",
    "\n",
    "        # Load 3D sequence data\n",
    "        mat_data = io.loadmat(os.path.join(os.getcwd(), self.data_dir, \"data\", measurement_3D_thermal_sequence_filename))\n",
    "        measurement_3D_thermal_sequence = np.float32(mat_data[\"imageArray\"])  # Shape (256, 320, 1810)\n",
    "        measurement_tensor = torch.tensor(measurement_3D_thermal_sequence).unsqueeze(0)  # Shape (1, 256, 320, 1810)\n",
    "\n",
    "        self.plot_tensor_and_polygon(measurement_tensor[:,:,:,100], measurement_ROI, \"Measurement Tensor (Frame 100)\")\n",
    "\n",
    "        # ############# CROPPING MANUALLY DEFINED ROI ###################\n",
    "\n",
    "        print(f\"(Before cropping) measurement_tensor.shape: {measurement_tensor.shape}\")\n",
    "\n",
    "        # 1. Extract bounding box coordinates from ROI polygon\n",
    "        x_coords = np.array(measurement_ROI['all_points_x'])\n",
    "        y_coords = np.array(measurement_ROI['all_points_y'])\n",
    "\n",
    "        x_min, x_max = int(x_coords.min()), int(x_coords.max())\n",
    "        y_min, y_max = int(y_coords.min()), int(y_coords.max())\n",
    "\n",
    "        # 2. Crop the tensor along height and width using the bounding box\n",
    "        # Slicing along height (y-axis) and width (x-axis)\n",
    "        cropped_measurement_tensor = measurement_tensor[:, y_min:y_max, x_min:x_max, :]\n",
    "\n",
    "        ################### LOGGING #############################\n",
    "\n",
    "        print(f\"(After cropping) cropped_measurement_tensor.shape: {cropped_measurement_tensor.shape}\")\n",
    "\n",
    "        roi_coords_in_cropped_tensor = {\n",
    "            'all_points_x': [x - x_min for x in measurement_ROI['all_points_x']],\n",
    "            'all_points_y': [y - y_min for y in measurement_ROI['all_points_y']]\n",
    "        }\n",
    "\n",
    "        self.plot_tensor_and_polygon(cropped_measurement_tensor[:,:,:,100], roi_coords_in_cropped_tensor, \"(Cropped) Measurement Tensor\\n(Frame 100)\")\n",
    "\n",
    "        # ############# NORMALIZE 3D SEQUENCE (3D STANDARDIZATION) ###################\n",
    "\n",
    "        # Standardize the volume channel-wise\n",
    "        # Mean and std are calculated along the spatial and depth dimensions (H, W, D)\n",
    "        mean = cropped_measurement_tensor.mean(dim=(1, 2, 3), keepdim=True)  # Keep dimensions for broadcasting\n",
    "        std = cropped_measurement_tensor.std(dim=(1, 2, 3), keepdim=True)\n",
    "\n",
    "        # Standardize: (value - mean) / std\n",
    "        cropped_normalized_measurement_tensor = (cropped_measurement_tensor - mean) / std\n",
    "\n",
    "        print(\"(Before normalization/standardization) cropped_measurement_tensor.shape:\", cropped_measurement_tensor.shape)\n",
    "        print(\"(After normalization/standardization) cropped_normalized_measurement_tensor.shape:\", cropped_normalized_measurement_tensor.shape)\n",
    "\n",
    "        self.plot_tensor_and_polygon(cropped_normalized_measurement_tensor[:,:,:,100], roi_coords_in_cropped_tensor, \"(Cropped) 3D_Standardized Measurement Tensor\\n(Frame 100)\")\n",
    "\n",
    "        # ############# TEMPORAL COMPRESSION ###################\n",
    "\n",
    "        model_input_depth_dim = self.model_input_dim[2]\n",
    "\n",
    "        cropped_normalized_compressed_measurement_tensor = self.compress_depth_tensor(cropped_normalized_measurement_tensor, model_input_depth_dim)\n",
    "\n",
    "        print(f\"(Before compression) cropped_normalized_measurement_tensor.shape: {cropped_normalized_measurement_tensor.shape}\")\n",
    "        print(f\"(After compression) cropped_normalized_compressed_measurement_tensor.shape: {cropped_normalized_compressed_measurement_tensor.shape}\")\n",
    "\n",
    "        self.plot_tensor_and_polygon(cropped_normalized_compressed_measurement_tensor[:,:,:,15], roi_coords_in_cropped_tensor, f\"(Cropped) 3D_Standardized/Compressed_{model_input_depth_dim} Measurement Tensor\\n(Frame 15 After Compression)\")\n",
    "\n",
    "        return cropped_normalized_compressed_measurement_tensor\n",
    "\n",
    "    def plot_patches(self, patches, tensor_type, number_slices_height_dim, number_slices_width_dim, channel=None, depth_frame=None, cmap='RdBu'):\n",
    "        \"\"\"\n",
    "        Plots the patches of measurement or label tensors with adjusted ROI polygons.\n",
    "\n",
    "        Args:\n",
    "            patches (list): List of tuples [(patch_tensor, patch_coord, adjusted_roi)].\n",
    "            tensor_type (str): Type of tensor ('Measurement' or 'Label').\n",
    "            channel (int, optional): Channel index to visualize (for label tensors).\n",
    "            depth_frame (int, optional): Frame index to visualize (for measurement tensors).\n",
    "            cmap (str): Colormap for visualization.\n",
    "        \"\"\"\n",
    "        patches_per_row = number_slices_width_dim\n",
    "        n_rows = number_slices_height_dim\n",
    "        fig, axes = plt.subplots(n_rows, patches_per_row, figsize=(patches_per_row * 5, n_rows * 5))\n",
    "        axes = axes.flatten()  # Flatten axes for easy indexing\n",
    "\n",
    "        for idx, (patch_tensor, patch_coord, adjusted_roi) in enumerate(patches):\n",
    "            ax = axes[idx]\n",
    "\n",
    "            # Determine what to plot based on tensor type\n",
    "            if tensor_type == 'Measurement' and depth_frame is not None:\n",
    "                data = patch_tensor[0, :, :, depth_frame].cpu().numpy()  # Frame-specific\n",
    "                # Plot the patch tensor\n",
    "                im = ax.imshow(data, cmap=cmap)\n",
    "            elif tensor_type == 'Label' and channel is not None:\n",
    "                data = patch_tensor[channel, :, :].cpu().numpy()  # Channel-specific\n",
    "                # Plot the patch tensor\n",
    "                im = ax.imshow(data, cmap=cmap, vmin=0, vmax=1)\n",
    "            else:\n",
    "                raise ValueError(\"Specify 'depth_frame' for Measurement or 'channel' for Label tensors.\")\n",
    "\n",
    "            \n",
    "            ax.set_title(f'{tensor_type} Patch Coord: ({patch_coord[0]}, {patch_coord[1]})')\n",
    "            ax.axis('off')\n",
    "            fig.colorbar(im, ax=ax, label='Pixel Value')\n",
    "\n",
    "            # Plot the adjusted polygon overlay\n",
    "            all_points_x = adjusted_roi['all_points_x']\n",
    "            all_points_y = adjusted_roi['all_points_y']\n",
    "            ax.plot(all_points_x + [all_points_x[0]], all_points_y + [all_points_y[0]], 'r-', linewidth=2)  # Close the polygon\n",
    "            ax.scatter(all_points_x, all_points_y, color='blue', zorder=5)  # Mark the vertices\n",
    "\n",
    "        # Hide unused axes\n",
    "        for idx in range(len(patches), len(axes)):\n",
    "            axes[idx].axis('off')\n",
    "\n",
    "        plt.tight_layout()  # Leave space for the title\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    def plot_tensor_and_polygon(self,tensor, roi, name_plot_tensor):\n",
    "        \"\"\"\n",
    "        Plot the mask and ROI polygon together for visualization.\n",
    "\n",
    "        Parameters:\n",
    "        - mask (torch.Tensor): The mask to be plotted.\n",
    "        - roi (dict): Dictionary with keys 'all_points_x' and 'all_points_y' representing the ROI polygon.\n",
    "        \"\"\"\n",
    "        if tensor.shape[0] == 1:\n",
    "            # Plotting the mask\n",
    "            fig, ax = plt.subplots(figsize=(6, 6))\n",
    "            im = ax.imshow(tensor[0].cpu().numpy(), cmap='RdBu')\n",
    "            plt.colorbar(im, ax=ax, label='Pixel Value')\n",
    "\n",
    "            # Correct the y-coordinates to match the image grid\n",
    "            all_points_x = roi['all_points_x']\n",
    "            all_points_y = roi['all_points_y']\n",
    "            plt.plot(all_points_x + [all_points_x[0]], all_points_y + [all_points_y[0]], 'r-', linewidth=2)  # Close the polygon\n",
    "            plt.scatter(all_points_x, all_points_y, color='blue', zorder=5)  # Mark the vertices\n",
    "            plt.suptitle(f'{name_plot_tensor} with ROI Polygon Overlay')\n",
    "            plt.show()\n",
    "        else:\n",
    "            fig, axes = plt.subplots(1, tensor.shape[0], figsize=(12, 6))\n",
    "            for i in range(tensor.shape[0]):\n",
    "                im = axes[i].imshow(tensor[i].cpu().numpy(), cmap='RdBu')\n",
    "                axes[i].set_title(f'Channel {i}')\n",
    "                fig.colorbar(im, ax=axes[i], label='Pixel Value')\n",
    "\n",
    "                # Add polygon overlay\n",
    "                all_points_x = roi['all_points_x']\n",
    "                all_points_y = roi['all_points_y']\n",
    "                axes[i].plot(all_points_x + [all_points_x[0]], all_points_y + [all_points_y[0]], 'r-', linewidth=2)  # Close the polygon\n",
    "                axes[i].scatter(all_points_x, all_points_y, color='blue', zorder=5)\n",
    "\n",
    "            plt.suptitle(f'{name_plot_tensor} with ROI Polygon Overlay')\n",
    "            plt.show()\n",
    "\n",
    "    def compress_depth_tensor(self, tensor, model_input_dim_depth):\n",
    "        num_channels, height, width, depth = tensor.shape\n",
    "\n",
    "        # Raise an exception if depth is smaller than model_input_dim_depth\n",
    "        if depth < model_input_dim_depth:\n",
    "            raise ValueError(f\"The depth of the input tensor ({depth}) must be greater than or equal to model_input_dim_depth ({model_input_dim_depth}).\")\n",
    "\n",
    "        block_size = depth // model_input_dim_depth\n",
    "        remainder = depth % model_input_dim_depth\n",
    "\n",
    "        print(f\"Block size: {block_size}\")\n",
    "        print(f\"Remainder: {remainder}\")\n",
    "\n",
    "        # Convert to NumPy\n",
    "        numpy_array = tensor.numpy()\n",
    "\n",
    "        # Apply Gaussian filter only along the last dimension (depth)\n",
    "        # sigma = block_size / 2 beacuse is the number of neighbours we look right and left\n",
    "        smoothed_numpy_array = gaussian_filter(numpy_array, sigma=(0, 0, 0, block_size / 2))\n",
    "\n",
    "        # Convert back to PyTorch tensor\n",
    "        smoothed_tensor = torch.tensor(smoothed_numpy_array, dtype=torch.float32)\n",
    "\n",
    "        # Generate fractional indices\n",
    "        indices = torch.linspace(0, depth - 1, model_input_dim_depth)\n",
    "        # print(f\"Indices shape: {indices.shape}\")\n",
    "        # print(f\"Indices: {indices}\")\n",
    "\n",
    "        # Round to nearest integer and clamp indices to valid range\n",
    "        indices = torch.clamp(indices.round().long(), 0, depth - 1)\n",
    "\n",
    "        # Select slices at these indices\n",
    "        reduced_tensor = smoothed_tensor[..., indices]\n",
    "\n",
    "        return reduced_tensor\n",
    "\n",
    "    def one_hot_encode(self, array):\n",
    "        # Get unique labels in the array\n",
    "        unique_labels = np.unique(array)\n",
    "        # Create a dictionary mapping each label to an index\n",
    "        label_to_index = {label: index for index, label in enumerate(unique_labels)}\n",
    "        # Shape for one-hot encoding: (height, width, num_classes)\n",
    "        one_hot_shape = array.shape + (len(unique_labels),)\n",
    "        # Initialize one-hot encoded array\n",
    "        one_hot_encoded = np.zeros(one_hot_shape, dtype=np.float32)\n",
    "        # Populate one-hot array\n",
    "        for label, index in label_to_index.items():\n",
    "            one_hot_encoded[..., index] = (array == label).astype(np.float32)\n",
    "        #Convert into tensor\n",
    "        one_hot_encoded = torch.tensor(one_hot_encoded, dtype=torch.float32)\n",
    "        # Shape for one-hot encoding: (num_classes, height, width)\n",
    "        one_hot_encoded = one_hot_encoded.permute(2, 0, 1)\n",
    "\n",
    "        return one_hot_encoded, label_to_index\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.is_inference_mode:\n",
    "            return len(self.metadata_dict_with_files_selected)\n",
    "        else:\n",
    "            return len(self.preprocessed_patches_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.is_inference_mode:\n",
    "            sample_id = list(self.metadata_dict_with_files_selected.keys())[idx]          \n",
    "            y = y_filename = self.preprocessed_info_dict[sample_id][\"label\"][\"cropped_label_tensor_filename\"]\n",
    "            x = patches_info_dict = self.preprocessed_info_dict[sample_id][\"measurement\"][f\"depth[{self.model_input_dim[2]}]\"][\"patches\"][f\"patch_size[{self.patch_size}]\"][f\"overlap[{self.overlap_key}]\"]\n",
    "            return x, y\n",
    "        else:\n",
    "            x_filename = self.preprocessed_patches_dataset[idx][\"patch_measurement_info\"][\"patch_path\"]\n",
    "            y_filename = self.preprocessed_patches_dataset[idx][\"patch_label_info\"][\"patch_path\"]\n",
    "    \n",
    "            print(\"----- Filenames (Dataset call: __getitem__) -----\")\n",
    "            print(f\"X Filename: {x_filename}\")\n",
    "            print(f\"Y Filename: {y_filename}\")\n",
    "            print(\"-------------------------------------------------\")\n",
    "    \n",
    "            x = torch.load(x_filename, weights_only=True)\n",
    "            y = torch.load(y_filename, weights_only=True)\n",
    "    \n",
    "            return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8cOsPFnwzZ3l"
   },
   "source": [
    "# MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All imports succeeded!\n"
     ]
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import itertools\n",
    "from collections.abc import Sequence\n",
    "from typing import Final\n",
    "\n",
    "# Third-party imports\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.checkpoint as checkpoint\n",
    "from torch.nn import LayerNorm\n",
    "from typing import Type\n",
    "from einops import rearrange\n",
    "\n",
    "from monai.networks.blocks import MLPBlock as Mlp\n",
    "from monai.networks.blocks import PatchEmbed, UnetOutBlock, UnetrBasicBlock, UnetrUpBlock\n",
    "from monai.networks.layers import DropPath, trunc_normal_\n",
    "from monai.utils import ensure_tuple_rep, look_up_option\n",
    "\n",
    "print(\"All imports succeeded!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "VooONFUYvj4f"
   },
   "outputs": [],
   "source": [
    "class WindowAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Window based multi-head self attention module with relative position bias based on: \"Liu et al.,\n",
    "    Swin Transformer: Hierarchical Vision Transformer using Shifted Windows\n",
    "    <https://arxiv.org/abs/2103.14030>\"\n",
    "    https://github.com/microsoft/Swin-Transformer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        num_heads: int,\n",
    "        window_size: Sequence[int],\n",
    "        qkv_bias: bool = False,\n",
    "        attn_drop: float = 0.0,\n",
    "        proj_drop: float = 0.0,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dim: Number of input feature channels.\n",
    "            num_heads: Number of attention heads.\n",
    "            window_size: Size of the local window for attention.\n",
    "            qkv_bias: If True, adds a learnable bias to query, key, value projections.\n",
    "            attn_drop: Dropout rate for attention weights.\n",
    "            proj_drop: Dropout rate for output projection.\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.dim = dim  # Dimension of input features\n",
    "        self.window_size = window_size  # Size of the attention window\n",
    "        self.num_heads = num_heads  # Number of attention heads\n",
    "\n",
    "        # Dimension per attention head\n",
    "        head_dim = dim // num_heads\n",
    "        # Scaling factor for attention scores to prevent large values during softmax\n",
    "        self.scale = head_dim**-0.5\n",
    "        # Check for meshgrid arguments compatibility\n",
    "        mesh_args = torch.meshgrid.__kwdefaults__\n",
    "\n",
    "        # Handle 3D window sizes (e.g., for 3D volumes)\n",
    "        if len(self.window_size) == 3:\n",
    "            # Create a parameter table for relative position biases\n",
    "            self.relative_position_bias_table = nn.Parameter(\n",
    "                torch.zeros(\n",
    "                    (2 * self.window_size[0] - 1) * (2 * self.window_size[1] - 1) * (2 * self.window_size[2] - 1),\n",
    "                    num_heads,\n",
    "                )\n",
    "            )\n",
    "            # Create coordinate grids for relative position computation\n",
    "            coords_d = torch.arange(self.window_size[0])\n",
    "            coords_h = torch.arange(self.window_size[1])\n",
    "            coords_w = torch.arange(self.window_size[2])\n",
    "            # Generate coordinate grids with indexing support\n",
    "            if mesh_args is not None:\n",
    "                coords = torch.stack(torch.meshgrid(coords_d, coords_h, coords_w, indexing=\"ij\"))\n",
    "            else:\n",
    "                coords = torch.stack(torch.meshgrid(coords_d, coords_h, coords_w))\n",
    "            coords_flatten = torch.flatten(coords, 1)  # Flatten the coordinates for easier computation\n",
    "            # Compute relative coordinates between each point\n",
    "            relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]\n",
    "            relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Permute dimensions for indexing\n",
    "            # Adjust relative coordinates for bias table indexing\n",
    "            relative_coords[:, :, 0] += self.window_size[0] - 1\n",
    "            relative_coords[:, :, 1] += self.window_size[1] - 1\n",
    "            relative_coords[:, :, 2] += self.window_size[2] - 1\n",
    "            # Map to flattened indices\n",
    "            relative_coords[:, :, 0] *= (2 * self.window_size[1] - 1) * (2 * self.window_size[2] - 1)\n",
    "            relative_coords[:, :, 1] *= 2 * self.window_size[2] - 1\n",
    "\n",
    "        else: # Handle other input tensors\n",
    "            raise ValueError(\"Unsupported dimensions. Expected input to have length of 3 dimensions.\")\n",
    "\n",
    "        # Register the relative position index as a buffer\n",
    "        relative_position_index = relative_coords.sum(-1)\n",
    "        self.register_buffer(\"relative_position_index\", relative_position_index)\n",
    "\n",
    "        # Define linear layers for query, key, value projections\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        # Dropout layers for attention and output projection\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "        # Initialize relative position bias table with truncated normal distribution\n",
    "        trunc_normal_(self.relative_position_bias_table, std=0.02)\n",
    "        # Softmax layer for attention normalization\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"\"\"\n",
    "        Forward pass for window-based self-attention.\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor of shape (num_windows, num_tokens, embed_dim).\n",
    "            mask: Attention mask to restrict certain positions.\n",
    "\n",
    "        Returns:\n",
    "            Tensor of shape (batch_size, num_tokens, embed_dim) after self-attention.\n",
    "        \"\"\"\n",
    "        b, n, c = x.shape\n",
    "        # Compute query, key, and value projections and reshape them\n",
    "        qkv = self.qkv(x).reshape(b, n, 3, self.num_heads, c // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]  # Separate into query, key, and value tensors\n",
    "        q = q * self.scale  # Scale query for better numerical stability\n",
    "        attn = q @ k.transpose(-2, -1)  # Compute dot-product attention scores\n",
    "\n",
    "        # Add relative position bias to attention scores\n",
    "        relative_position_bias = self.relative_position_bias_table[\n",
    "            self.relative_position_index.clone()[:n, :n].reshape(-1)\n",
    "        ].reshape(n, n, -1)\n",
    "        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()\n",
    "        attn = attn + relative_position_bias.unsqueeze(0)  # Broadcast bias across batch and heads\n",
    "\n",
    "        # Apply attention mask if provided\n",
    "        if mask is not None:\n",
    "            nw = mask.shape[0]  # Number of windows\n",
    "            attn = attn.view(b // nw, nw, self.num_heads, n, n) + mask.unsqueeze(1).unsqueeze(0)\n",
    "            attn = attn.view(-1, self.num_heads, n, n)\n",
    "            attn = self.softmax(attn)  # Apply softmax to normalize attention scores\n",
    "        else:\n",
    "            attn = self.softmax(attn)  # Apply softmax to normalize attention scores\n",
    "\n",
    "        # Apply dropout to attention weights\n",
    "        attn = self.attn_drop(attn).to(v.dtype)\n",
    "        # Compute attention-weighted sum of values\n",
    "        x = (attn @ v).transpose(1, 2).reshape(b, n, c)\n",
    "        # Apply linear projection and dropout to the output\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def window_partition(x, window_size):\n",
    "    \"\"\"Window partition operation based on: \"Liu et al.,\n",
    "    Swin Transformer: Hierarchical Vision Transformer using Shifted Windows\"\n",
    "    <https://arxiv.org/abs/2103.14030>\"\n",
    "    https://github.com/microsoft/Swin-Transformer\n",
    "\n",
    "    This function partitions an input tensor into smaller windows based on the specified window size.\n",
    "    This is used in Swin Transformer models to divide the input into regions for applying self-attention efficiently.\n",
    "\n",
    "    Args:\n",
    "        x (Tensor): Input tensor of shape (batch_size, depth, height, width, channels) for 3D input data.\n",
    "        window_size (Sequence[int]): The size of each local window for partitioning.\n",
    "                                     It should be a tuple specifying the size for each spatial dimension (depth, height, width).\n",
    "\n",
    "    Returns:\n",
    "        Tensor: Partitioned tensor of shape (num_windows, window_size_product, channels) where `num_windows`\n",
    "                is the total number of windows, and `window_size_product` is the product of the window dimensions.\n",
    "\n",
    "    Example:\n",
    "        >>> import torch\n",
    "        >>> # Example 3D tensor with batch size 1, depth 8, height 8, width 8, and 3 channels\n",
    "        >>> x = torch.arange(1, 1 * 8 * 8 * 8 * 3 + 1).view(1, 8, 8, 8, 3)\n",
    "        >>> window_size = (4, 4, 4)\n",
    "        >>> windows = window_partition(x, window_size)\n",
    "        >>> print(\"Shape of partitioned windows:\", windows.shape)\n",
    "        >>> # Output shape: (8, 4*4*4, 3) -> (num_windows, window_size_product, channels)\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the shape of the input tensor\n",
    "    x_shape = x.size()\n",
    "\n",
    "    # Handle 5D input tensors (e.g., 3D input data with channels and batch dimensions)\n",
    "    if len(x_shape) == 5:\n",
    "        b, d, h, w, c = x_shape  # Extract batch, depth, height, width, and channels\n",
    "\n",
    "        # Reshape the input tensor so that each dimension becomes divisible by the window size\n",
    "        # This effectively creates smaller windows within the tensor\n",
    "        x = x.view(\n",
    "            b,\n",
    "            d // window_size[0],  # Number of windows along the depth dimension\n",
    "            window_size[0],       # Size of each window along the depth dimension\n",
    "            h // window_size[1],  # Number of windows along the height dimension\n",
    "            window_size[1],       # Size of each window along the height dimension\n",
    "            w // window_size[2],  # Number of windows along the width dimension\n",
    "            window_size[2],       # Size of each window along the width dimension\n",
    "            c                     # Channels (kept the same)\n",
    "        )\n",
    "\n",
    "        # Rearrange the dimensions to bring window dimensions next to each other and flatten each window\n",
    "        # Permute moves the dimensions around to the specified order, making it ready for further processing\n",
    "        windows = (\n",
    "            x.permute(0, 1, 3, 5, 2, 4, 6, 7)  # Change order of dimensions for easier window processing\n",
    "             .contiguous()                     # Ensures that data is stored contiguously in memory\n",
    "             .view(-1, window_size[0] * window_size[1] * window_size[2], c)  # Flatten each window\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported dimensions. Expected input to have length of 5 dimensions (b, d, h, w, c).\")\n",
    "\n",
    "    return windows\n",
    "\n",
    "\n",
    "def window_reverse(windows, window_size, dims):\n",
    "    \"\"\"\n",
    "    Window reverse operation based on: \"Liu et al.,\n",
    "    Swin Transformer: Hierarchical Vision Transformer using Shifted Windows\"\n",
    "    <https://arxiv.org/abs/2103.14030>\n",
    "    https://github.com/microsoft/Swin-Transformer\n",
    "\n",
    "    This function reverses the window partitioning process and reconstructs the original spatial dimensions\n",
    "    from the smaller windows. It reassembles the partitioned windows back into their original spatial arrangement.\n",
    "\n",
    "    Args:\n",
    "        windows: Tensor representing partitioned windows. Shape typically is (num_windows, window_size_product, channels).\n",
    "        window_size: Size of the local window (e.g., (depth, height, width) for 3D).\n",
    "        dims: Dimension values of the original spatial dimensions (before window partitioning).\n",
    "\n",
    "    Returns:\n",
    "        Tensor of shape corresponding to the original dimensions before window partitioning.\n",
    "\n",
    "    Example:\n",
    "        >>> import torch\n",
    "        >>> # Example 3D tensor with batch size 1, depth 8, height 8, width 8, and 3 channels (partitioned into windows)\n",
    "        >>> windows = torch.randn(8, 4*4*4, 3)  # 8 windows, each with size 4x4x4 and 3 channels\n",
    "        >>> window_size = (4, 4, 4)\n",
    "        >>> dims = (1, 8, 8, 8)  # Original dimensions: (batch_size, depth, height, width)\n",
    "        >>> x = window_reverse(windows, window_size, dims)\n",
    "        >>> print(\"Shape of reconstructed tensor:\", x.shape)\n",
    "        >>> # Output shape: (1, 8, 8, 8, 3)\n",
    "    \"\"\"\n",
    "\n",
    "    # Handle 3D data (e.g., 5D input tensor with shape (batch_size, depth, height, width, channels))\n",
    "    if len(dims) == 4:\n",
    "        b, d, h, w = dims  # Unpack batch size and spatial dimensions (depth, height, width)\n",
    "\n",
    "        # Reshape windows to form a structured tensor with individual window dimensions reassembled\n",
    "        x = windows.view(\n",
    "            b,\n",
    "            d // window_size[0],  # Number of windows along the depth dimension\n",
    "            h // window_size[1],  # Number of windows along the height dimension\n",
    "            w // window_size[2],  # Number of windows along the width dimension\n",
    "            window_size[0],  # Depth of each window\n",
    "            window_size[1],  # Height of each window\n",
    "            window_size[2],  # Width of each window\n",
    "            -1,  # Number of channels (kept the same)\n",
    "        )\n",
    "\n",
    "        # Permute dimensions to restore the original spatial arrangement by rearranging window dimensions\n",
    "        # The order of permutation restores the windows to their original tensor layout\n",
    "        x = x.permute(0, 1, 4, 2, 5, 3, 6, 7).contiguous().view(b, d, h, w, -1)\n",
    "\n",
    "    else:\n",
    "        # Raise an error if unsupported dimensions are provided\n",
    "        raise ValueError(\"Unsupported dimensions. Expected input to have length of 4 dimensions (b, d, h, w).\")\n",
    "\n",
    "    # Return the tensor with original spatial dimensions restored\n",
    "    return x\n",
    "\n",
    "\n",
    "def get_window_size(x_size, window_size, shift_size=None):\n",
    "    \"\"\"Computing window size based on: \"Liu et al.,\n",
    "    Swin Transformer: Hierarchical Vision Transformer using Shifted Windows\n",
    "    <https://arxiv.org/abs/2103.14030>\"\n",
    "    https://github.com/microsoft/Swin-Transformer\n",
    "\n",
    "    Computing window size and optional shift size adjustments based on the input size.\n",
    "\n",
    "    This function adjusts the window size and shift size based on the dimensions of the input (`x_size`).\n",
    "    If the input size for a specific dimension is smaller than or equal to the corresponding window size,\n",
    "    the function sets the window size to the input size and the shift size to zero for that dimension.\n",
    "\n",
    "    Args:\n",
    "        x_size (tuple): The input size as a tuple of dimensions (e.g., height, width, depth).\n",
    "        window_size (tuple): The local window size for each dimension.\n",
    "        shift_size (tuple, optional): The amount to shift the window. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Adjusted window size. If `shift_size` is provided, also returns the adjusted shift size.\n",
    "\n",
    "    Example:\n",
    "        >>> x_size = (10, 20, 15)\n",
    "        >>> window_size = (7, 7, 7)\n",
    "        >>> shift_size = (3, 3, 3)\n",
    "        >>> get_window_size(x_size, window_size, shift_size)\n",
    "        ((7, 7, 7), (0, 3, 3))\n",
    "    \"\"\"\n",
    "    # Create a mutable list from the provided window_size for adjustments\n",
    "    use_window_size = list(window_size)\n",
    "    if shift_size is not None:\n",
    "        # Create a mutable list from the provided shift_size for adjustments\n",
    "        use_shift_size = list(shift_size)\n",
    "\n",
    "    # Iterate over each dimension of the input size\n",
    "    for i in range(len(x_size)):\n",
    "        # If the input size in the current dimension is less than or equal to the window size\n",
    "        if x_size[i] <= window_size[i]:\n",
    "            # Adjust the window size to match the input size\n",
    "            use_window_size[i] = x_size[i]\n",
    "            if shift_size is not None:\n",
    "                # Set the shift size to 0 for this dimension since the window size matches input size\n",
    "                use_shift_size[i] = 0\n",
    "\n",
    "    # Return adjusted window size and optionally adjusted shift size\n",
    "    if shift_size is None:\n",
    "        return tuple(use_window_size)\n",
    "    else:\n",
    "        return tuple(use_window_size), tuple(use_shift_size)\n",
    "\n",
    "\n",
    "def compute_mask(dims, window_size, shift_size, device):\n",
    "    \"\"\"\n",
    "    Computing region masks based on: \"Liu et al.,\n",
    "    Swin Transformer: Hierarchical Vision Transformer using Shifted Windows\n",
    "    <https://arxiv.org/abs/2103.14030>\"\n",
    "    https://github.com/microsoft/Swin-TransformerComputing region masks based on: \"Liu et al., Swin Transformer: Hierarchical Vision Transformer using Shifted Windows\"\n",
    "    https://arxiv.org/abs/2103.14030\n",
    "\n",
    "    This function divides the input tensor into regions, assigns a unique integer label to each region, and creates an\n",
    "    attention mask (`attn_mask`) by comparing labels within and across windows. This mask restricts attention computations\n",
    "    to only valid elements, optimizing efficiency and adhering to the Swin Transformer's local attention mechanism.\n",
    "\n",
    "    Args:\n",
    "        dims: dimension values.\n",
    "        window_size: local window size.\n",
    "        shift_size: shift size.\n",
    "        device: device.\n",
    "    \"\"\"\n",
    "\n",
    "    cnt = 0\n",
    "\n",
    "    if len(dims) == 3:\n",
    "        # For a 3D input tensor (depth, height, width), create an initial mask filled with zeros\n",
    "        d, h, w = dims\n",
    "        img_mask = torch.zeros((1, d, h, w, 1), device=device)\n",
    "\n",
    "        # Dividing the input tensor into 3D regions using slices for depth, height, and width dimensions\n",
    "        # Each dimension is divided into three slices based on the window and shift sizes.\n",
    "        # Example window_size = (7, 7, 7), shift_size = (3, 3, 3) will divide each dimension as:\n",
    "        # - slice(-window_size[0]):  Covers elements from the beginning up to index, in this case, -7 (not inclusive)\n",
    "        # - slice(-window_size[0], -shift_size[0]): Covers elements between indices `-7` to `-3`\n",
    "        # - slice(-shift_size[0], None): Covers the last `3` elements\n",
    "        for d in slice(-window_size[0]), slice(-window_size[0], -shift_size[0]), slice(-shift_size[0], None):\n",
    "            for h in slice(-window_size[1]), slice(-window_size[1], -shift_size[1]), slice(-shift_size[1], None):\n",
    "                for w in slice(-window_size[2]), slice(-window_size[2], -shift_size[2]), slice(-shift_size[2], None):\n",
    "                    # Assign a unique integer label to each region within the 3D space\n",
    "                    img_mask[:, d, h, w, :] = cnt\n",
    "                    cnt += 1\n",
    "    elif len(dims) == 2:\n",
    "        raise ValueError(\"2D input is not supported. Please provide a 3D input with dimensions (d, h, w).\")\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported dimensions. Expected input to have length of 3 dimensions (d, h, w).\")\n",
    "\n",
    "    # The `img_mask` tensor, which contains unique integer labels for different regions of the input tensor,\n",
    "    # is now partitioned into smaller windows of size specified by `window_size` using the `window_partition` function.\n",
    "    # This function divides the tensor spatially into separate non-overlapping windows for localized processing,\n",
    "    # facilitating efficient computation of self-attention in the Swin Transformer by focusing within each window.\n",
    "    # mask_windows => tensor of shape (num_windows, window_size_product, 1)\n",
    "    mask_windows = window_partition(img_mask, window_size)\n",
    "\n",
    "    # Since `img_mask` initially had an extra singleton dimension (i.e., shape (1, d, h, w, 1) for 3D data),\n",
    "    # we remove this last dimension using `squeeze(-1)`.\n",
    "    # This operation reduces the dimensionality of `mask_windows` by eliminating the singleton dimension,\n",
    "    # resulting in a tensor that contains the labels of regions in each window.\n",
    "    # mask_windows => tensor of shape (num_windows, window_size_product)\n",
    "    mask_windows = mask_windows.squeeze(-1)\n",
    "\n",
    "    # Create an attention mask for controlling the attention mechanism in the Swin Transformer.\n",
    "    # The attention mask is generated by comparing the labels of elements in different windows:\n",
    "    # - `mask_windows.unsqueeze(1)` expands the dimensions of `mask_windows` for broadcasting so that\n",
    "    #   each window label can be compared with every other window label.\n",
    "    # - `mask_windows.unsqueeze(2)` similarly expands the dimensions of `mask_windows` for element-wise comparisons.\n",
    "\n",
    "    # The subtraction operation (`mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)`) generates a tensor\n",
    "    # that contains zero values when elements belong to the same window and non-zero values otherwise.\n",
    "    # This tensor effectively encodes information about which elements can attend to each other:\n",
    "    # - Elements with zero values (same window labels) can attend to each other.\n",
    "    # - Elements with non-zero values (different window labels) cannot attend to each other.\n",
    "\n",
    "    # The mask is further refined using `masked_fill`:\n",
    "    # - `masked_fill(attn_mask != 0, float(-100.0))` sets large negative values (-100.0) for elements\n",
    "    #   that belong to different windows, effectively blocking attention between them by making their\n",
    "    #   attention scores very low (close to negative infinity).\n",
    "    # - `masked_fill(attn_mask == 0, float(0.0))` sets zero values for elements within the same window,\n",
    "    #   allowing attention between them without modification of their scores.\n",
    "    attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)\n",
    "    attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))\n",
    "\n",
    "\n",
    "    return attn_mask\n",
    "\n",
    "\n",
    "class SwinTransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Swin Transformer block based on: \"Liu et al.,\n",
    "    Swin Transformer: Hierarchical Vision Transformer using Shifted Windows\n",
    "    <https://arxiv.org/abs/2103.14030>\"\n",
    "    https://github.com/microsoft/Swin-Transformer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        num_heads: int,\n",
    "        window_size: Sequence[int],\n",
    "        shift_size: Sequence[int],\n",
    "        mlp_ratio: float = 4.0,\n",
    "        qkv_bias: bool = True,\n",
    "        drop: float = 0.0,\n",
    "        attn_drop: float = 0.0,\n",
    "        drop_path: float = 0.0,\n",
    "        act_layer: str = \"GELU\",\n",
    "        norm_layer: type[LayerNorm] = nn.LayerNorm,\n",
    "        use_checkpoint: bool = False,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dim: Number of input feature channels.\n",
    "            num_heads: Number of attention heads in the multi-head self-attention mechanism.\n",
    "            window_size: Size of the local window for attention computations.\n",
    "            shift_size: Size of the shift applied to the window during the shifted-window mechanism.\n",
    "            mlp_ratio: Ratio of the hidden dimension size in the MLP to the embedding dimension size.\n",
    "            qkv_bias: Boolean indicating whether to add a bias term to the query, key, and value tensors.\n",
    "            drop: Dropout rate for the final output projection.\n",
    "            attn_drop: Dropout rate for the attention scores.\n",
    "            drop_path: Drop path (stochastic depth) rate.\n",
    "            act_layer: Activation function used in the MLP layers (e.g., GELU).\n",
    "            norm_layer: Normalization layer applied before and after attention (default: LayerNorm).\n",
    "            use_checkpoint: If True, use gradient checkpointing to save memory during training.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.num_heads = num_heads\n",
    "        self.window_size = window_size\n",
    "        self.shift_size = shift_size\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "        self.use_checkpoint = use_checkpoint\n",
    "\n",
    "        # Layer normalization before self-attention\n",
    "        self.norm1 = norm_layer(dim)\n",
    "\n",
    "        # Window-based multi-head self-attention mechanism\n",
    "        self.attn = WindowAttention(\n",
    "            dim,\n",
    "            window_size=self.window_size,\n",
    "            num_heads=num_heads,\n",
    "            qkv_bias=qkv_bias,\n",
    "            attn_drop=attn_drop,\n",
    "            proj_drop=drop,\n",
    "        )\n",
    "\n",
    "        # Optional drop path (stochastic depth)\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()\n",
    "\n",
    "        # Second normalization layer\n",
    "        self.norm2 = norm_layer(dim)\n",
    "\n",
    "        # MLP block with one hidden layer\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(hidden_size=dim, mlp_dim=mlp_hidden_dim, act=act_layer, dropout_rate=drop, dropout_mode=\"swin\")\n",
    "\n",
    "    def forward_part1(self, x, mask_matrix):\n",
    "        \"\"\"\n",
    "        Applies the first part of the forward pass in the Swin Transformer block, including:\n",
    "          - 1. Layer normalization\n",
    "          - 2. Handling input shape and padding\n",
    "          - 3. Applying window partitioning and shifted window self-attention\n",
    "          - 4. Reversing window operations\n",
    "          - 5. Removing padding (if applied)\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input tensor of shape (batch_size, depth, height, width, channels) or 4D equivalent.\n",
    "            mask_matrix (Tensor): Precomputed attention mask to control self-attention behavior.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Output tensor after applying all transformations.\n",
    "        \"\"\"\n",
    "        # Get the shape of the input tensor (could be 5D or 4D)\n",
    "        x_shape = x.size()\n",
    "\n",
    "        # Apply layer normalization to stabilize and optimize the learning process\n",
    "        x = self.norm1(x)\n",
    "\n",
    "        # Handle 5D input tensors (e.g., 3D images)\n",
    "        if len(x_shape) == 5:\n",
    "            b, d, h, w, c = x.shape  # Unpack the dimensions: batch size, depth, height, width, and channels\n",
    "\n",
    "            # Calculate effective window size and shift size based on input dimensions\n",
    "            window_size, shift_size = get_window_size((d, h, w), self.window_size, self.shift_size)\n",
    "\n",
    "            # Calculate padding needed to make dimensions divisible by the window size\n",
    "            # No padding on the left/top/front sides (pad_l, pad_t, pad_d0 are zero)\n",
    "            pad_l = pad_t = pad_d0 = 0\n",
    "            # Calculate padding for the right/bottom/back sides to ensure divisibility\n",
    "            pad_d1 = (window_size[0] - d % window_size[0]) % window_size[0]  # Depth dimension padding\n",
    "            pad_b = (window_size[1] - h % window_size[1]) % window_size[1]  # Height dimension padding\n",
    "            pad_r = (window_size[2] - w % window_size[2]) % window_size[2]  # Width dimension padding\n",
    "\n",
    "            # Apply padding to the input tensor to ensure its dimensions are divisible by the window size\n",
    "            # Padding order: (width padding right, width padding left, height padding bottom, height padding top, ...)\n",
    "            x = F.pad(x, (0, 0, pad_l, pad_r, pad_t, pad_b, pad_d0, pad_d1))\n",
    "\n",
    "            # Update dimension variables to reflect changes after padding\n",
    "            _, dp, hp, wp, _ = x.shape  # dp, hp, wp are the new depth, height, and width after padding\n",
    "            dims = [b, dp, hp, wp]  # Store updated dimensions\n",
    "\n",
    "        else:  # Handle other input tensors (e.g., 2D input data with channels and batch dimensions)\n",
    "            raise ValueError(\"Unsupported dimensions. Expected input to have length of 5 dimensions (b, d, h, w, c).\")\n",
    "\n",
    "        # Check if any shift is required (shift_size > 0)\n",
    "        if any(i > 0 for i in shift_size):\n",
    "            # Apply negative rolling shift along depth, height, and width if input is 5D\n",
    "            if len(x_shape) == 5:\n",
    "                # Roll (shift) elements along depth, height, and width dimensions (CYCLE SHIFT)\n",
    "                shifted_x = torch.roll(x, shifts=(-shift_size[0], -shift_size[1], -shift_size[2]), dims=(1, 2, 3))\n",
    "            # Set attention mask to the precomputed mask matrix for shifted attention\n",
    "            attn_mask = mask_matrix\n",
    "        else:\n",
    "            # No shift is needed, retain original input\n",
    "            shifted_x = x\n",
    "            attn_mask = None  # No attention mask is required for non-shifted windows\n",
    "\n",
    "        # Partition the (shifted) input tensor into windows for applying window-based self-attention\n",
    "        x_windows = window_partition(shifted_x, window_size)\n",
    "        # After window partitioning:\n",
    "        # - For 3D input, x_windows has shape (num_windows, window_d * window_h * window_w, channels)\n",
    "        # Here, `num_windows` is the number of windows formed, `window_d`, `window_h`, `window_w` are\n",
    "        # window dimensions, and `channels` is the number of feature channels.\n",
    "\n",
    "        # Apply window-based self-attention mechanism using `self.attn`\n",
    "        # This computes self-attention independently within each window\n",
    "        attn_windows = self.attn(x_windows, mask=attn_mask)\n",
    "        # After applying attention:\n",
    "        # - attn_windows retains the shape (num_windows, window_area, channels) where `window_area` is the product of window dimensions (e.g., window_d * window_h * window_w for 3D).\n",
    "        # - The attention mechanism is performed independently within each window, and the shape of the output remains consistent.\n",
    "\n",
    "        # attn_windows has shape (num_windows, window_area, channels) after attention\n",
    "        # Reverse window partitioning by reshaping attention windows back to original shape per window\n",
    "        attn_windows = attn_windows.view(-1, *(window_size + (c,)))\n",
    "        # After reshaping:\n",
    "        # - attn_windows now has shape (num_windows, window_d, window_h, window_w, channels) for 3D input\n",
    "\n",
    "        # Restore spatial structure of the original input using window reversing\n",
    "        shifted_x = window_reverse(attn_windows, window_size, dims)\n",
    "        # After window_reverse:\n",
    "        # - shifted_x is restored to its spatial structure with shape (batch_size, depth, height, width, channels) for 3D\n",
    "\n",
    "\n",
    "        # If a shift was applied earlier, roll back the shift to restore original structure\n",
    "        if any(i > 0 for i in shift_size):\n",
    "            if len(x_shape) == 5:\n",
    "                # Apply positive rolling shift to revert the previous shift operation (REVERSE CYCLE SHIFT)\n",
    "                x = torch.roll(shifted_x, shifts=(shift_size[0], shift_size[1], shift_size[2]), dims=(1, 2, 3))\n",
    "        else:\n",
    "            # No shift was applied, retain the tensor as is\n",
    "            x = shifted_x\n",
    "\n",
    "        # Remove padding if any was applied earlier to restore original dimensions\n",
    "        if len(x_shape) == 5:\n",
    "            # Check if any padding was added in depth, height, or width dimensions\n",
    "            if pad_d1 > 0 or pad_r > 0 or pad_b > 0:\n",
    "                # Slice out the padded regions to return to original dimensions\n",
    "                x = x[:, :d, :h, :w, :].contiguous()\n",
    "\n",
    "        return x  # Return the processed tensor\n",
    "\n",
    "\n",
    "    def forward_part2(self, x):\n",
    "        \"\"\"\n",
    "        Applies a series of operations on the input tensor 'x', including:\n",
    "          - 1. Normalization\n",
    "          - 2. Multi-layer perceptron (MLP) Transformation\n",
    "          - 3. Drop Path Regularization\n",
    "        This function contributes to the forward pass (second part) of the Swin Transformer block.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input tensor to be processed.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Output tensor after applying normalization, MLP, and drop path regularization.\n",
    "        \"\"\"\n",
    "        # Normalize the input tensor 'x' across the last dimension\n",
    "        # Layer normalization helps stabilize and optimize the learning process\n",
    "        x = self.norm2(x)\n",
    "\n",
    "        # Pass the normalized tensor through a Multi-Layer Perceptron (MLP)\n",
    "        # The MLP typically includes linear transformations, non-linear activations, and optional dropout\n",
    "        x = self.mlp(x)\n",
    "\n",
    "        # Apply drop path regularization (also known as stochastic depth)\n",
    "        # Drop path regularization randomly drops entire layers or paths during training to improve generalization\n",
    "        # In this case, if dropped, the output of this function will be nullified, effectively skipping the contribution of this part during training\n",
    "        return self.drop_path(x)\n",
    "\n",
    "\n",
    "\n",
    "    def load_from(self, weights, n_block, layer):\n",
    "        \"\"\"\n",
    "        Load weights from a pre-trained Swin Transformer model.\n",
    "        This method copies specific parameters from a state_dict into the corresponding layers of this block.\n",
    "        \"\"\"\n",
    "        root = f\"module.{layer}.0.blocks.{n_block}.\"\n",
    "        block_names = [\n",
    "            \"norm1.weight\",\n",
    "            \"norm1.bias\",\n",
    "            \"attn.relative_position_bias_table\",\n",
    "            \"attn.relative_position_index\",\n",
    "            \"attn.qkv.weight\",\n",
    "            \"attn.qkv.bias\",\n",
    "            \"attn.proj.weight\",\n",
    "            \"attn.proj.bias\",\n",
    "            \"norm2.weight\",\n",
    "            \"norm2.bias\",\n",
    "            \"mlp.fc1.weight\",\n",
    "            \"mlp.fc1.bias\",\n",
    "            \"mlp.fc2.weight\",\n",
    "            \"mlp.fc2.bias\",\n",
    "        ]\n",
    "        with torch.no_grad():\n",
    "            # Copy each relevant parameter from the weights\n",
    "            self.norm1.weight.copy_(weights[\"state_dict\"][root + block_names[0]])\n",
    "            self.norm1.bias.copy_(weights[\"state_dict\"][root + block_names[1]])\n",
    "            self.attn.relative_position_bias_table.copy_(weights[\"state_dict\"][root + block_names[2]])\n",
    "            self.attn.relative_position_index.copy_(weights[\"state_dict\"][root + block_names[3]])\n",
    "            self.attn.qkv.weight.copy_(weights[\"state_dict\"][root + block_names[4]])\n",
    "            self.attn.qkv.bias.copy_(weights[\"state_dict\"][root + block_names[5]])\n",
    "            self.attn.proj\n",
    "\n",
    "    def forward(self, x, mask_matrix):\n",
    "        \"\"\"\n",
    "        Forward pass of the SwinTransformerBlock.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor.\n",
    "            mask_matrix (torch.Tensor): Attention mask matrix to restrict computations.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor after applying the Swin Transformer block.\n",
    "        \"\"\"\n",
    "        # Save a copy of the input tensor as a shortcut (residual connection)\n",
    "        shortcut = x\n",
    "\n",
    "        # Check if gradient checkpointing is used; if so, compute the first part of the forward pass\n",
    "        if self.use_checkpoint:\n",
    "            # Use PyTorch's checkpointing to save memory during training by re-computing forward pass during backward pass\n",
    "            x = checkpoint.checkpoint(self.forward_part1, x, mask_matrix, use_reentrant=False)\n",
    "        else:\n",
    "            # Regular first part of the forward pass without checkpointing\n",
    "            x = self.forward_part1(x, mask_matrix)\n",
    "\n",
    "        # Add the residual connection and apply drop path regularization\n",
    "        x = shortcut + self.drop_path(x)\n",
    "\n",
    "        # Perform the second part of the forward pass, with optional checkpointing for memory savings\n",
    "        if self.use_checkpoint:\n",
    "            x = x + checkpoint.checkpoint(self.forward_part2, x, use_reentrant=False)\n",
    "        else:\n",
    "            # If not using checkpointing, directly compute the second part of the forward pass\n",
    "            x = x + self.forward_part2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class BasicLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Basic Swin Transformer layer in one stage based on: \"Liu et al.,\n",
    "    Swin Transformer: Hierarchical Vision Transformer using Shifted Windows\n",
    "    <https://arxiv.org/abs/2103.14030>\"\n",
    "    https://github.com/microsoft/Swin-Transformer\n",
    "\n",
    "    This class implements a single stage of Swin Transformer, which operates on input data\n",
    "    using window-based self-attention and shift mechanisms to enhance spatial interactions.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        depth: int,\n",
    "        num_heads: int,\n",
    "        window_size: Sequence[int],\n",
    "        drop_path: list,\n",
    "        mlp_ratio: float = 4.0,\n",
    "        qkv_bias: bool = False,\n",
    "        drop: float = 0.0,\n",
    "        attn_drop: float = 0.0,\n",
    "        norm_layer: type[LayerNorm] = nn.LayerNorm,\n",
    "        downsample: nn.Module | None = None,\n",
    "        use_checkpoint: bool = False,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initializes the BasicLayer for a Swin Transformer block stage.\n",
    "\n",
    "        Args:\n",
    "            dim (int): Number of feature channels for the input.\n",
    "            depth (int): Number of Swin Transformer blocks in this stage.\n",
    "            num_heads (int): Number of attention heads used in the multi-head attention module.\n",
    "            window_size (Sequence[int]): Size of the local attention window (e.g., [7, 7, 7]).\n",
    "            drop_path (list): List containing the stochastic depth rates for each block.\n",
    "            mlp_ratio (float): Ratio of MLP hidden dimensions to the input dimension.\n",
    "            qkv_bias (bool): If True, adds a learnable bias to query, key, and value tensors.\n",
    "            drop (float): Dropout rate applied to MLP layers.\n",
    "            attn_drop (float): Dropout rate applied to attention weights.\n",
    "            norm_layer (LayerNorm): Normalization layer used in the blocks.\n",
    "            downsample (nn.Module | None): Optional downsampling module applied at the end of the layer.\n",
    "            use_checkpoint (bool): If True, enables gradient checkpointing to reduce memory usage.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Set window size, shift size, and no-shift size for the layer\n",
    "        self.window_size = window_size\n",
    "        self.shift_size = tuple(i // 2 for i in window_size)  # Shift size is half of the window size\n",
    "        self.no_shift = tuple(0 for i in window_size)  # No shift is represented by zeros\n",
    "        self.depth = depth  # Number of Swin Transformer blocks in this stage\n",
    "        self.use_checkpoint = use_checkpoint  # Use checkpointing for memory efficiency\n",
    "\n",
    "        # Create a list of Swin Transformer blocks for this stage\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [\n",
    "                SwinTransformerBlock(\n",
    "                    dim=dim,  # Number of feature channels\n",
    "                    num_heads=num_heads,  # Number of attention heads\n",
    "                    window_size=self.window_size,  # Window size for attention\n",
    "                    shift_size=self.no_shift if (i % 2 == 0) else self.shift_size,  # Alternate between no shift and shift\n",
    "                    mlp_ratio=mlp_ratio,  # MLP hidden dimension ratio\n",
    "                    qkv_bias=qkv_bias,  # Add bias to query, key, value tensors if True\n",
    "                    drop=drop,  # Dropout rate for MLP\n",
    "                    attn_drop=attn_drop,  # Dropout rate for attention\n",
    "                    drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,  # Drop path rate for stochastic depth\n",
    "                    norm_layer=norm_layer,  # Normalization layer\n",
    "                    use_checkpoint=use_checkpoint,  # Use gradient checkpointing\n",
    "                )\n",
    "                for i in range(depth)  # Create `depth` number of blocks\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Initialize optional downsampling module if provided\n",
    "        self.downsample = downsample\n",
    "        if callable(self.downsample):  # Check if downsample is callable\n",
    "            self.downsample = downsample(dim=dim, norm_layer=norm_layer, spatial_dims=len(self.window_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the BasicLayer.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input tensor of shape (b, c, d, h, w) for 3D data.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Output tensor after applying Swin Transformer blocks and optional downsampling.\n",
    "        \"\"\"\n",
    "        # Get the shape of the input tensor (batch size, channels, depth, height, width)\n",
    "        x_shape = x.size()\n",
    "\n",
    "        # Handle 3D input tensors (e.g., volumetric data with batch size, channels, depth, height, width)\n",
    "        if len(x_shape) == 5:\n",
    "            b, c, d, h, w = x_shape  # Extract dimensions from input shape\n",
    "\n",
    "            # Determines the effective window_size and shift_size based on the dimensions\n",
    "            # (d, h, w) of the input tensor and pre-defined window size and shift size values from the instance.\n",
    "\n",
    "            window_size, shift_size = get_window_size((d, h, w), self.window_size, self.shift_size)\n",
    "\n",
    "            x = rearrange(x, \"b c d h w -> b d h w c\")\n",
    "\n",
    "            # dp, hp, and wp values are used to define the \"padded\" dimensions of the input,\n",
    "            # ensuring compatibility with window-based operation\n",
    "            dp = int(np.ceil(d / window_size[0])) * window_size[0]\n",
    "            hp = int(np.ceil(h / window_size[1])) * window_size[1]\n",
    "            wp = int(np.ceil(w / window_size[2])) * window_size[2]\n",
    "\n",
    "            # The `compute_mask` function generates a mask that partitions the input tensor into distinct regions,\n",
    "            # based on specified dimensions (`[dp, hp, wp]`), a `window_size`, and a `shift_size`.\n",
    "            # This mask is used to control the attention mechanism during computation, ensuring that elements\n",
    "            # only attend to others within the same window or shifted region.\n",
    "            #\n",
    "            # Key details:\n",
    "            # - `dims = [dp, hp, wp]` are the padded input dimensions (depth, height, and width).\n",
    "            # - `window_size` specifies the size of each window for partitioning, e.g., (7, 7, 7).\n",
    "            # - `shift_size` specifies the window shift amount, used to enhance spatial interactions.\n",
    "            # - `x.device` indicates where the computation occurs (CPU/GPU).\n",
    "            #\n",
    "            # The function divides the input tensor into regions, assigns a unique integer label to each region,\n",
    "            # and creates an attention mask (`attn_mask`) by comparing labels within and across windows.\n",
    "            # This mask restricts attention computations to only valid elements, optimizing efficiency\n",
    "            # and adhering to the Swin Transformer's local attention mechanism.\n",
    "\n",
    "            attn_mask = compute_mask([dp, hp, wp], window_size, shift_size, x.device)\n",
    "\n",
    "            # Apply each Swin Transformer block in sequence to the input tensor\n",
    "            for blk in self.blocks:\n",
    "                x = blk(x, attn_mask)\n",
    "\n",
    "            # Reshape the output back to its original shape with updated channels\n",
    "            # Before reshaping, 'x' has dimensions (b, dp, hp, wp, c), where dp, hp, and wp may include padding\n",
    "            # After reshaping, 'x' returns to its original spatial dimensions (b, d, h, w, -1), removing any padding\n",
    "            # -1 means that the last dimension size is infered by dividing the total number of elements by (b * d * h * w).\n",
    "            # It adjusts the feature dimension (channel dimension) to accommodate transformations done by the Swin Transformer blocks.\n",
    "            x = x.view(b, d, h, w, -1)\n",
    "\n",
    "            # Apply optional downsampling if a downsample module is defined\n",
    "            if self.downsample is not None:\n",
    "                x = self.downsample(x)\n",
    "\n",
    "            # Rearrange tensor back to original format (b, c, d, h, w)\n",
    "            x = rearrange(x, \"b d h w c -> b c d h w\")\n",
    "\n",
    "        else:\n",
    "            # Raise an error if the input tensor does not have the expected number of dimensions\n",
    "            raise ValueError(\"Unsupported dimensions. Expected input to have length of 5 dimensions (b, d, h, w, c).\")\n",
    "\n",
    "        return x  # Return the processed tensor\n",
    "\n",
    "\n",
    "class PatchMergingV2(nn.Module):\n",
    "    \"\"\"\n",
    "    Patch merging layer based on: \"Liu et al.,\n",
    "    Swin Transformer: Hierarchical Vision Transformer using Shifted Windows\n",
    "    <https://arxiv.org/abs/2103.14030>\"\n",
    "    https://github.com/microsoft/Swin-Transformer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim: int, norm_layer: Type[LayerNorm] = nn.LayerNorm, spatial_dims: int = 3) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dim: number of feature channels.\n",
    "            norm_layer: normalization layer.\n",
    "            spatial_dims: number of spatial dims.\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        if spatial_dims == 3:\n",
    "            self.reduction = nn.Linear(8 * dim, 2 * dim, bias=False)\n",
    "            self.norm = norm_layer(8 * dim)\n",
    "        else:\n",
    "          raise ValueError(f\"expecting 3D dim, got {dim}.\")\n",
    "    def forward(self, x):\n",
    "\n",
    "        x_shape = x.size()\n",
    "        if len(x_shape) == 5:\n",
    "            b, d, h, w, c = x_shape\n",
    "            pad_input = (h % 2 == 1) or (w % 2 == 1) or (d % 2 == 1)\n",
    "            if pad_input:\n",
    "                x = F.pad(x, (0, 0, 0, w % 2, 0, h % 2, 0, d % 2))\n",
    "            x0 = x[:, 0::2, 0::2, 0::2, :]\n",
    "            x1 = x[:, 1::2, 0::2, 0::2, :]\n",
    "            x2 = x[:, 0::2, 1::2, 0::2, :]\n",
    "            x3 = x[:, 0::2, 0::2, 1::2, :]\n",
    "            x4 = x[:, 1::2, 0::2, 1::2, :]\n",
    "            x5 = x[:, 1::2, 1::2, 0::2, :]\n",
    "            x6 = x[:, 0::2, 1::2, 1::2, :]\n",
    "            x7 = x[:, 1::2, 1::2, 1::2, :]\n",
    "            x = torch.cat([x0, x1, x2, x3, x4, x5, x6, x7], -1)\n",
    "\n",
    "        else:\n",
    "          raise ValueError(f\"expecting 5D x, got {x.shape}.\")\n",
    "\n",
    "        x = self.norm(x)\n",
    "        x = self.reduction(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class PatchMerging(PatchMergingV2):\n",
    "    \"\"\"The `PatchMerging` module previously defined in v0.9.0.\"\"\"\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_shape = x.size()\n",
    "        if len(x_shape) == 4:\n",
    "            return super().forward(x)\n",
    "        if len(x_shape) != 5:\n",
    "            raise ValueError(f\"expecting 5D x, got {x.shape}.\")\n",
    "        b, d, h, w, c = x_shape\n",
    "        pad_input = (h % 2 == 1) or (w % 2 == 1) or (d % 2 == 1)\n",
    "        if pad_input:\n",
    "            x = F.pad(x, (0, 0, 0, w % 2, 0, h % 2, 0, d % 2))\n",
    "        x0 = x[:, 0::2, 0::2, 0::2, :]\n",
    "        x1 = x[:, 1::2, 0::2, 0::2, :]\n",
    "        x2 = x[:, 0::2, 1::2, 0::2, :]\n",
    "        x3 = x[:, 0::2, 0::2, 1::2, :]\n",
    "        x4 = x[:, 1::2, 0::2, 1::2, :]\n",
    "        x5 = x[:, 0::2, 1::2, 0::2, :]\n",
    "        x6 = x[:, 0::2, 0::2, 1::2, :]\n",
    "        x7 = x[:, 1::2, 1::2, 1::2, :]\n",
    "        x = torch.cat([x0, x1, x2, x3, x4, x5, x6, x7], -1)\n",
    "        x = self.norm(x)\n",
    "        x = self.reduction(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "MERGING_MODE = {\"merging\": PatchMerging, \"mergingv2\": PatchMergingV2}\n",
    "\n",
    "class SwinTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Swin Transformer based on: \"Liu et al.,\n",
    "    Swin Transformer: Hierarchical Vision Transformer using Shifted Windows\n",
    "    <https://arxiv.org/abs/2103.14030>\"\n",
    "    https://github.com/microsoft/Swin-Transformer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_chans: int,\n",
    "        embed_dim: int,\n",
    "        window_size: Sequence[int],\n",
    "        patch_size: Sequence[int],\n",
    "        depths: Sequence[int],\n",
    "        num_heads: Sequence[int],\n",
    "        mlp_ratio: float = 4.0,\n",
    "        qkv_bias: bool = True,\n",
    "        drop_rate: float = 0.0,\n",
    "        attn_drop_rate: float = 0.0,\n",
    "        drop_path_rate: float = 0.0,\n",
    "        norm_layer: type[LayerNorm] = nn.LayerNorm,\n",
    "        patch_norm: bool = False,\n",
    "        use_checkpoint: bool = False,\n",
    "        spatial_dims: int = 3,\n",
    "        downsample=\"merging\",\n",
    "        use_v2=False,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initializes the Swin Transformer model.\n",
    "\n",
    "        Args:\n",
    "            in_chans: Number of input channels.\n",
    "            embed_dim: Dimension of linear projection output channels.\n",
    "            window_size: Local window size used for window-based attention.\n",
    "            patch_size: Size of input patches.\n",
    "            depths: Number of layers in each transformer stage.\n",
    "            num_heads: Number of attention heads in each stage.\n",
    "            mlp_ratio: Ratio of MLP hidden dimension to embedding dimension.\n",
    "            qkv_bias: Boolean indicating whether to add a learnable bias to query, key, and value tensors.\n",
    "            drop_rate: Dropout rate applied to the input embeddings.\n",
    "            attn_drop_rate: Dropout rate specific to the attention mechanism.\n",
    "            drop_path_rate: Rate for stochastic depth (drop path).\n",
    "            norm_layer: Normalization layer type.\n",
    "            patch_norm: Whether to add normalization after patch embedding.\n",
    "            use_checkpoint: Enables gradient checkpointing for reduced memory usage.\n",
    "            spatial_dims: Number of spatial dimensions (e.g., 3 for 3D data).\n",
    "            downsample: Module used for downsampling between stages.\n",
    "            use_v2: Boolean indicating whether to use an updated version with residual convolutional blocks.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_layers = len(depths)  # Number of stages in the transformer\n",
    "        self.embed_dim = embed_dim  # Embedding dimension size\n",
    "        self.patch_norm = patch_norm  # Whether to normalize after patch embedding\n",
    "        self.window_size = window_size  # Size of the attention window\n",
    "        self.patch_size = patch_size  # Patch size for embedding input\n",
    "\n",
    "        # Initialize the patch embedding layer\n",
    "        self.patch_embed = PatchEmbed(\n",
    "            patch_size=patch_size,\n",
    "            in_chans=in_chans,\n",
    "            embed_dim=embed_dim,\n",
    "            norm_layer=norm_layer if self.patch_norm else None,  # Apply normalization if specified\n",
    "            spatial_dims=spatial_dims,\n",
    "        )\n",
    "\n",
    "        # Dropout applied to positionally encoded input embeddings\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "\n",
    "        # Calculate drop path rate schedule for each layer using linear interpolation\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]\n",
    "\n",
    "        self.use_v2 = use_v2  # Boolean to check if using version 2\n",
    "        self.layers1 = nn.ModuleList()  # List for the first stage's layers\n",
    "        self.layers2 = nn.ModuleList()  # List for the second stage's layers\n",
    "        self.layers3 = nn.ModuleList()  # List for the third stage's layers\n",
    "        self.layers4 = nn.ModuleList()  # List for the fourth stage's layers\n",
    "\n",
    "        # If using version 2, initialize additional layers with residual convolutional blocks\n",
    "        if self.use_v2:\n",
    "            self.layers1c = nn.ModuleList()\n",
    "            self.layers2c = nn.ModuleList()\n",
    "            self.layers3c = nn.ModuleList()\n",
    "            self.layers4c = nn.ModuleList()\n",
    "\n",
    "        # Set up the downsampling module\n",
    "        down_sample_mod = look_up_option(downsample, MERGING_MODE) if isinstance(downsample, str) else downsample\n",
    "\n",
    "        # Loop through each stage and initialize layers\n",
    "        for i_layer in range(self.num_layers):\n",
    "            # Create a BasicLayer instance for each stage\n",
    "            layer = BasicLayer(\n",
    "                dim=int(embed_dim * 2**i_layer),  # Double the dimension for each stage\n",
    "                depth=depths[i_layer],  # Number of layers in this stage\n",
    "                num_heads=num_heads[i_layer],  # Number of attention heads\n",
    "                window_size=self.window_size,  # Size of the attention window\n",
    "                drop_path=dpr[sum(depths[:i_layer]) : sum(depths[: i_layer + 1])],  # Drop path rate for this stage\n",
    "                mlp_ratio=mlp_ratio,  # MLP hidden dimension to embedding dimension ratio\n",
    "                qkv_bias=qkv_bias,  # Bias for query, key, and value tensors\n",
    "                drop=drop_rate,  # General dropout rate\n",
    "                attn_drop=attn_drop_rate,  # Attention dropout rate\n",
    "                norm_layer=norm_layer,  # Normalization layer type\n",
    "                downsample=down_sample_mod,  # Downsampling module\n",
    "                use_checkpoint=use_checkpoint,  # Enable gradient checkpointing for memory efficiency\n",
    "            )\n",
    "\n",
    "            # Append the layer to the appropriate module list\n",
    "            if i_layer == 0:\n",
    "                self.layers1.append(layer)\n",
    "            elif i_layer == 1:\n",
    "                self.layers2.append(layer)\n",
    "            elif i_layer == 2:\n",
    "                self.layers3.append(layer)\n",
    "            elif i_layer == 3:\n",
    "                self.layers4.append(layer)\n",
    "\n",
    "            # Add corresponding residual convolutional layers if using version 2\n",
    "            if self.use_v2:\n",
    "                layerc = UnetrBasicBlock(\n",
    "                    spatial_dims=spatial_dims,\n",
    "                    in_channels=embed_dim * 2**i_layer,\n",
    "                    out_channels=embed_dim * 2**i_layer,\n",
    "                    kernel_size=3,\n",
    "                    stride=1,\n",
    "                    norm_name=\"instance\",\n",
    "                    res_block=True,\n",
    "                )\n",
    "                if i_layer == 0:\n",
    "                    self.layers1c.append(layerc)\n",
    "                elif i_layer == 1:\n",
    "                    self.layers2c.append(layerc)\n",
    "                elif i_layer == 2:\n",
    "                    self.layers3c.append(layerc)\n",
    "                elif i_layer == 3:\n",
    "                    self.layers4c.append(layerc)\n",
    "\n",
    "        # Calculate the number of features after the final stage\n",
    "        self.num_features = int(embed_dim * 2 ** (self.num_layers - 1))\n",
    "\n",
    "    def proj_out(self, x, normalize=False):\n",
    "        \"\"\"\n",
    "        Applies projection and optional normalization to the input tensor.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input tensor.\n",
    "            normalize (bool): Whether to apply layer normalization.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Projected and normalized tensor.\n",
    "        \"\"\"\n",
    "        if normalize:\n",
    "            x_shape = x.shape\n",
    "            ch = int(x_shape[1])  # Number of channels\n",
    "            if len(x_shape) == 5:  # If input is 5D (3D spatial data)\n",
    "                x = rearrange(x, \"b c d h w -> b d h w c\")  # Rearrange dimensions for normalization\n",
    "                x = F.layer_norm(x, [ch])  # Apply layer normalization\n",
    "                x = rearrange(x, \"b d h w c -> b c d h w\")  # Rearrange back to original dimensions\n",
    "            else:\n",
    "                # Handle other input dimensions (e.g., 2D data)\n",
    "                raise ValueError(\"Unsupported dimensions. Expected input to have length of 5 dimensions (b, d, h, w, c).\")\n",
    "        return x\n",
    "\n",
    "    def forward(self, x, normalize=True):\n",
    "        \"\"\"\n",
    "        Forward pass for the Swin Transformer.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input tensor.\n",
    "            normalize (bool): Whether to apply normalization after projection.\n",
    "\n",
    "        Returns:\n",
    "            List[Tensor]: Output tensors from each stage.\n",
    "        \"\"\"\n",
    "        # Apply patch embedding and dropout to the input\n",
    "        x0 = self.patch_embed(x)\n",
    "        x0 = self.pos_drop(x0)\n",
    "\n",
    "        # Apply projection and normalization\n",
    "        x0_out = self.proj_out(x0, normalize)\n",
    "\n",
    "        # Forward pass through each stage, conditionally using residual convolutional layers if specified\n",
    "        if self.use_v2:\n",
    "            x0 = self.layers1c[0](x0.contiguous())\n",
    "        x1 = self.layers1[0](x0.contiguous())\n",
    "        x1_out = self.proj_out(x1, normalize)\n",
    "        if self.use_v2:\n",
    "            x1 = self.layers2c[0](x1.contiguous())\n",
    "        x2 = self.layers2[0](x1.contiguous())\n",
    "        x2_out = self.proj_out(x2, normalize)\n",
    "        if self.use_v2:\n",
    "            x2 = self.layers3c[0](x2.contiguous())\n",
    "        x3 = self.layers3[0](x2.contiguous())\n",
    "        x3_out = self.proj_out(x3, normalize)\n",
    "        if self.use_v2:\n",
    "            x3 = self.layers4c[0](x3.contiguous())\n",
    "        x4 = self.layers4[0](x3.contiguous())\n",
    "        x4_out = self.proj_out(x4, normalize)\n",
    "\n",
    "        # Return outputs from each stage\n",
    "        return [x0_out, x1_out, x2_out, x3_out, x4_out]\n",
    "\n",
    "\n",
    "class SwinUNETR(nn.Module):\n",
    "    \"\"\"\n",
    "    Swin UNETR based on: \"Hatamizadeh et al.,\n",
    "    Swin UNETR: Swin Transformers for Semantic Segmentation of Brain Tumors in MRI Images\n",
    "    <https://arxiv.org/abs/2201.01266>\"\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        depths: Sequence[int] = (2, 2, 2, 2),\n",
    "        num_heads: Sequence[int] = (3, 6, 12, 24),\n",
    "        feature_size: int = 24,\n",
    "        norm_name: tuple | str = \"instance\",\n",
    "        drop_rate: float = 0.0,\n",
    "        attn_drop_rate: float = 0.0,\n",
    "        dropout_path_rate: float = 0.0,\n",
    "        normalize: bool = True,\n",
    "        use_checkpoint: bool = False,\n",
    "        spatial_dims: int = 3,\n",
    "        downsample=\"merging\",\n",
    "        use_v2=False,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_channels: dimension of input channels.\n",
    "            out_channels: dimension of output channels.\n",
    "            feature_size: dimension of network feature size.\n",
    "            depths: number of layers in each stage.\n",
    "            num_heads: number of attention heads.\n",
    "            norm_name: feature normalization type and arguments.\n",
    "            drop_rate: dropout rate.\n",
    "            attn_drop_rate: attention dropout rate.\n",
    "            dropout_path_rate: drop path rate.\n",
    "            normalize: normalize output intermediate features in each stage.\n",
    "            use_checkpoint: use gradient checkpointing for reduced memory usage.\n",
    "            spatial_dims: number of spatial dimensions (e.g. 3 for 3D data).\n",
    "            downsample: module used for downsampling, available options are `\"mergingv2\"`, `\"merging\"`.\n",
    "            use_v2: using swinunetr_v2, which adds a residual convolution block at the beginning of each swin stage.\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # Ensure sizes match spatial dimensions\n",
    "        patch_size = ensure_tuple_rep(2, spatial_dims)\n",
    "        window_size = ensure_tuple_rep(7, spatial_dims)\n",
    "\n",
    "        # Ensure valid spatial dimensions\n",
    "        if not (spatial_dims == 3):\n",
    "            raise ValueError(\"Spatial dimension should be 3.\")\n",
    "\n",
    "        # Validate rates between 0 and 1\n",
    "        if not (0 <= drop_rate <= 1):\n",
    "            raise ValueError(\"Dropout rate should be between 0 and 1.\")\n",
    "        if not (0 <= attn_drop_rate <= 1):\n",
    "            raise ValueError(\"Attention dropout rate should be between 0 and 1.\")\n",
    "        if not (0 <= dropout_path_rate <= 1):\n",
    "            raise ValueError(\"Drop path rate should be between 0 and 1.\")\n",
    "\n",
    "        # Ensure feature size is divisible by 12 for the multi-head attention mechanism\n",
    "        if feature_size % 12 != 0:\n",
    "            raise ValueError(\"Feature size should be divisible by 12.\")\n",
    "\n",
    "        self.normalize = normalize\n",
    "\n",
    "        # Define the Swin Transformer-based encoder (`SwinTransformer`) to be used in this UNETR\n",
    "        self.swinViT = SwinTransformer(\n",
    "            in_chans=in_channels,\n",
    "            embed_dim=feature_size,\n",
    "            window_size=window_size,\n",
    "            patch_size=patch_size,\n",
    "            depths=depths,\n",
    "            num_heads=num_heads,\n",
    "            mlp_ratio=4.0,\n",
    "            qkv_bias=True,\n",
    "            drop_rate=drop_rate,\n",
    "            attn_drop_rate=attn_drop_rate,\n",
    "            drop_path_rate=dropout_path_rate,\n",
    "            norm_layer=nn.LayerNorm,\n",
    "            use_checkpoint=use_checkpoint,\n",
    "            spatial_dims=spatial_dims,\n",
    "            downsample=look_up_option(downsample, MERGING_MODE) if isinstance(downsample, str) else downsample,\n",
    "            use_v2=use_v2,\n",
    "        )\n",
    "\n",
    "        # Encoder stages - Use `UnetrBasicBlock` for encoding input features\n",
    "        # These layers transform the input tensor into feature maps at different resolutions\n",
    "        self.encoder1 = UnetrBasicBlock(\n",
    "            spatial_dims=spatial_dims,\n",
    "            in_channels=in_channels,\n",
    "            out_channels=feature_size,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            norm_name=norm_name,\n",
    "            res_block=True,\n",
    "        )\n",
    "\n",
    "        self.encoder2 = UnetrBasicBlock(\n",
    "            spatial_dims=spatial_dims,\n",
    "            in_channels=feature_size,\n",
    "            out_channels=feature_size,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            norm_name=norm_name,\n",
    "            res_block=True,\n",
    "        )\n",
    "\n",
    "        self.encoder3 = UnetrBasicBlock(\n",
    "            spatial_dims=spatial_dims,\n",
    "            in_channels=2 * feature_size,\n",
    "            out_channels=2 * feature_size,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            norm_name=norm_name,\n",
    "            res_block=True,\n",
    "        )\n",
    "\n",
    "        self.encoder4 = UnetrBasicBlock(\n",
    "            spatial_dims=spatial_dims,\n",
    "            in_channels=4 * feature_size,\n",
    "            out_channels=4 * feature_size,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            norm_name=norm_name,\n",
    "            res_block=True,\n",
    "        )\n",
    "\n",
    "        self.encoder10 = UnetrBasicBlock(\n",
    "            spatial_dims=spatial_dims,\n",
    "            in_channels=16 * feature_size,\n",
    "            out_channels=16 * feature_size,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            norm_name=norm_name,\n",
    "            res_block=True,\n",
    "        )\n",
    "\n",
    "        # Decoder stages - Use `UnetrUpBlock` for decoding feature maps to higher resolutions\n",
    "        # These layers upsample the feature maps to reconstruct the original spatial resolution\n",
    "        self.decoder5 = UnetrUpBlock(\n",
    "            spatial_dims=spatial_dims,\n",
    "            in_channels=16 * feature_size,\n",
    "            out_channels=8 * feature_size,\n",
    "            kernel_size=3,\n",
    "            upsample_kernel_size=2,\n",
    "            norm_name=norm_name,\n",
    "            res_block=True,\n",
    "        )\n",
    "\n",
    "        self.decoder4 = UnetrUpBlock(\n",
    "            spatial_dims=spatial_dims,\n",
    "            in_channels=feature_size * 8,\n",
    "            out_channels=feature_size * 4,\n",
    "            kernel_size=3,\n",
    "            upsample_kernel_size=2,\n",
    "            norm_name=norm_name,\n",
    "            res_block=True,\n",
    "        )\n",
    "\n",
    "        self.decoder3 = UnetrUpBlock(\n",
    "            spatial_dims=spatial_dims,\n",
    "            in_channels=feature_size * 4,\n",
    "            out_channels=feature_size * 2,\n",
    "            kernel_size=3,\n",
    "            upsample_kernel_size=2,\n",
    "            norm_name=norm_name,\n",
    "            res_block=True,\n",
    "        )\n",
    "\n",
    "        self.decoder2 = UnetrUpBlock(\n",
    "            spatial_dims=spatial_dims,\n",
    "            in_channels=feature_size * 2,\n",
    "            out_channels=feature_size,\n",
    "            kernel_size=3,\n",
    "            upsample_kernel_size=2,\n",
    "            norm_name=norm_name,\n",
    "            res_block=True,\n",
    "        )\n",
    "\n",
    "        self.decoder1 = UnetrUpBlock(\n",
    "            spatial_dims=spatial_dims,\n",
    "            in_channels=feature_size,\n",
    "            out_channels=feature_size,\n",
    "            kernel_size=3,\n",
    "            upsample_kernel_size=2,\n",
    "            norm_name=norm_name,\n",
    "            res_block=True,\n",
    "        )\n",
    "\n",
    "        # Output block that takes the final feature map and converts it to desired output channels\n",
    "        self.out = UnetOutBlock(spatial_dims=spatial_dims, in_channels=feature_size, out_channels=out_channels)\n",
    "\n",
    "    def load_from(self, weights):\n",
    "        # Load weights from a pretrained model\n",
    "        with torch.no_grad():\n",
    "            self.swinViT.patch_embed.proj.weight.copy_(weights[\"state_dict\"][\"module.patch_embed.proj.weight\"])\n",
    "            self.swinViT.patch_embed.proj.bias.copy_(weights[\"state_dict\"][\"module.patch_embed.proj.bias\"])\n",
    "            for bname, block in self.swinViT.layers1[0].blocks.named_children():\n",
    "                block.load_from(weights, n_block=bname, layer=\"layers1\")\n",
    "            # Load weights for downsampling layers and other components\n",
    "            self.swinViT.layers1[0].downsample.reduction.weight.copy_(\n",
    "                weights[\"state_dict\"][\"module.layers1.0.downsample.reduction.weight\"]\n",
    "            )\n",
    "            self.swinViT.layers1[0].downsample.norm.weight.copy_(\n",
    "                weights[\"state_dict\"][\"module.layers1.0.downsample.norm.weight\"]\n",
    "            )\n",
    "            self.swinViT.layers1[0].downsample.norm.bias.copy_(\n",
    "                weights[\"state_dict\"][\"module.layers1.0.downsample.norm.bias\"]\n",
    "            )\n",
    "            for bname, block in self.swinViT.layers2[0].blocks.named_children():\n",
    "                block.load_from(weights, n_block=bname, layer=\"layers2\")\n",
    "            self.swinViT.layers2[0].downsample.reduction.weight.copy_(\n",
    "                weights[\"state_dict\"][\"module.layers2.0.downsample.reduction.weight\"]\n",
    "            )\n",
    "            self.swinViT.layers2[0].downsample.norm.weight.copy_(\n",
    "                weights[\"state_dict\"][\"module.layers2.0.downsample.norm.weight\"]\n",
    "            )\n",
    "            self.swinViT.layers2[0].downsample.norm.bias.copy_(\n",
    "                weights[\"state_dict\"][\"module.layers2.0.downsample.norm.bias\"]\n",
    "            )\n",
    "            # Repeating for layers3 and layers4\n",
    "\n",
    "    def forward(self, x_in):\n",
    "        \"\"\"\n",
    "        Forward pass for the SwinUNETR model.\n",
    "\n",
    "        Args:\n",
    "            x_in: Input tensor, typically with shape (batch, channels, depth, height, width).\n",
    "\n",
    "        Returns:\n",
    "            logits: Output predictions after applying the Swin Transformer and decoding layers.\n",
    "        \"\"\"\n",
    "\n",
    "        # Pass input through the Swin Transformer encoder\n",
    "        hidden_states_out = self.swinViT(x_in, self.normalize)\n",
    "\n",
    "        # Apply encoder blocks to extract features at multiple resolutions\n",
    "        enc0 = self.encoder1(x_in)\n",
    "        enc1 = self.encoder2(hidden_states_out[0])\n",
    "        enc2 = self.encoder3(hidden_states_out[1])\n",
    "        enc3 = self.encoder4(hidden_states_out[2])\n",
    "\n",
    "        # Apply the decoder blocks in a hierarchical manner to reconstruct the image\n",
    "        dec4 = self.encoder10(hidden_states_out[4])\n",
    "        dec3 = self.decoder5(dec4, hidden_states_out[3])\n",
    "        dec2 = self.decoder4(dec3, enc3)\n",
    "        dec1 = self.decoder3(dec2, enc2)\n",
    "        dec0 = self.decoder2(dec1, enc1)\n",
    "\n",
    "        # Generate final output from the last upsampled decoder output\n",
    "        out = self.decoder1(dec0, enc0)\n",
    "        logits = self.out(out)\n",
    "\n",
    "        return logits\n",
    "\n",
    "\n",
    "class SwinUNETR_AIRT(torch.nn.Module):\n",
    "    def __init__(self, input_dimensions, in_channels, out_classes, **kwargs):\n",
    "        super(SwinUNETR_AIRT, self).__init__()\n",
    "\n",
    "        self.ensure_all_dimensions_divisible(input_dimensions)\n",
    "\n",
    "        self.model = SwinUNETR(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_classes,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "        self.conv_layer = nn.Conv3d(input_dimensions[2], 1, kernel_size=1, stride=1, padding=0, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # batch_size, in_channels, height, width, depth\n",
    "        x = x.permute(0, 1, 4, 2, 3)\n",
    "        # batch_size, in_channels, depth, height, width\n",
    "        x = self.model(x)\n",
    "        # batch_size, out_classes, depth, height, width\n",
    "        x = x.permute(0, 2, 1, 3, 4)\n",
    "        # batch_size, depth, out_classes, height, width\n",
    "        x = self.conv_layer(x)\n",
    "        # batch_size, 1, out_classes, height, width\n",
    "        x = x.squeeze(1)\n",
    "        # batch_size, out_classes, height, width\n",
    "        return x\n",
    "\n",
    "    def ensure_all_dimensions_divisible(self, input_dimensions, divisor=32):\n",
    "        \"\"\"\n",
    "        Ensure that all dimensions in input_dimensions are divisible by the given divisor.\n",
    "        If not, raise an exception.\n",
    "\n",
    "        Args:\n",
    "            input_dimensions (tuple): Input dimensions (e.g., (128, 128, 128)).\n",
    "            divisor (int): The number to ensure divisibility by (default is 32).\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If any dimension is not divisible by the divisor.\n",
    "        \"\"\"\n",
    "        for dim in input_dimensions:\n",
    "            if dim % divisor != 0:\n",
    "                raise ValueError(\n",
    "                    f\"Dimension {dim} is not divisible by {divisor}. All dimensions must be divisible.\"\n",
    "                )\n",
    "        print(\"All dimensions are divisible by\", divisor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qhfDmrpq3vTJ"
   },
   "source": [
    "# LIGHTNING MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All imports succeeded!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, random_split, Dataset\n",
    "import pytorch_lightning as pl\n",
    "import torch.nn.functional as F\n",
    "from monai.losses import DiceLoss\n",
    "from monai.metrics import ConfusionMatrixMetric, MeanIoU, DiceMetric\n",
    "import numpy as np\n",
    "from monai.inferers import sliding_window_inference\n",
    "from functools import partial\n",
    "from fractions import Fraction\n",
    "\n",
    "print(\"All imports succeeded!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "0CZsIk23Rc_q"
   },
   "outputs": [],
   "source": [
    "# Define a PyTorch Lightning model wrapper\n",
    "class SwinUNETR_AIRT_LightningModel(pl.LightningModule):\n",
    "    def __init__(self, patch_dimensions, **kwargs):\n",
    "        super(SwinUNETR_AIRT_LightningModel, self).__init__()\n",
    "\n",
    "        ###################### MODEL INSTANTIATION ###########################\n",
    "\n",
    "        self.patch_dimensions = patch_dimensions\n",
    "\n",
    "        self.model = SwinUNETR_AIRT(input_dimensions=self.patch_dimensions, in_channels=1, out_classes=2, **kwargs)\n",
    "\n",
    "        ###################### LOGGING PARAMETERS ###########################\n",
    "\n",
    "        self.enable_batch_logging_into_console = False  # Boolean flag for controlling batch logging into console\n",
    "        self.enable_training_epoch_logging_into_console = True  # Boolean flag for controlling training epoch logging into console\n",
    "        self.enable_validation_epoch_logging_into_console = True  # Boolean flag for controlling validation epoch logging into console\n",
    "\n",
    "        ###################### LOSS & OTHER METRICS ###########################\n",
    "\n",
    "        self.include_background_in_loss_and_metrics = False  # Boolean flag for controlling inclusion of background in loss and metrics\n",
    "\n",
    "        # DiceLoss ask that the model's output must have at least two channels.\n",
    "        # The first channel is assumed to represent the background class,\n",
    "        # while subsequent channels represent different foreground classes.\n",
    "        #\n",
    "        # If include_background=False, the Dice loss computation will exclude the first channel\n",
    "        # (i.e., it will not compute the Dice score for the first class).\n",
    "        # The loss will be computed only for the rest of the channels.\n",
    "        #\n",
    "        # DiceLoss does not apply any activation function by default (this is our case)\n",
    "        # Therefore, we have to apply the activation function (in our case, softmax) before computing the loss\n",
    "        # over the model's output (y_hat)\n",
    "        #\n",
    "        # y_hat (prediction) is expected to be a multiple-channel tensor containing,\n",
    "        # in each channel/class, the probability corresponding to the channel/class for each pixel/voxel (i.e. the probability that pixel belongs to the class represented by that channel)\n",
    "        # (I.e. y_hat shape: (OUTPUT_CHANNELS, HEIGHT, WIDTH) )\n",
    "        # The first channel is assumed to represent the background class, if include_background=True.\n",
    "        #\n",
    "        # y (ground truth) is expected to be a multiple-channel tensor where each channel represents\n",
    "        # a class, and the grid corresponding to that channel has to be one-hot encoded (just 1s and 0s) representing when\n",
    "        # the corrresponding pixel is labeled with that class (1) or not (0).\n",
    "        # (I.e. y shape: (OUTPUT_CHANNELS, HEIGHT, WIDTH) )\n",
    "        # The first channel is assumed to represent the background class, if include_background=True.\n",
    "\n",
    "        self.loss_fn = DiceLoss(include_background=self.include_background_in_loss_and_metrics)\n",
    "\n",
    "        # When using include_background=True in ConfusionMatrixMetric,\n",
    "        # the model's output must have at least two channels.\n",
    "        # The first channel is assumed to represent the background class,\n",
    "        # while subsequent channels represent different foreground classes.\n",
    "        #\n",
    "        # Since reduction=\"mean_batch\", ConfusionMatrixMetric computes confusion matrices per sample\n",
    "        # and returns their average across the batch.\n",
    "        #\n",
    "        # y_hat (prediction) is expected to be a multiple-channel tensor containing,\n",
    "        # in each channel/class, the probability corresponding to the channel/class for each pixel/voxel (i.e. the probability that pixel belongs to the class represented by that channel)\n",
    "        # (I.e. y_hat shape: (OUTPUT_CHANNELS, HEIGHT, WIDTH) )\n",
    "        # The first channel is assumed to represent the background class, if include_background=True.\n",
    "        #\n",
    "        # y (ground truth) is expected to be a multiple-channel tensor where each channel represents\n",
    "        # a class, and the grid corresponding to that channel has to be one-hot encoded (just 1s and 0s) representing when\n",
    "        # the corrresponding pixel is labeled with that class (1) or not (0).\n",
    "        # (I.e. y shape: (OUTPUT_CHANNELS, HEIGHT, WIDTH) )\n",
    "        # The first channel is assumed to represent the background class, if include_background=True.\n",
    "\n",
    "        self.confusion_matrix_metric = ConfusionMatrixMetric(include_background=self.include_background_in_loss_and_metrics, metric_name=\"confusion_matrix\", reduction=\"mean_batch\")\n",
    "\n",
    "        # When using include_background=True in MeanIoU,\n",
    "        # the model's output must have at least two channels.\n",
    "        # The first channel is assumed to represent the background class,\n",
    "        # while subsequent channels represent different foreground classes.\n",
    "        #\n",
    "        # Since reduction=\"mean\", MeanIoU will return the average IoU score across all classes,\n",
    "        # including the background.\n",
    "        #\n",
    "        # y_hat (prediction) is expected to be a multiple-channel tensor containing,\n",
    "        # in each channel/class, the probability corresponding to the channel/class for each pixel/voxel (i.e. the probability that pixel belongs to the class represented by that channel)\n",
    "        # (I.e. y_hat shape: (OUTPUT_CHANNELS, HEIGHT, WIDTH) )\n",
    "        # The first channel is assumed to represent the background class, if include_background=True.\n",
    "        #\n",
    "        # y (ground truth) is expected to be a multiple-channel tensor where each channel represents\n",
    "        # a class, and the grid corresponding to that channel has to be one-hot encoded (just 1s and 0s) representing when\n",
    "        # the corrresponding pixel is labeled with that class (1) or not (0).\n",
    "        # (I.e. y shape: (OUTPUT_CHANNELS, HEIGHT, WIDTH) )\n",
    "        # The first channel is assumed to represent the background class, if include_background=True.\n",
    "\n",
    "        self.mean_iou_metric = MeanIoU(include_background=self.include_background_in_loss_and_metrics, reduction=\"mean\")\n",
    "\n",
    "        # When using include_background=True in DiceMetric,\n",
    "        # the model's output must have at least two channels.\n",
    "        # The first channel is assumed to represent the background class,\n",
    "        # while subsequent channels represent different foreground classes.\n",
    "        #\n",
    "        # Since reduction=\"mean\", DiceMetric will return the average Dice coefficent across all classes,\n",
    "        # including the background (since include_background=True).\n",
    "        #\n",
    "        # y_hat (prediction) is expected to be a multiple-channel tensor containing,\n",
    "        # in each channel/class, the probability corresponding to the channel/class for each pixel/voxel (i.e. the probability that pixel belongs to the class represented by that channel)\n",
    "        # (I.e. y_hat shape: (OUTPUT_CHANNELS, HEIGHT, WIDTH) )\n",
    "        # The first channel is assumed to represent the background class, if include_background=True.\n",
    "        #\n",
    "        # y (ground truth) is expected to be a multiple-channel tensor where each channel represents\n",
    "        # a class, and the grid corresponding to that channel has to be one-hot encoded (just 1s and 0s) representing when\n",
    "        # the corrresponding pixel is labeled with that class (1) or not (0).\n",
    "        # (I.e. y shape: (OUTPUT_CHANNELS, HEIGHT, WIDTH) )\n",
    "        # The first channel is assumed to represent the background class, if include_background=True.\n",
    "\n",
    "        self.dice_metric = DiceMetric(include_background=self.include_background_in_loss_and_metrics, reduction=\"mean\")\n",
    "\n",
    "        self.training_batch_losses_in_epoch = []  # Store losses for training batches in training epoch\n",
    "        self.training_batch_mean_ious_in_epoch = []  # Store mean_ious for training batches in training epoch\n",
    "        self.training_batch_dice_coeffs_in_epoch = []  # Store dice_coeffs for training batches in training epoch\n",
    "        self.training_batch_confusion_matrices_in_epoch = []  # Store confusion_matrices for training batches in training epoch\n",
    "\n",
    "        self.validation_batch_losses_in_epoch = []  # Store losses for validation batches in validation epoch\n",
    "        self.validation_batch_mean_ious_in_epoch = []  # Store mean_ious for validation batches in validation epoch\n",
    "        self.validation_batch_dice_coeffs_in_epoch = []  # Store dice_coeffs for validation batches in validation epoch\n",
    "        self.validation_batch_confusion_matrices_in_epoch = []  # Store confusion_matrices for validation batches in validation epoch\n",
    "\n",
    "    def forward(self, x):\n",
    "        pred = self.model(x)\n",
    "        return pred\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "\n",
    "        if self.enable_batch_logging_into_console and self.enable_training_epoch_logging_into_console:\n",
    "            print(f\"Training: Epoch {self.current_epoch + 1}, Batch {batch_idx}\")\n",
    "\n",
    "        x, y = batch # x shape: (batch_size, input_channels, height, width, depth), y shape: (batch_size, output_channels, height, width)\n",
    "        print(f\"Training Step - x.shape before model processing: {x.shape}\")\n",
    "        print(f\"Training Step - y.shape before model processing: {y.shape}\")\n",
    "        y_hat = self(x) # y_hat shape: (batch_size, output_channels, height, width, 1)\n",
    "        print(f\"Training Step - y_hat.shape after model prediction: {y_hat.shape}\")\n",
    "\n",
    "        # Conversion to probabilities with softmax ativation function (POST_PROCESSING)\n",
    "        # Manually apply softmax across the output_channels (dim=1)\n",
    "        y_hat_probabilties = F.softmax(y_hat, dim=1)\n",
    "\n",
    "        loss = self.loss_fn(y_hat_probabilties, y)\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True, batch_size=self.trainer.train_dataloader.batch_size)\n",
    "        self.training_batch_losses_in_epoch.append(loss)\n",
    "\n",
    "        # # We detach y_hat_probabilties because of they will no longer be used for the loss computation, so gradients don't need to\n",
    "        # # be computed and store for the next computations\n",
    "        # y_hat_probabilties = y_hat_probabilties.detach()\n",
    "\n",
    "        # # Confusion matrix computation\n",
    "        # conf_matrix = self.confusion_matrix_metric(y_pred=y_hat_probabilties, y=y)  # confusion_matrix_metric value order tensor([[[tp, fp, tn, fn]]])\n",
    "        # if self.enable_batch_logging_into_console and self.enable_training_epoch_logging_into_console:\n",
    "        #     print(f\"- Confusion Matrix (raw) (tensor([[[tp, fp, tn, fn]]])): {conf_matrix}\")\n",
    "\n",
    "        # # Normalization\n",
    "        # tp_fn_sum = conf_matrix[:, :, [0, 3]].sum(dim=-1, keepdim=True).clamp_min(1)  # TP + FN\n",
    "        # fp_tn_sum = conf_matrix[:, :, [1, 2]].sum(dim=-1, keepdim=True).clamp_min(1)  # FP + TN\n",
    "        # normalized_conf_matrix = conf_matrix / torch.cat([tp_fn_sum, fp_tn_sum, fp_tn_sum, tp_fn_sum], dim=-1)\n",
    "        # if self.enable_batch_logging_into_console and self.enable_training_epoch_logging_into_console:\n",
    "        #     print(f\"- Normalized Confusion Matrix (tensor([[[norm_tp, norm_fp, norm_tn, norm_fn]]])): {normalized_conf_matrix}\")\n",
    "\n",
    "        # self.training_batch_confusion_matrices_in_epoch.append(normalized_conf_matrix)\n",
    "\n",
    "        # # Mean IoU metric\n",
    "        # mean_iou_metric = self.mean_iou_metric(y_pred=y_hat_probabilties, y=y)\n",
    "        # # For each output class, there is a mean_iou between foregroung and background.\n",
    "        # # Therefore, here we compute the average between all output classes\n",
    "        # mean_iou_metric_avg_between_output_classes = mean_iou_metric.mean()\n",
    "        # self.log('val_mean_iou', mean_iou_metric_avg_between_output_classes, on_step=True, on_epoch=True, prog_bar=True, batch_size=self.trainer.val_dataloaders.batch_size)\n",
    "        # if self.enable_batch_logging_into_console and self.enable_training_epoch_logging_into_console:\n",
    "        #     print(f\"- Mean IoU (Mean between Foreground and Background)): {mean_iou_metric_avg_between_output_classes}\")\n",
    "\n",
    "        # self.training_batch_mean_ious_in_epoch.append(mean_iou_metric_avg_between_output_classes)\n",
    "\n",
    "        # # Dice Coefficient Metric\n",
    "        # dice_coeffcient_metric = self.dice_metric(y_pred=y_hat_probabilties, y=y)\n",
    "        # # For each output class, there is a mean_iou between foregroun and background.\n",
    "        # # Therefore, here we compute the average between all output classes\n",
    "        # dice_coeffcient_metric_avg_between_output_classes = dice_coeffcient_metric.mean()\n",
    "        # self.log('val_dice_coeff', dice_coeffcient_metric_avg_between_output_classes, on_step=True, on_epoch=True, prog_bar=True, batch_size=self.trainer.val_dataloaders.batch_size)\n",
    "        # if self.enable_batch_logging_into_console and self.enable_training_epoch_logging_into_console:\n",
    "        #     print(f\"- Dice Coefficent: {dice_coeffcient_metric_avg_between_output_classes}\")\n",
    "\n",
    "        # self.training_batch_dice_coeffs_in_epoch.append(dice_coeffcient_metric_avg_between_output_classes)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "\n",
    "        self.model.eval()  # Evaluation mode\n",
    "        \n",
    "        if self.enable_batch_logging_into_console and self.enable_validation_epoch_logging_into_console:\n",
    "            print(f\"Validation: Epoch {self.current_epoch + 1}, Batch {batch_idx}\")\n",
    "\n",
    "        x, y = batch # x format: (batch_size, dict_patches), y format: (batch_size, label_path)\n",
    "\n",
    "        losses_batch = [] \n",
    "\n",
    "        for sample_x, sample_y in zip(x, y): \n",
    "            label_ground_truth = torch.load(sample_y, weights_only=True) # Shape (out_classes, height, width)\n",
    "            y = label_ground_truth.unsqueeze(0) # Adding 1 dimension to match model output format (batch_size, out_classes, height, width)\n",
    "            # Create a tensor of the same shape with all values set to NaN\n",
    "            y_hat = torch.empty(label_ground_truth.shape).fill_(float('nan'))\n",
    "            patches_dict_info = sample_x\n",
    "            for patch_key in list(patches_dict_info.keys()):\n",
    "                patch_coordinates = patches_dict_info[patch_key][\"patch_coord\"]\n",
    "                patch_path = patches_dict_info[patch_key][\"patch_path\"]\n",
    "                patch_tensor = torch.load(patch_path, weights_only=True) # Shape (in_channels, height, width, depth)\n",
    "                patch_tensor = patch_tensor.unsqueeze(0) # Adding 1 dimension to match model input format (batch_size, in_channels, height, width, depth)\n",
    "                with torch.no_grad():\n",
    "                    patch_prediction = self(patch_tensor) # Model makes prediction over patch\n",
    "                for i in range(patch_coordinates[0], patch_coordinates[0]+self.patch_dimensions[0]):\n",
    "                    for j in range(patch_coordinates[1], patch_coordinates[1]+self.patch_dimensions[1]):\n",
    "                        is_pixel_without_prediction = torch.isnan(y_hat[:, :, i, j]).any()\n",
    "                        if is_pixel_without_prediction:\n",
    "                            y_hat[:, :, i, j] = patch_prediction[:, :, i-patch_coordinates[0], j-patch_coordinates[1]]\n",
    "                            \n",
    "            # Conversion to probabilities with softmax ativation function (POST_PROCESSING)\n",
    "            # Manually apply softmax across the output_channels (dim=1)\n",
    "            y_hat_probabilties = F.softmax(y_hat, dim=1)\n",
    "\n",
    "            loss = self.loss_fn(y_hat_probabilties, y)\n",
    "            losses_batch.append(loss)\n",
    "\n",
    "            # #############PLOTTING################\n",
    "            # Extract the ground truth and predicted probabilities for the second channel\n",
    "            ground_truth_channel = y[:, 1, :, :].squeeze()  # Shape: (height, width)\n",
    "            predicted_probabilities_channel = y_hat_probabilities[:, 1, :, :].squeeze()  # Shape: (height, width)\n",
    "        \n",
    "            # Create a figure to plot\n",
    "            fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "            fig.suptitle(\"Ground Truth vs Predicted Probabilities (Channel 2)\")\n",
    "        \n",
    "            # Ground truth plot\n",
    "            im1 = axs[0].imshow(ground_truth_channel.cpu().numpy(), cmap='viridis', interpolation='none')\n",
    "            axs[0].set_title(\"Ground Truth\")\n",
    "            axs[0].axis('off')\n",
    "            fig.colorbar(im1, ax=axs[0])\n",
    "        \n",
    "            # Predicted probabilities plot\n",
    "            im2 = axs[1].imshow(predicted_probabilities_channel.cpu().detach().numpy(), cmap='viridis', interpolation='none')\n",
    "            axs[1].set_title(\"Predicted Probabilities\")\n",
    "            axs[1].axis('off')\n",
    "            fig.colorbar(im2, ax=axs[1])\n",
    "        \n",
    "            # Show the plot\n",
    "            plt.show()\n",
    "\n",
    "            ########################################\n",
    "\n",
    "        average_loss_batch = np.mean(losses_batch)\n",
    "        self.log('val_loss', average_loss_batch, on_step=True, on_epoch=True, prog_bar=True, batch_size=self.trainer.val_dataloaders.batch_size)\n",
    "        if self.enable_batch_logging_into_console and self.enable_validation_epoch_logging_into_console:\n",
    "            print(f\"- Dice Loss: {average_loss_batch}\")\n",
    "            \n",
    "        # Return additional outputs (optional)\n",
    "        return {\"val_loss\": average_loss_batch}\n",
    "                    \n",
    "\n",
    "        ###############################################################################\n",
    "\n",
    "        # Step 2: (Optional) Convert class indices to one-hot format\n",
    "        # y_pred_one_hot = torch.nn.functional.one_hot(y_hat_indices, num_classes=y_hat_probabilties.shape[1])  # Shape: (batch_size, height, width, output_channels)\n",
    "        # y_pred_one_hot = y_pred_one_hot.permute(0, 3, 1, 2)  # Rearrange to (batch_size, output_channels, height, width)\n",
    "\n",
    "        # # Confusion matrix computation\n",
    "        # conf_matrix = self.confusion_matrix_metric(y_pred=y_hat_probabilties, y=y)  # confusion_matrix_metric value order tensor([[[tp, fp, tn, fn]]])\n",
    "        # if self.enable_batch_logging_into_console and self.enable_validation_epoch_logging_into_console:\n",
    "        #     print(f\"- Confusion Matrix (raw) (tensor([[[tp, fp, tn, fn]]])): {conf_matrix}\")\n",
    "\n",
    "        # # Normalization\n",
    "        # tp_fn_sum = conf_matrix[:, :, [0, 3]].sum(dim=-1, keepdim=True).clamp_min(1)  # TP + FN\n",
    "        # fp_tn_sum = conf_matrix[:, :, [1, 2]].sum(dim=-1, keepdim=True).clamp_min(1)  # FP + TN\n",
    "        # normalized_conf_matrix = conf_matrix / torch.cat([tp_fn_sum, fp_tn_sum, fp_tn_sum, tp_fn_sum], dim=-1)\n",
    "        # if self.enable_batch_logging_into_console and self.enable_validation_epoch_logging_into_console:\n",
    "        #     print(f\"- Normalized Confusion Matrix (tensor([[[norm_tp, norm_fp, norm_tn, norm_fn]]])): {normalized_conf_matrix}\")\n",
    "\n",
    "        # self.validation_batch_confusion_matrices_in_epoch.append(normalized_conf_matrix.detach())\n",
    "\n",
    "        # # Mean IoU metric\n",
    "        # mean_iou_metric = self.mean_iou_metric(y_pred=y_hat_probabilties, y=y)\n",
    "        # # For each output class, there is a mean_iou between foregroun and background.\n",
    "        # # Therefore, here we compute the average between all output classes\n",
    "        # mean_iou_metric_avg_between_output_classes = mean_iou_metric.mean()\n",
    "        # self.log('val_mean_iou', mean_iou_metric_avg_between_output_classes, on_step=True, on_epoch=True, prog_bar=True, batch_size=self.trainer.val_dataloaders.batch_size)\n",
    "        # if self.enable_batch_logging_into_console and self.enable_validation_epoch_logging_into_console:\n",
    "        #     print(f\"- Mean IoU (Mean between Foreground and Background)): {mean_iou_metric_avg_between_output_classes}\")\n",
    "        # self.validation_batch_mean_ious_in_epoch.append(mean_iou_metric_avg_between_output_classes)\n",
    "\n",
    "        # # Dice Coefficient Metric\n",
    "        # dice_coeffcient_metric = self.dice_metric(y_pred=y_hat_probabilties, y=y)\n",
    "        # # For each output class, there is a mean_iou between foregroun and background.\n",
    "        # # Therefore, here we compute the average between all output classes\n",
    "        # dice_coeffcient_metric_avg_between_output_classes = dice_coeffcient_metric.mean()\n",
    "        # self.log('val_dice_coeff', dice_coeffcient_metric_avg_between_output_classes, on_step=True, on_epoch=True, prog_bar=True, batch_size=self.trainer.val_dataloaders.batch_size)\n",
    "        # if self.enable_batch_logging_into_console and self.enable_validation_epoch_logging_into_console:\n",
    "        #     print(f\"- Dice Coefficent: {dice_coeffcient_metric_avg_between_output_classes}\")\n",
    "        # self.validation_batch_dice_coeffs_in_epoch.append(dice_coeffcient_metric_avg_between_output_classes)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "\n",
    "        x, y = batch # x shape: (batch_size, input_channels, height, width, depth), y shape: (batch_size, output_channels, height, width)\n",
    "\n",
    "        ###################### SLIDING WINDOW INFERER INSTANTIATION ###########################\n",
    "\n",
    "        # By using functools.partial, we are preconfiguring sliding_window_inference with specific\n",
    "        # arguments so that it can later be called with just the required input.\n",
    "\n",
    "        image_3D_spatial_dim = x.shape[2:] #height, width, depth\n",
    "        model_output_spatial_dim = list(x.shape[2:]) #height, width, depth\n",
    "        model_output_spatial_dim[2] = 1\n",
    "\n",
    "        adjusted_overlap = self.calculate_overlap(self.patch_dimensions, model_output_spatial_dim, image_3D_spatial_dim) #[overlap_height, overlap_width, overlap_depth]\n",
    "\n",
    "        model_inferer = partial(\n",
    "            sliding_window_inference,\n",
    "            roi_size= self.patch_dimensions, # Specifies the patch size to use for sliding window inference.\n",
    "            sw_batch_size=1, # Specifies how many patches to process simultaneously.\n",
    "            predictor=self.model, # The model used for predictions.\n",
    "            overlap = adjusted_overlap #The overlap between adjacent patches.\n",
    "        )\n",
    "\n",
    "        ######################################################################################\n",
    "\n",
    "        y_hat = model_inferer(x)  # y_hat shape: (batch_size, output_channels, height, width, 1)\n",
    "        y_hat = y_hat.squeeze(4) # y_hat shape: (batch_size, output_channels, height, width)\n",
    "\n",
    "        # Conversion to probabilities with softmax ativation function (POST_PROCESSING)\n",
    "        # Manually apply softmax across the output_channels (dim=1)\n",
    "        y_hat_probabilties = F.softmax(y_hat, dim=1)\n",
    "\n",
    "        loss = self.loss_fn(y_hat_probabilties, y)\n",
    "        self.log('val_loss', loss, on_step=True, on_epoch=True, prog_bar=True, batch_size=self.trainer.val_dataloaders.batch_size)\n",
    "        if self.enable_batch_logging_into_console and self.enable_validation_epoch_logging_into_console:\n",
    "            print(f\"- Dice Loss: {loss}\")\n",
    "        self.validation_batch_losses_in_epoch.append(loss)\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "\n",
    "    # Overriding hooks for demonstration\n",
    "    def on_train_epoch_start(self):\n",
    "\n",
    "        self.model.train()\n",
    "\n",
    "        if self.enable_training_epoch_logging_into_console:\n",
    "            # Start of an epoch\n",
    "            print(f\"{'=' * 40}\")\n",
    "            print(f\"Starting training epoch {self.current_epoch + 1}...\")\n",
    "            print(f\"{'=' * 40}\")\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        if self.enable_training_epoch_logging_into_console:\n",
    "            # End of an epoch\n",
    "            print(f\"{'=' * 40}\")\n",
    "            print(f\"Finished training epoch {self.current_epoch + 1}\")\n",
    "            print(f\"{'=' * 40}\")\n",
    "\n",
    "            # Average Dice Loss for the Epoch\n",
    "            average_loss_for_epoch = torch.stack(self.training_batch_losses_in_epoch).mean()\n",
    "            print(f\"==> Average Dice Loss for Training Epoch (Patch Level) {self.current_epoch + 1}: {average_loss_for_epoch:.6f}\")\n",
    "            print()\n",
    "\n",
    "            # # Average Normalized Confusion Matrix for the Epoch\n",
    "            # avg_normalized_conf_matrix = torch.stack(self.training_batch_confusion_matrices_in_epoch).mean(dim=0)\n",
    "            # print(f\"==> Average Normalized Confusion Matrix for Training Epoch {self.current_epoch + 1}:\")\n",
    "            # print(avg_normalized_conf_matrix)\n",
    "            # print(\"**Format: (tensor([[[norm_tp, norm_fp, norm_tn, norm_fn]]]))\")\n",
    "            # print()\n",
    "\n",
    "            # # Average Mean IoU for the Epoch\n",
    "            # average_mean_iou_for_epoch = torch.stack(self.training_batch_mean_ious_in_epoch).mean()\n",
    "            # print(f\"==> Average Mean IoU (Mean between Foreground and Background) for Training Epoch {self.current_epoch + 1}: {average_mean_iou_for_epoch:.6f}\")\n",
    "            # print()\n",
    "\n",
    "            # # Average Dice Coefficient for the Epoch\n",
    "            # average_dice_coeff_for_epoch = torch.stack(self.training_batch_dice_coeffs_in_epoch).mean()\n",
    "            # print(f\"==> Average Dice Coefficient for Training Epoch {self.current_epoch + 1}: {average_dice_coeff_for_epoch:.6f}\")\n",
    "            # print(f\"{'=' * 40}\")\n",
    "\n",
    "        # Clearing lists for the next epoch\n",
    "        self.training_batch_losses_in_epoch.clear()\n",
    "        # self.training_batch_confusion_matrices_in_epoch.clear()\n",
    "        # self.training_batch_mean_ious_in_epoch.clear()\n",
    "        # self.training_batch_dice_coeffs_in_epoch.clear()\n",
    "\n",
    "\n",
    "    def on_validation_epoch_start(self):\n",
    "\n",
    "        self.model.eval()\n",
    "\n",
    "        if self.enable_validation_epoch_logging_into_console:\n",
    "            # Start of an epoch\n",
    "            print(f\"{'=' * 40}\")\n",
    "            print(f\"Starting validation epoch ...\")\n",
    "            print(f\"{'=' * 40}\")\n",
    "\n",
    "    def on_validation_epoch_end(self, outputs):\n",
    "        if self.enable_validation_epoch_logging_into_console:\n",
    "            # End of an epoch\n",
    "            print(f\"{'=' * 40}\")\n",
    "            print(f\"Finished validation epoch\")\n",
    "            print(f\"{'=' * 40}\")\n",
    "\n",
    "            # Average Dice Loss for the Epoch\n",
    "            # Optional: Aggregate outputs returned by validation_step\n",
    "            avg_loss = torch.stack([x[\"val_loss\"] for x in outputs]).mean()\n",
    "            print(f\"==> Average Dice Loss for Validation Epoch {self.current_epoch + 1}: {avg_loss:.6f}\")\n",
    "            print()\n",
    "\n",
    "            # # Average Normalized Confusion Matrix for the Epoch\n",
    "            # avg_normalized_conf_matrix = torch.stack(self.validation_batch_confusion_matrices_in_epoch).mean(dim=0)\n",
    "            # print(f\"==> Average Normalized Confusion Matrix for Validation Epoch {self.current_epoch + 1}:\")\n",
    "            # print(avg_normalized_conf_matrix)\n",
    "            # print(\"**Format: (tensor([[[norm_tp, norm_fp, norm_tn, norm_fn]]]))\")\n",
    "            # print()\n",
    "\n",
    "            # # Average Mean IoU for the Epoch\n",
    "            # average_mean_iou_for_epoch = torch.stack(self.validation_batch_mean_ious_in_epoch).mean()\n",
    "            # print(f\"==> Average Mean IoU (Mean between Foreground and Background) for Validation Epoch {self.current_epoch + 1}: {average_mean_iou_for_epoch:.6f}\")\n",
    "            # print()\n",
    "\n",
    "            # # Average Dice Coefficient for the Epoch\n",
    "            # average_dice_coeff_for_epoch = torch.stack(self.validation_batch_dice_coeffs_in_epoch).mean()\n",
    "            # print(f\"==> Average Dice Coefficient for Validation Epoch {self.current_epoch + 1}: {average_dice_coeff_for_epoch:.6f}\")\n",
    "            # print(f\"{'=' * 40}\")\n",
    "\n",
    "            # Clearing lists for the next epoch\n",
    "            self.validation_batch_losses_in_epoch.clear()\n",
    "            self.validation_batch_confusion_matrices_in_epoch.clear()\n",
    "            self.validation_batch_mean_ious_in_epoch.clear()\n",
    "            self.validation_batch_dice_coeffs_in_epoch.clear()\n",
    "\n",
    "\n",
    "    def on_test_epoch_start(self):\n",
    "\n",
    "        self.model.eval()\n",
    "\n",
    "        # Start of an epoch\n",
    "        print(f\"{'=' * 40}\")\n",
    "        print(f\"Starting testing epoch...\")\n",
    "        print(f\"{'=' * 40}\")\n",
    "\n",
    "        ## STILL TO DO ##\n",
    "        ## STILL TO DO ##\n",
    "        ## STILL TO DO ##\n",
    "\n",
    "\n",
    "    def on_test_epoch_end(self):\n",
    "        # End of an epoch\n",
    "        print(f\"{'=' * 40}\")\n",
    "        print(f\"Finished testing epoch\")\n",
    "        print(f\"{'=' * 40}\")\n",
    "\n",
    "        ## STILL TO DO ##\n",
    "        ## STILL TO DO ##\n",
    "        ## STILL TO DO ##\n",
    "\n",
    "    # Function to compute the smallest valid overlap value\n",
    "    def calculate_overlap(self, model_input_dims, model_output_dims, image_spatial_dim):\n",
    "        overlap_values = []\n",
    "        for in_dim_model, out_dim_model, dim_image in zip(model_input_dims,model_output_dims,image_spatial_dim):\n",
    "            smallest_overlap = None\n",
    "            zoom_scale = out_dim_model / in_dim_model\n",
    "            for stride in range(1, in_dim_model + 1):  # Stride must be <= model_input_spatial_dim\n",
    "                if (dim_image - in_dim_model) % stride == 0:\n",
    "                    overlap = 1 - (stride / in_dim_model)\n",
    "                    if 0 <= overlap < 1 and (overlap*in_dim_model*zoom_scale).is_integer():\n",
    "                        if smallest_overlap is None or overlap < smallest_overlap:\n",
    "                            smallest_overlap = overlap\n",
    "            if smallest_overlap is None:\n",
    "                raise ValueError(f\"No possible overlap value for dimension with model_input_spatial_dim={in_dim_model} \"\n",
    "                                f\"and image_spatial_dim={dim_image}.\")\n",
    "            overlap_values.append(smallest_overlap)\n",
    "        return overlap_values\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "output_embedded_package_id": "1aKglJu6CDXRmPGoVZj_HfcuHio6Ss53x"
    },
    "executionInfo": {
     "elapsed": 124089,
     "status": "ok",
     "timestamp": 1732788821976,
     "user": {
      "displayName": "AdriÃ¡n De Miguel Palacio",
      "userId": "14927901167627228922"
     },
     "user_tz": -60
    },
    "id": "UKS8T-C6tvXs",
    "outputId": "fc3efb75-d937-4cfb-f935-3f3dd02cbb70"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The directory '/home/adrian/AIRT_Segmentation_Project/experimenting/data/train_data/preprocessed_files' exists.\n",
      "The json file '/home/adrian/AIRT_Segmentation_Project/experimenting/data/train_data/preprocessed_files/preprocessed_info.json' exists.\n",
      "The label and measurement corresponding to 'R_003' sample have already been processed at least once.\n",
      "The measurement corresponding to 'R_003' sample have already been processed with depth [128] at least once.\n",
      "The label corresponding to 'R_003' sample have already been processed with patch size [64x64] at least once.\n",
      "The measurement corresponding to 'R_003' sample have already been processed with depth [128] and patch size [64x64] at least once.\n",
      "The label corresponding to 'R_003' sample have already been processed with patch size [64x64] and overlap [0_25x0_25]\n",
      "The measurement corresponding to 'R_003' sample have already been processed with depth [128], patch size [64x64] and overlap [0_25x0_25]\n",
      "The label and measurement corresponding to 'Z_003' sample have already been processed at least once.\n",
      "The measurement corresponding to 'Z_003' sample have already been processed with depth [128] at least once.\n",
      "The label corresponding to 'Z_003' sample have already been processed with patch size [64x64] at least once.\n",
      "The measurement corresponding to 'Z_003' sample have already been processed with depth [128] and patch size [64x64] at least once.\n",
      "The label corresponding to 'Z_003' sample have already been processed with patch size [64x64] and overlap [0_25x0_25]\n",
      "The measurement corresponding to 'Z_003' sample have already been processed with depth [128], patch size [64x64] and overlap [0_25x0_25]\n",
      "The JSON file '/home/adrian/AIRT_Segmentation_Project/experimenting/data/train_data/preprocessed_files/preprocessed_info.json' has been updated.\n",
      "The directory '/home/adrian/AIRT_Segmentation_Project/experimenting/data/train_data/preprocessed_files' exists.\n",
      "The json file '/home/adrian/AIRT_Segmentation_Project/experimenting/data/train_data/preprocessed_files/preprocessed_info.json' exists.\n",
      "The label and measurement corresponding to 'R_002' sample have already been processed at least once.\n",
      "The measurement corresponding to 'R_002' sample have already been processed with depth [128] at least once.\n",
      "The label corresponding to 'R_002' sample have already been processed with patch size [64x64] at least once.\n",
      "The measurement corresponding to 'R_002' sample have already been processed with depth [128] and patch size [64x64] at least once.\n",
      "The label corresponding to 'R_002' sample have already been processed with patch size [64x64] and overlap [0_00x0_00]\n",
      "The measurement corresponding to 'R_002' sample have already been processed with depth [128], patch size [64x64] and overlap [0_00x0_00]\n",
      "The label and measurement corresponding to 'Z_002' sample have already been processed at least once.\n",
      "The measurement corresponding to 'Z_002' sample have already been processed with depth [128] at least once.\n",
      "The label corresponding to 'Z_002' sample have already been processed with patch size [64x64] at least once.\n",
      "The measurement corresponding to 'Z_002' sample have already been processed with depth [128] and patch size [64x64] at least once.\n",
      "The label corresponding to 'Z_002' sample have already been processed with patch size [64x64] and overlap [0_00x0_00]\n",
      "The measurement corresponding to 'Z_002' sample have already been processed with depth [128], patch size [64x64] and overlap [0_00x0_00]\n",
      "The JSON file '/home/adrian/AIRT_Segmentation_Project/experimenting/data/train_data/preprocessed_files/preprocessed_info.json' has been updated.\n"
     ]
    }
   ],
   "source": [
    "metadata_dict_with_files_selected_training = {\n",
    "    \"R_003\": {\n",
    "        \"3D_thermal_sequence_filename\": \"R_003.mat\",\n",
    "        \"label_filename\": \"R_003_binary.png\",\n",
    "        \"stratified_group\": \"A\",\n",
    "        \"ROI\": {\n",
    "          \"all_points_x\": [\n",
    "            75,\n",
    "            80,\n",
    "            250,\n",
    "            255,\n",
    "            75\n",
    "          ],\n",
    "          \"all_points_y\": [\n",
    "            32,\n",
    "            215,\n",
    "            214,\n",
    "            31,\n",
    "            32\n",
    "          ]\n",
    "        }\n",
    "    },\n",
    "    \"Z_003\": {\n",
    "        \"3D_thermal_sequence_filename\": \"Z_003.mat\",\n",
    "        \"label_filename\": \"Z_003_binary.png\",\n",
    "        \"stratified_group\": \"B\",\n",
    "        \"ROI\": {\n",
    "          \"all_points_x\": [\n",
    "            79,\n",
    "            84,\n",
    "            255,\n",
    "            259,\n",
    "            79\n",
    "          ],\n",
    "          \"all_points_y\": [\n",
    "            32,\n",
    "            215,\n",
    "            215,\n",
    "            32,\n",
    "            32\n",
    "          ]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "metadata_dict_with_files_selected_validation = {\n",
    "    \"R_002\": {\n",
    "        \"3D_thermal_sequence_filename\": \"R_002.mat\",\n",
    "        \"label_filename\": \"R_002_binary.png\",\n",
    "        \"stratified_group\": \"A\",\n",
    "        \"ROI\": {\n",
    "          \"all_points_x\": [\n",
    "            74,\n",
    "            80,\n",
    "            250,\n",
    "            254,\n",
    "            74\n",
    "          ],\n",
    "          \"all_points_y\": [\n",
    "            32,\n",
    "            214,\n",
    "            214,\n",
    "            32,\n",
    "            32\n",
    "          ]\n",
    "        }\n",
    "    },\n",
    "    \"Z_002\": {\n",
    "       \"3D_thermal_sequence_filename\": \"Z_002.mat\",\n",
    "       \"label_filename\": \"Z_002_binary.png\",\n",
    "       \"stratified_group\": \"B\",\n",
    "       \"ROI\": {\n",
    "         \"all_points_x\": [\n",
    "            77,\n",
    "            83,\n",
    "            255,\n",
    "            258,\n",
    "            77\n",
    "          ],\n",
    "          \"all_points_y\": [\n",
    "            30,\n",
    "            214,\n",
    "            214,\n",
    "            32,\n",
    "            30\n",
    "          ]\n",
    "        }\n",
    "    }\n",
    "\n",
    "}\n",
    "\n",
    "data_dir = \"data/train_data\"\n",
    "patch_dims = (64,64,128)\n",
    "overlap = [0.25,0.35] #e.g. [0.25,0.35] -> 0.25 overlap in the height dimension & 0.35 overlap in the width one\n",
    "overlap_inference = [0,0]\n",
    "\n",
    "def custom_collate(batch):\n",
    "    # Collect x (dictionaries) into a list\n",
    "    batch_x = [item[0] for item in batch]    \n",
    "    # Collect y (strings) into a list\n",
    "    batch_y = [item[1] for item in batch]\n",
    "    \n",
    "    return batch_x, batch_y\n",
    "\n",
    "# Instantiate the dataset\n",
    "train_dataset = SwinUnetr3DDataset(\n",
    "    is_inference_mode = False,\n",
    "    metadata_dict_with_files_selected=metadata_dict_with_files_selected_training,\n",
    "    data_dir=data_dir,\n",
    "    model_input_dim=patch_dims,\n",
    "    overlap = overlap\n",
    ")\n",
    "\n",
    "val_dataset = SwinUnetr3DDataset(\n",
    "    is_inference_mode = True,\n",
    "    metadata_dict_with_files_selected=metadata_dict_with_files_selected_validation,\n",
    "    data_dir=data_dir,\n",
    "    model_input_dim=patch_dims,\n",
    "    overlap = overlap_inference\n",
    ")\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, collate_fn=custom_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu124"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VersiÃ³n de PyTorch: 2.5.1+cu124\n",
      "VersiÃ³n de CUDA utilizada por PyTorch: 12.4\n",
      "Â¿CUDA estÃ¡ disponible?: True\n",
      "Nombre de la GPU: NVIDIA GeForce RTX 2080 Ti\n",
      "NÃºmero de GPUs disponibles: 1\n",
      "VersiÃ³n de CUDA: 12.4\n",
      "VersiÃ³n de cuDNN: 90100\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"VersiÃ³n de PyTorch:\", torch.__version__)\n",
    "print(\"VersiÃ³n de CUDA utilizada por PyTorch:\", torch.version.cuda)\n",
    "\n",
    "# Verifica si CUDA estÃ¡ disponible\n",
    "print(\"Â¿CUDA estÃ¡ disponible?:\", torch.cuda.is_available())\n",
    "\n",
    "# Si estÃ¡ disponible, obtÃ©n mÃ¡s detalles\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Nombre de la GPU:\", torch.cuda.get_device_name(0))\n",
    "    print(\"NÃºmero de GPUs disponibles:\", torch.cuda.device_count())\n",
    "    print(\"VersiÃ³n de CUDA:\", torch.version.cuda)\n",
    "    print(\"VersiÃ³n de cuDNN:\", torch.backends.cudnn.version())\n",
    "else:\n",
    "    print(\"CUDA no estÃ¡ configurado correctamente o no hay GPU compatible.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(\"aten::slow_conv3d_forward\" in dir(torch.ops.aten))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([1, 8, 16, 16, 16])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "input = torch.rand(1, 3, 16, 16, 16).cuda()\n",
    "weight = torch.rand(8, 3, 3, 3, 3).cuda()\n",
    "bias = torch.rand(8).cuda()\n",
    "\n",
    "output = F.conv3d(input, weight, bias, stride=1, padding=1)\n",
    "print(\"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "executionInfo": {
     "elapsed": 1374,
     "status": "error",
     "timestamp": 1732723111492,
     "user": {
      "displayName": "AdriÃ¡n De Miguel Palacio",
      "userId": "14927901167627228922"
     },
     "user_tz": -60
    },
    "id": "nY8E6ofC3-XQ",
    "outputId": "5ddcc094-ef9e-48ea-f6a9-46275f3a0f87"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All dimensions are divisible by 32\n",
      "Using accelerator: <pytorch_lightning.accelerators.cuda.CUDAAccelerator object at 0x7f8557272ef0>\n",
      "Number of devices used: 1\n"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Model checkpoint callback\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='val_loss',\n",
    "    dirpath='checkpoints',\n",
    "    filename='best-checkpoint-{epoch:02d}-{val_loss:.2f}',\n",
    "    save_top_k=1,\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "# Early stopping callback\n",
    "early_stopping_callback = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=4,\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "model = SwinUNETR_AIRT_LightningModel(patch_dimensions = patch_dims)\n",
    "trainer = Trainer(\n",
    "    max_epochs=10,\n",
    "    callbacks=[checkpoint_callback, early_stopping_callback],\n",
    "    log_every_n_steps=1 # log every n batches\n",
    ")\n",
    "\n",
    "print(f\"Using accelerator: {trainer.accelerator}\")\n",
    "print(f\"Number of devices used: {trainer.num_devices}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "618354f79b604413b52341bce1850d46",
      "103d962ab53d41f8b17dfdb5076853a9",
      "3d31582240dd4d85b4945c28c40520bc",
      "0eb73d039c04409992bcf27b6dc1f8c6",
      "707d0082b1b04739abfb43bc02f8d459",
      "1d6f534e8e0b497e845500c9ee0bd053",
      "fbee2d5ebb1f46adaf61a101dd302efe",
      "7d821e544ea449ffa5c17eb2db8b86b6",
      "432be9bd650c4538a5a60a0c2ab7c818",
      "7d081968a9fa4c0e915751a1f6930f8e",
      "2c09a3793100498c8e8958a9f682bffd",
      "eade6eb8b252415ba3e0835969c05473",
      "3f174bc8d83e47fb87bb672f53473fa8",
      "3db97f9cb00f4a65b0ef6a5f1cbcb2dc",
      "1d296410613446c0b467f02f07050416",
      "73779b80d7b142339ed56787718cd25e",
      "f6b717443fbb41b589ef1ac464fb179b",
      "2038b91e978c46c2a169006864ef1171",
      "58de0fb02b844035ba08236ab120b64b",
      "5d1f5f1867e646b79a57375272e68f2e",
      "703beecc5b8f485ea6544962bbf94dbd",
      "7641561cb6d440d3b56638ba6ec4771e"
     ]
    },
    "id": "cvDjmick3moF",
    "outputId": "a6b386af-fef7-48a7-947f-963246e66873"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type           | Params | Mode \n",
      "---------------------------------------------------\n",
      "0 | model   | SwinUNETR_AIRT | 15.7 M | train\n",
      "1 | loss_fn | DiceLoss       | 0      | train\n",
      "---------------------------------------------------\n",
      "15.7 M    Trainable params\n",
      "0         Non-trainable params\n",
      "15.7 M    Total params\n",
      "62.813    Total estimated model params size (MB)\n",
      "273       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ea1fe9de5da49609a68efa4b0cdc3fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |                                                                                            â¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Starting validation epoch ...\n",
      "========================================\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'SwinUNETR_AIRT' object has no attribute 'device'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Fit the model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# When you call trainer.fit(), by default, PyTorch Lightning performs a \"sanity check\" on the validation data\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# before the first training epoch starts. This sanity check typically involves running a few batches\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# (usually two by default) of the validation data to ensure that:\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# 1. There are no issues with the validation loop or dataset.\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# 2. The model can perform a forward and backward pass without errors.\u001b[39;00m\n",
      "File \u001b[0;32m~/AIRT_Segmentation_Project/experimenting/.env_experimenting/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:538\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 538\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    539\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/AIRT_Segmentation_Project/experimenting/.env_experimenting/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:47\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     50\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m~/AIRT_Segmentation_Project/experimenting/.env_experimenting/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:574\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    568\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    570\u001b[0m     ckpt_path,\n\u001b[1;32m    571\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    572\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    573\u001b[0m )\n\u001b[0;32m--> 574\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/AIRT_Segmentation_Project/experimenting/.env_experimenting/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:981\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    976\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_connector\u001b[38;5;241m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m    978\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    979\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    980\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 981\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    983\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    985\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    986\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/AIRT_Segmentation_Project/experimenting/.env_experimenting/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1023\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1021\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[1;32m   1022\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m isolate_rng():\n\u001b[0;32m-> 1023\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_sanity_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1024\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[1;32m   1025\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_loop\u001b[38;5;241m.\u001b[39mrun()\n",
      "File \u001b[0;32m~/AIRT_Segmentation_Project/experimenting/.env_experimenting/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1052\u001b[0m, in \u001b[0;36mTrainer._run_sanity_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1049\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_sanity_check_start\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1051\u001b[0m \u001b[38;5;66;03m# run eval step\u001b[39;00m\n\u001b[0;32m-> 1052\u001b[0m \u001b[43mval_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1054\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_sanity_check_end\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1056\u001b[0m \u001b[38;5;66;03m# reset logger connector\u001b[39;00m\n",
      "File \u001b[0;32m~/AIRT_Segmentation_Project/experimenting/.env_experimenting/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:178\u001b[0m, in \u001b[0;36m_no_grad_context.<locals>._decorator\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    176\u001b[0m     context_manager \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mno_grad\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context_manager():\n\u001b[0;32m--> 178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/AIRT_Segmentation_Project/experimenting/.env_experimenting/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py:135\u001b[0m, in \u001b[0;36m_EvaluationLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mis_last_batch \u001b[38;5;241m=\u001b[39m data_fetcher\u001b[38;5;241m.\u001b[39mdone\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;66;03m# run step hooks\u001b[39;00m\n\u001b[0;32m--> 135\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;66;03m# this needs to wrap the `*_step` call too (not just `next`) for `dataloader_iter` support\u001b[39;00m\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/AIRT_Segmentation_Project/experimenting/.env_experimenting/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py:396\u001b[0m, in \u001b[0;36m_EvaluationLoop._evaluation_step\u001b[0;34m(self, batch, batch_idx, dataloader_idx, dataloader_iter)\u001b[0m\n\u001b[1;32m    390\u001b[0m hook_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_step\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mtesting \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation_step\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    391\u001b[0m step_args \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_step_args_from_hook_kwargs(hook_kwargs, hook_name)\n\u001b[1;32m    393\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m using_dataloader_iter\n\u001b[1;32m    394\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m (dataloader_iter,)\n\u001b[1;32m    395\u001b[0m )\n\u001b[0;32m--> 396\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mstep_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mincrement_processed()\n\u001b[1;32m    400\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m using_dataloader_iter:\n\u001b[1;32m    401\u001b[0m     \u001b[38;5;66;03m# update the hook kwargs now that the step method might have consumed the iterator\u001b[39;00m\n",
      "File \u001b[0;32m~/AIRT_Segmentation_Project/experimenting/.env_experimenting/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:319\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 319\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    322\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m~/AIRT_Segmentation_Project/experimenting/.env_experimenting/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py:411\u001b[0m, in \u001b[0;36mStrategy.validation_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    409\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module:\n\u001b[1;32m    410\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_redirection(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation_step\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 411\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlightning_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[24], line 187\u001b[0m, in \u001b[0;36mSwinUNETR_AIRT_LightningModel.validation_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalidation_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, batch_idx):\n\u001b[0;32m--> 187\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m) \n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39meval()  \u001b[38;5;66;03m# Evaluation mode\u001b[39;00m\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menable_batch_logging_into_console \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menable_validation_epoch_logging_into_console:\n",
      "File \u001b[0;32m~/AIRT_Segmentation_Project/experimenting/.env_experimenting/lib/python3.10/site-packages/torch/nn/modules/module.py:1931\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1929\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1930\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1931\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   1932\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1933\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'SwinUNETR_AIRT' object has no attribute 'device'"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "trainer.fit(model, train_loader, val_loader)\n",
    "# When you call trainer.fit(), by default, PyTorch Lightning performs a \"sanity check\" on the validation data\n",
    "# before the first training epoch starts. This sanity check typically involves running a few batches\n",
    "# (usually two by default) of the validation data to ensure that:\n",
    "#\n",
    "# 1. There are no issues with the validation loop or dataset.\n",
    "# 2. The model can perform a forward and backward pass without errors."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyN90/alqYH/AUG/iw9MruM/",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0eb73d039c04409992bcf27b6dc1f8c6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7d081968a9fa4c0e915751a1f6930f8e",
      "placeholder": "â",
      "style": "IPY_MODEL_2c09a3793100498c8e8958a9f682bffd",
      "value": "â1/1â[05:47&lt;00:00,ââ0.00it/s]"
     }
    },
    "103d962ab53d41f8b17dfdb5076853a9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1d6f534e8e0b497e845500c9ee0bd053",
      "placeholder": "â",
      "style": "IPY_MODEL_fbee2d5ebb1f46adaf61a101dd302efe",
      "value": "SanityâCheckingâDataLoaderâ0:â100%"
     }
    },
    "1d296410613446c0b467f02f07050416": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_703beecc5b8f485ea6544962bbf94dbd",
      "placeholder": "â",
      "style": "IPY_MODEL_7641561cb6d440d3b56638ba6ec4771e",
      "value": "â1/18â[00:46&lt;13:09,ââ0.02it/s,âv_num=6,âtrain_loss_step=0.687]"
     }
    },
    "1d6f534e8e0b497e845500c9ee0bd053": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2038b91e978c46c2a169006864ef1171": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2c09a3793100498c8e8958a9f682bffd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3d31582240dd4d85b4945c28c40520bc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7d821e544ea449ffa5c17eb2db8b86b6",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_432be9bd650c4538a5a60a0c2ab7c818",
      "value": 1
     }
    },
    "3db97f9cb00f4a65b0ef6a5f1cbcb2dc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_58de0fb02b844035ba08236ab120b64b",
      "max": 18,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_5d1f5f1867e646b79a57375272e68f2e",
      "value": 1
     }
    },
    "3f174bc8d83e47fb87bb672f53473fa8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f6b717443fbb41b589ef1ac464fb179b",
      "placeholder": "â",
      "style": "IPY_MODEL_2038b91e978c46c2a169006864ef1171",
      "value": "Epochâ0:âââ6%"
     }
    },
    "432be9bd650c4538a5a60a0c2ab7c818": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "58de0fb02b844035ba08236ab120b64b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": "2",
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5d1f5f1867e646b79a57375272e68f2e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "618354f79b604413b52341bce1850d46": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_103d962ab53d41f8b17dfdb5076853a9",
       "IPY_MODEL_3d31582240dd4d85b4945c28c40520bc",
       "IPY_MODEL_0eb73d039c04409992bcf27b6dc1f8c6"
      ],
      "layout": "IPY_MODEL_707d0082b1b04739abfb43bc02f8d459"
     }
    },
    "703beecc5b8f485ea6544962bbf94dbd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "707d0082b1b04739abfb43bc02f8d459": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "inline-flex",
      "flex": null,
      "flex_flow": "row wrap",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": "hidden",
      "width": "100%"
     }
    },
    "73779b80d7b142339ed56787718cd25e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "inline-flex",
      "flex": null,
      "flex_flow": "row wrap",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "100%"
     }
    },
    "7641561cb6d440d3b56638ba6ec4771e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7d081968a9fa4c0e915751a1f6930f8e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7d821e544ea449ffa5c17eb2db8b86b6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": "2",
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "eade6eb8b252415ba3e0835969c05473": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3f174bc8d83e47fb87bb672f53473fa8",
       "IPY_MODEL_3db97f9cb00f4a65b0ef6a5f1cbcb2dc",
       "IPY_MODEL_1d296410613446c0b467f02f07050416"
      ],
      "layout": "IPY_MODEL_73779b80d7b142339ed56787718cd25e"
     }
    },
    "f6b717443fbb41b589ef1ac464fb179b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fbee2d5ebb1f46adaf61a101dd302efe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
