{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kicCnhfBzgmD"
   },
   "source": [
    "# DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G0ptulfx4DSF",
    "outputId": "e73dae6c-5d9c-4f87-d923-0bfdc5d1117b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All imports work correctly!\n"
     ]
    }
   ],
   "source": [
    "# Test imports\n",
    "import os\n",
    "import json\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from torch.utils.data import DataLoader, random_split, Dataset\n",
    "from scipy import io\n",
    "import matplotlib.image as mpimg\n",
    "from scipy.ndimage import gaussian_filter\n",
    "import torch\n",
    "from matplotlib.path import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from monai.transforms import CropForeground, SpatialCrop\n",
    "import numpy as np\n",
    "import math\n",
    "from itertools import product\n",
    "\n",
    "print(\"All imports work correctly!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "fVSVmLI3uuYC"
   },
   "outputs": [],
   "source": [
    "# Define a custom 3D dataset for testing purposes\n",
    "class SwinUnetr3DDataset(Dataset):\n",
    "    def __init__(self, is_inference_mode: bool, metadata_dict_with_files_selected: dict, data_dir: str, model_input_dim: tuple, overlap):\n",
    "        \"\"\"\n",
    "        Initializes the class instance with the provided parameters.\n",
    "\n",
    "        Parameters:\n",
    "        metadata_dict_with_files_selected (dict): Dictionary containing the selected files for the dataset with their metadata (e.g., 3D_thermal_sequence_filename, label_filename, ROI, stratified group, etc.).\n",
    "        data_dir (str): Directory where the data is located.\n",
    "        model_input_dim (tuple): 3D tuple indicating the dimensions of the model input (height, width, depth).\n",
    "        \"\"\"\n",
    "\n",
    "        self.metadata_dict_with_files_selected = metadata_dict_with_files_selected\n",
    "        self.data_dir = data_dir\n",
    "\n",
    "        self.is_inference_mode = is_inference_mode # e.g. False\n",
    "\n",
    "        self.model_input_dim = model_input_dim #e.g. (64,64,128)\n",
    "\n",
    "        self.overlap = overlap #e.g. [0.25,0.35]\n",
    "        formatted_overlap_dim_0 = f\"{self.overlap[0]:.2f}\"\n",
    "        formatted_overlap_dim_1 = f\"{self.overlap[1]:.2f}\"\n",
    "\n",
    "        self.patch_size = f\"{self.model_input_dim[0]}x{self.model_input_dim[1]}\" #e.g. 64x64\n",
    "        self.overlap_key = f\"{formatted_overlap_dim_0.replace('.', '_')}x{formatted_overlap_dim_1.replace('.', '_')}\" #e.g. 0_25x0_35\n",
    "\n",
    "        self.preprocessed_dir = os.path.join(os.getcwd(), self.data_dir, \"preprocessed_files\")\n",
    "        self.preprocessed_info_json_path = os.path.join(self.preprocessed_dir, \"preprocessed_info.json\")\n",
    "        self.preprocessed_info_dict = None\n",
    "\n",
    "        self.preprocessed_patches_dataset = []\n",
    "\n",
    "        # ########################### PREPROCESSING (AND DISK SAVING) ###########################\n",
    "\n",
    "        if os.path.isdir(self.preprocessed_dir):\n",
    "            print(f\"The directory '{self.preprocessed_dir}' exists.\")\n",
    "        else:\n",
    "            os.makedirs(self.preprocessed_dir)\n",
    "            print(f\"The directory '{self.preprocessed_dir}' did not exist and has been created.\")\n",
    "\n",
    "        if os.path.isfile(self.preprocessed_info_json_path):\n",
    "            print(f\"The json file '{self.preprocessed_info_json_path}' exists.\")\n",
    "            self.preprocessed_info_dict = self.load_preprocessed_info()\n",
    "        else:\n",
    "            self.preprocessed_info_dict = {}\n",
    "\n",
    "\n",
    "        for sample_id, sample_metadata in self.metadata_dict_with_files_selected.items():\n",
    "            # Checks\n",
    "            if sample_id in self.preprocessed_info_dict:\n",
    "                print(f\"The label and measurement corresponding to '{sample_id}' sample have already been processed at least once.\")\n",
    "            else:\n",
    "                # Create directory for the sample\n",
    "                preprocessed_sample_dir = os.path.join(self.preprocessed_dir, f\"{sample_id}\")\n",
    "                os.makedirs(preprocessed_sample_dir)\n",
    "                print(f\"The directory '{preprocessed_sample_dir}' did not exist and has been created.\")\n",
    "                preprocessed_sample_label_dir = os.path.join(preprocessed_sample_dir, \"label\")\n",
    "                os.makedirs(preprocessed_sample_label_dir)\n",
    "                print(f\"The directory '{preprocessed_sample_label_dir}' did not exist and has been created.\")\n",
    "                preprocessed_sample_label_patches_dir = os.path.join(preprocessed_sample_dir, \"label\", \"patches\")\n",
    "                os.makedirs(preprocessed_sample_label_patches_dir)\n",
    "                print(f\"The directory '{preprocessed_sample_label_patches_dir}' did not exist and has been created.\")\n",
    "                preprocessed_sample_measurement_dir = os.path.join(preprocessed_sample_dir, \"measurement\")\n",
    "                os.makedirs(preprocessed_sample_measurement_dir)\n",
    "                print(f\"The directory '{preprocessed_sample_measurement_dir}' did not exist and has been created.\")\n",
    "                # Create and save the cropped label in the directory just created\n",
    "                cropped_label_tensor, cropped_bbox_info = self.load_and_crop_label(sample_id, sample_metadata)\n",
    "                cropped_label_tensor_filename = os.path.join(preprocessed_sample_label_dir,f\"{sample_id}_cropped_label.pt\")\n",
    "                torch.save(cropped_label_tensor, cropped_label_tensor_filename)\n",
    "                # Update the information in the preprocessed_info_json\n",
    "                self.update_dictionary(self.preprocessed_info_dict, sample_id, {})\n",
    "                self.update_dictionary(self.preprocessed_info_dict, sample_id, \"measurement\" ,{})\n",
    "                self.update_dictionary(self.preprocessed_info_dict, sample_id, \"label\" ,{})\n",
    "                self.update_dictionary(self.preprocessed_info_dict, sample_id, \"cropped_bbox_info\", cropped_bbox_info)\n",
    "                self.update_dictionary(self.preprocessed_info_dict, sample_id, \"label\" ,\"cropped_label_tensor_filename\", cropped_label_tensor_filename)\n",
    "                self.update_dictionary(self.preprocessed_info_dict, sample_id, \"label\" ,\"patches\", {})\n",
    "\n",
    "            #Check depth directory for measurement\n",
    "            if f\"depth[{self.model_input_dim[2]}]\" in self.preprocessed_info_dict[sample_id][\"measurement\"]:\n",
    "                print(f\"The measurement corresponding to '{sample_id}' sample have already been processed with depth [{self.model_input_dim[2]}] at least once.\")\n",
    "            else:\n",
    "                # Create directory for the depth we will apply inside the measurement directory\n",
    "                measurement_depth_dir = os.path.join(self.preprocessed_dir, f\"{sample_id}\", \"measurement\", f\"depth[{self.model_input_dim[2]}]\")\n",
    "                os.makedirs(measurement_depth_dir)\n",
    "                print(f\"The directory '{measurement_depth_dir}' did not exist and has been created.\")\n",
    "                measurement_depth_patches_dir = os.path.join(measurement_depth_dir, \"patches\")\n",
    "                os.makedirs(measurement_depth_patches_dir)\n",
    "                print(f\"The directory '{measurement_depth_patches_dir}' did not exist and has been created.\")\n",
    "                # Create and save the cropped, standarized & depth compressed 3D measurement in the directory just created\n",
    "                cropped_standarized_depth_compressed_measurement_tensor = self.load_crop_standardize_depth_compress_measurement(sample_id, sample_metadata)\n",
    "                cropped_standarized_depth_compressed_measurement_tensor_filename = os.path.join(measurement_depth_dir, f\"{sample_id}_cropped_standarized_depth[{self.model_input_dim[2]}].pt\")\n",
    "                torch.save(cropped_standarized_depth_compressed_measurement_tensor, cropped_standarized_depth_compressed_measurement_tensor_filename)\n",
    "                # Update the information in the preprocessed_info_json\n",
    "                self.update_dictionary(self.preprocessed_info_dict, sample_id, \"measurement\", f\"depth[{self.model_input_dim[2]}]\", {})\n",
    "                self.update_dictionary(self.preprocessed_info_dict, sample_id, \"measurement\", f\"depth[{self.model_input_dim[2]}]\", \"cropped_standarized_depth_compressed_measurement_tensor_filename\", cropped_standarized_depth_compressed_measurement_tensor_filename)\n",
    "                self.update_dictionary(self.preprocessed_info_dict, sample_id, \"measurement\", f\"depth[{self.model_input_dim[2]}]\", \"patches\", {})\n",
    "\n",
    "            #Check patches_size directory for label\n",
    "            if  f\"patch_size[{self.patch_size}]\" in self.preprocessed_info_dict[sample_id][\"label\"][\"patches\"]:\n",
    "                print(f\"The label corresponding to '{sample_id}' sample have already been processed with patch size [{self.patch_size}] at least once.\")\n",
    "            else:\n",
    "                label_patches_size_dir = os.path.join(self.preprocessed_dir, f\"{sample_id}\", \"label\", \"patches\", f\"patch_size[{self.patch_size}]\")\n",
    "                os.makedirs(label_patches_size_dir)\n",
    "                print(f\"The directory '{label_patches_size_dir}' did not exist and has been created.\")\n",
    "                self.update_dictionary(self.preprocessed_info_dict, sample_id, \"label\", \"patches\", f\"patch_size[{self.patch_size}]\", {})\n",
    "\n",
    "            #Check patches_size directory for measurement\n",
    "            if  f\"patch_size[{self.patch_size}]\" in self.preprocessed_info_dict[sample_id][\"measurement\"][f\"depth[{self.model_input_dim[2]}]\"][\"patches\"]:\n",
    "                print(f\"The measurement corresponding to '{sample_id}' sample have already been processed with depth [{self.model_input_dim[2]}] and patch size [{self.patch_size}] at least once.\")\n",
    "            else:\n",
    "                measurement_depth_patches_size_dir = os.path.join(self.preprocessed_dir, f\"{sample_id}\", \"measurement\", f\"depth[{self.model_input_dim[2]}]\", \"patches\", f\"patch_size[{self.patch_size}]\")\n",
    "                os.makedirs(measurement_depth_patches_size_dir)\n",
    "                print(f\"The directory '{measurement_depth_patches_size_dir}' did not exist and has been created.\")\n",
    "                self.update_dictionary(self.preprocessed_info_dict, sample_id, \"measurement\", f\"depth[{self.model_input_dim[2]}]\", \"patches\", f\"patch_size[{self.patch_size}]\", {})\n",
    "\n",
    "\n",
    "            #LABEL_PATCHES\n",
    "            if f\"overlap[{self.overlap_key}]\" in self.preprocessed_info_dict[sample_id][\"label\"][\"patches\"][f\"patch_size[{self.patch_size}]\"]:\n",
    "                print(f\"The label corresponding to '{sample_id}' sample have already been processed with patch size [{self.patch_size}] and overlap [{self.overlap_key}]\")\n",
    "            else:\n",
    "                label_overlap_dir = os.path.join(self.preprocessed_dir, f\"{sample_id}\", \"label\", \"patches\", f\"patch_size[{self.patch_size}]\", f\"overlap[{self.overlap_key}]\")\n",
    "                os.makedirs(label_overlap_dir)\n",
    "                print(f\"The directory '{label_overlap_dir}' did not exist and has been created.\")\n",
    "                cropped_bbox_info = self.preprocessed_info_dict[sample_id][\"cropped_bbox_info\"]\n",
    "                cropped_label_tensor_filename = self.preprocessed_info_dict[sample_id][\"label\"][\"cropped_label_tensor_filename\"]\n",
    "                cropped_label_tensor = torch.load(cropped_label_tensor_filename, weights_only=True)\n",
    "                label_patches = self.create_patches(cropped_label_tensor, sample_metadata[\"ROI\"], cropped_bbox_info)\n",
    "\n",
    "                label_patches_dictionary = {}\n",
    "                # Save patches\n",
    "                for i, label_patch in enumerate(label_patches):\n",
    "                    label_tensor_patch, label_tensor_patch_coord, roi_adjusted_patch = label_patch\n",
    "                    preprocessed_label_patch_filename = os.path.join(f\"label_{sample_id}_patch_size[{self.patch_size}]_overlap[{self.overlap_key}]_coords[{label_tensor_patch_coord[0]}x{label_tensor_patch_coord[1]}].pt\")\n",
    "                    patch_patch= os.path.join(label_overlap_dir, preprocessed_label_patch_filename)\n",
    "                    torch.save(label_tensor_patch, patch_patch)\n",
    "                    label_patches_dictionary[\"_\".join(map(str, label_tensor_patch_coord))] = {\n",
    "                        \"patch_coord\": label_tensor_patch_coord,\n",
    "                        \"patch_path\": patch_patch,\n",
    "                        \"roi_adjusted_patch\": roi_adjusted_patch\n",
    "                    }\n",
    "                self.update_dictionary(self.preprocessed_info_dict, sample_id, \"label\", \"patches\", f\"patch_size[{self.patch_size}]\", f\"overlap[{self.overlap_key}]\", label_patches_dictionary)\n",
    "\n",
    "            #MEASUREMENT_PATCHES\n",
    "            if f\"overlap[{self.overlap_key}]\" in self.preprocessed_info_dict[sample_id][\"measurement\"][f\"depth[{self.model_input_dim[2]}]\"][\"patches\"][f\"patch_size[{self.patch_size}]\"]:\n",
    "                print(f\"The measurement corresponding to '{sample_id}' sample have already been processed with depth [{self.model_input_dim[2]}], patch size [{self.patch_size}] and overlap [{self.overlap_key}]\")\n",
    "            else:\n",
    "                measurement_overlap_dir = os.path.join(self.preprocessed_dir, f\"{sample_id}\", \"measurement\", f\"depth[{self.model_input_dim[2]}]\",\"patches\", f\"patch_size[{self.patch_size}]\", f\"overlap[{self.overlap_key}]\")\n",
    "                os.makedirs(measurement_overlap_dir)\n",
    "                print(f\"The directory '{measurement_overlap_dir}' did not exist and has been created.\")\n",
    "                cropped_bbox_info = self.preprocessed_info_dict[sample_id][\"cropped_bbox_info\"]\n",
    "                cropped_standarized_depth_compressed_measurement_tensor_filename = self.preprocessed_info_dict[sample_id][\"measurement\"][f\"depth[{self.model_input_dim[2]}]\"][\"cropped_standarized_depth_compressed_measurement_tensor_filename\"]\n",
    "                cropped_standarized_depth_compressed_measurement_tensor = torch.load(cropped_standarized_depth_compressed_measurement_tensor_filename, weights_only=True)\n",
    "                measurement_patches = self.create_patches(cropped_standarized_depth_compressed_measurement_tensor, sample_metadata[\"ROI\"], cropped_bbox_info)\n",
    "\n",
    "                measurement_patches_dictionary = {}\n",
    "                # Save patches\n",
    "                for i, measurement_patch in enumerate(measurement_patches):\n",
    "                    measurement_tensor_patch, measurement_tensor_patch_coord, roi_adjusted_patch = measurement_patch\n",
    "                    preprocessed_measurement_patch_filename = os.path.join(f\"{sample_id}_depth[{self.model_input_dim[2]}]_patch_size[{self.patch_size}]_overlap[{self.overlap_key}]_coords[{measurement_tensor_patch_coord[0]}x{measurement_tensor_patch_coord[1]}].pt\")\n",
    "                    patch_patch = os.path.join(measurement_overlap_dir, preprocessed_measurement_patch_filename)\n",
    "                    torch.save(measurement_tensor_patch, patch_patch)\n",
    "                    measurement_patches_dictionary[\"_\".join(map(str, measurement_tensor_patch_coord))] = {\n",
    "                        \"patch_coord\": measurement_tensor_patch_coord,\n",
    "                        \"patch_path\": patch_patch,\n",
    "                        \"roi_adjusted_patch\": roi_adjusted_patch\n",
    "                    }\n",
    "                self.update_dictionary(self.preprocessed_info_dict, sample_id, \"measurement\", f\"depth[{self.model_input_dim[2]}]\", \"patches\", f\"patch_size[{self.patch_size}]\", f\"overlap[{self.overlap_key}]\", measurement_patches_dictionary)\n",
    "\n",
    "            label_patches_dict = self.preprocessed_info_dict[sample_id][\"label\"][\"patches\"][f\"patch_size[{self.patch_size}]\"][f\"overlap[{self.overlap_key}]\"]\n",
    "            measurement_patches_dict = self.preprocessed_info_dict[sample_id][\"measurement\"][f\"depth[{self.model_input_dim[2]}]\"][\"patches\"][f\"patch_size[{self.patch_size}]\"][f\"overlap[{self.overlap_key}]\"]\n",
    "            for (key1, value1), (key2, value2) in zip(label_patches_dict.items(), measurement_patches_dict.items()):\n",
    "                self.preprocessed_patches_dataset.append(\n",
    "                    {\n",
    "                        \"sample_id\": sample_id,\n",
    "                        \"coord\": key1, # Key 1 and Key 2 are equal\n",
    "                        \"patch_label_info\": value1,\n",
    "                        \"patch_measurement_info\": value2\n",
    "                    }\n",
    "                )\n",
    "\n",
    "        self.update_preprocessed_info() #Updates \"preprocessed_info.json\"\n",
    "\n",
    "\n",
    "    def load_preprocessed_info(self):\n",
    "        # If it exists, attempt to load existing data from the file\n",
    "        with open(self.preprocessed_info_json_path, 'r') as file:\n",
    "            try:\n",
    "                preprocessed_info_dict = json.load(file)  # Load JSON data into a dictionary\n",
    "            except json.JSONDecodeError:\n",
    "                raise ValueError(\n",
    "                    f\"The JSON file '{self.preprocessed_info_json_path}' is corrupted or not formatted correctly.\"\n",
    "                )\n",
    "        return preprocessed_info_dict\n",
    "\n",
    "\n",
    "    def update_preprocessed_info(self):\n",
    "        ## Rewrite JSON in disk\n",
    "        with open(self.preprocessed_info_json_path, 'w') as file:\n",
    "            json.dump(self.preprocessed_info_dict, file, indent=4)  # The indent parameter makes it human-readable\n",
    "\n",
    "        print(f\"The JSON file '{self.preprocessed_info_json_path}' has been updated.\")\n",
    "\n",
    "\n",
    "    def update_dictionary(self, dictionary, *args):\n",
    "        # If it's just a key-value pair\n",
    "        if len(args) == 2:\n",
    "            if args[0] not in dictionary:\n",
    "                dictionary[args[0]] = args[1]\n",
    "            else:\n",
    "                dictionary[args[0]].update(args[1])\n",
    "        else:\n",
    "            current_level = dictionary\n",
    "            # Iterate through all but the last argument to handle nested keys\n",
    "            for key in args[:-2]:\n",
    "                current_level = current_level[key]  # Move deeper into the nested dictionary\n",
    "\n",
    "            # The last two arguments are the final key-value pair to update\n",
    "            final_key, final_value = args[-2], args[-1]\n",
    "\n",
    "            if final_key not in current_level:\n",
    "                current_level[final_key] = final_value\n",
    "            else:\n",
    "                current_level[final_key].update(final_value)\n",
    "\n",
    "\n",
    "    def create_patches(self, cropped_tensor, ROI_coordinates, cropped_bbox_info):\n",
    "\n",
    "        for i in range(2):  # Check for i=0 (height) and i=1 (width)\n",
    "            cropped_dim = cropped_tensor.shape[i + 1]  # i+1 due to channel in dim 0\n",
    "            required_dim = self.model_input_dim[i]\n",
    "\n",
    "            if cropped_dim - required_dim == 0:\n",
    "                continue  # Dimension matches exactly; no issue\n",
    "            elif cropped_dim - required_dim > 0:\n",
    "                continue  # Cropped dimension is larger than required\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    f\"The cropped ROI area is too small for the predefined patch size. \"\n",
    "                    f\"Dimension {i} mismatch: cropped ROI area dimension {i} = {cropped_dim} \"\n",
    "                    f\"vs. patch dimension {i} = {required_dim}.\"\n",
    "                )\n",
    "\n",
    "        slices_per_dim = {}\n",
    "\n",
    "        for i in range(2):  # Check for i=0 (height) and i=1 (width)\n",
    "            stride = int(self.model_input_dim[i] * (1-self.overlap[i]))\n",
    "            print(f\"Dim {i} - Stride: {stride}\")\n",
    "            print(f\"Dim {i} - Model Input Dim: {self.model_input_dim[i]}\")\n",
    "            print(f\"Dim {i} - Cropped Tensor Shape: {cropped_tensor.shape[i + 1]}\")\n",
    "            slices = []\n",
    "            for j in range(0, cropped_tensor.shape[i + 1], stride):\n",
    "                slice_start = j\n",
    "                slice_end = j + self.model_input_dim[i]\n",
    "                if slice_end <= cropped_tensor.shape[i + 1]:\n",
    "                    slice_to_add = slice(slice_start, slice_end)\n",
    "                    print(f\"Dim {i} - Slice: {slice_to_add}\")\n",
    "                    slices.append(slice_to_add)\n",
    "                else:\n",
    "                    out_of_bounds = slice_end - cropped_tensor.shape[i + 1]\n",
    "                    slice_to_add = slice(slice_start - out_of_bounds, slice_end - out_of_bounds)\n",
    "                    if not slice_to_add in slices:\n",
    "                      print(f\"Dim {i} - Slice: {slice_to_add}\")\n",
    "                      slices.append(slice_to_add)\n",
    "                    else:\n",
    "                      break\n",
    "            slices_per_dim[i] = slices\n",
    "\n",
    "        # Get the lists of slices\n",
    "        slices_height_dim = slices_per_dim[0]\n",
    "        slices_width_dim = slices_per_dim[1]\n",
    "\n",
    "        # Generate all combinations (Cartesian product) of slices between both dimensions\n",
    "        patches_slices = list(product(slices_height_dim, slices_width_dim))\n",
    "\n",
    "        # List to store the resulting patches\n",
    "        patches_list = []\n",
    "\n",
    "        # Iterate over all the patches\n",
    "        for patch_slices in patches_slices:\n",
    "            patch_slice_height_dim = patch_slices[0]\n",
    "            patch_slice_width_dim = patch_slices[1]\n",
    "\n",
    "            roi_start = (patch_slice_height_dim.start, patch_slice_width_dim.start)\n",
    "\n",
    "            if len(cropped_tensor.shape) == 3: # Label tensor\n",
    "                patch_tensor = cropped_tensor[:, patch_slice_height_dim, patch_slice_width_dim]\n",
    "            else: # Measurement tensor\n",
    "                patch_tensor = cropped_tensor[:, patch_slice_height_dim, patch_slice_width_dim, :]\n",
    "\n",
    "            roi_coords_in_cropped_tensor = {\n",
    "                'all_points_x': [x - cropped_bbox_info[\"x_coord\"] for x in ROI_coordinates['all_points_x']],\n",
    "                'all_points_y': [y - cropped_bbox_info[\"y_coord\"] for y in ROI_coordinates['all_points_y']]\n",
    "            }\n",
    "\n",
    "            roi_adjusted_patch = {\n",
    "                'all_points_x': [x - roi_start[1] for x in roi_coords_in_cropped_tensor['all_points_x']],\n",
    "                'all_points_y': [y - roi_start[0] for y in roi_coords_in_cropped_tensor['all_points_y']]\n",
    "            }\n",
    "            # Append the cropped tensor and adjusted ROI to the list\n",
    "            patches_list.append((patch_tensor, (roi_start[0], roi_start[1]), roi_adjusted_patch))\n",
    "\n",
    "        if len(cropped_tensor.shape) == 3: # Label tensor\n",
    "                # Plotting Label Tensor Patches\n",
    "                self.plot_patches(\n",
    "                    patches_list,\n",
    "                    tensor_type='Label',\n",
    "                    channel=1,  # Specify the label channel (e.g., foreground)\n",
    "                    cmap='RdBu',\n",
    "                    number_slices_height_dim=len(slices_height_dim),\n",
    "                    number_slices_width_dim=len(slices_width_dim)\n",
    "                )\n",
    "        else: # Measurement tensor\n",
    "                # Plotting Measurement Tensor Patches\n",
    "                self.plot_patches(\n",
    "                    patches_list,\n",
    "                    tensor_type='Measurement',\n",
    "                    depth_frame=15,  # Specify the frame index\n",
    "                    cmap='RdBu',\n",
    "                    number_slices_height_dim=len(slices_height_dim),\n",
    "                    number_slices_width_dim=len(slices_width_dim)\n",
    "                )\n",
    "\n",
    "\n",
    "        return patches_list\n",
    "\n",
    "    def load_and_crop_label(self, measurement_id, measurement_data):\n",
    "        measurement_label_filename = measurement_data[\"label_filename\"]\n",
    "        measurement_ROI = measurement_data[\"ROI\"]\n",
    "\n",
    "        # ############# LABEL LOADING ###################\n",
    "\n",
    "        # Load label data\n",
    "        label_img_ndarray = mpimg.imread(os.path.join(os.getcwd(), self.data_dir, \"labels\", measurement_label_filename))\n",
    "        if len(label_img_ndarray.shape) == 3:\n",
    "            label_img_ndarray = label_img_ndarray[..., 0] # Shape (256, 320)\n",
    "\n",
    "        # ############# LABEL CONVERSION TO MULTI-CHANNEL (IT IS ADAPTED TO THE MODEL) ###################\n",
    "\n",
    "        # Convert label to multi-channel for the model\n",
    "        label_one_hot_encoded, label_mapping = self.one_hot_encode(label_img_ndarray)\n",
    "        label_tensor = label_one_hot_encoded # Shape (2, 256, 320)\n",
    "\n",
    "        self.plot_tensor_and_polygon(label_tensor, measurement_ROI, \"Label Tensor\")\n",
    "\n",
    "        # ############# CROPPING LABEL USING MANUALLY DEFINED ROI ###################\n",
    "\n",
    "        print(f\"(Before cropping) label_tensor.shape: {label_tensor.shape}\")\n",
    "\n",
    "        # 1. Extract bounding box coordinates from ROI polygon\n",
    "        x_coords = np.array(measurement_ROI['all_points_x'])\n",
    "        y_coords = np.array(measurement_ROI['all_points_y'])\n",
    "\n",
    "        x_min, x_max = int(x_coords.min()), int(x_coords.max())\n",
    "        y_min, y_max = int(y_coords.min()), int(y_coords.max())\n",
    "\n",
    "        # 2. Crop the tensor along height and width using the bounding box\n",
    "        # Slicing along height (y-axis) and width (x-axis)\n",
    "        cropped_label_tensor = label_tensor[:, y_min:y_max, x_min:x_max]\n",
    "\n",
    "        cropped_bbox_info = {\n",
    "            'y_coord': y_min,\n",
    "            'x_coord': x_min,\n",
    "            'height': y_max-y_min,\n",
    "            'width': x_max-x_min,\n",
    "        }\n",
    "\n",
    "        ################### LOGGING #############################\n",
    "\n",
    "        print(f\"(After cropping) cropped_label_tensor.shape: {cropped_label_tensor.shape}\")\n",
    "\n",
    "        roi_coords_in_cropped_tensor = {\n",
    "            'all_points_x': [x - x_min for x in measurement_ROI['all_points_x']],\n",
    "            'all_points_y': [y - y_min for y in measurement_ROI['all_points_y']]\n",
    "        }\n",
    "\n",
    "        self.plot_tensor_and_polygon(cropped_label_tensor, roi_coords_in_cropped_tensor,\"(Cropped) Label Tensor\")\n",
    "\n",
    "        return cropped_label_tensor, cropped_bbox_info\n",
    "\n",
    "    def load_crop_standardize_depth_compress_measurement(self, measurement_id, measurement_data):\n",
    "        measurement_3D_thermal_sequence_filename = measurement_data[\"3D_thermal_sequence_filename\"]\n",
    "        measurement_ROI = measurement_data[\"ROI\"]\n",
    "\n",
    "        # ############# 3D SEQUENCE DATA LOADING ###################\n",
    "\n",
    "        # Load 3D sequence data\n",
    "        mat_data = io.loadmat(os.path.join(os.getcwd(), self.data_dir, \"data\", measurement_3D_thermal_sequence_filename))\n",
    "        measurement_3D_thermal_sequence = np.float32(mat_data[\"imageArray\"])  # Shape (256, 320, 1810)\n",
    "        measurement_tensor = torch.tensor(measurement_3D_thermal_sequence).unsqueeze(0)  # Shape (1, 256, 320, 1810)\n",
    "\n",
    "        self.plot_tensor_and_polygon(measurement_tensor[:,:,:,100], measurement_ROI, \"Measurement Tensor (Frame 100)\")\n",
    "\n",
    "        # ############# CROPPING MANUALLY DEFINED ROI ###################\n",
    "\n",
    "        print(f\"(Before cropping) measurement_tensor.shape: {measurement_tensor.shape}\")\n",
    "\n",
    "        # 1. Extract bounding box coordinates from ROI polygon\n",
    "        x_coords = np.array(measurement_ROI['all_points_x'])\n",
    "        y_coords = np.array(measurement_ROI['all_points_y'])\n",
    "\n",
    "        x_min, x_max = int(x_coords.min()), int(x_coords.max())\n",
    "        y_min, y_max = int(y_coords.min()), int(y_coords.max())\n",
    "\n",
    "        # 2. Crop the tensor along height and width using the bounding box\n",
    "        # Slicing along height (y-axis) and width (x-axis)\n",
    "        cropped_measurement_tensor = measurement_tensor[:, y_min:y_max, x_min:x_max, :]\n",
    "\n",
    "        ################### LOGGING #############################\n",
    "\n",
    "        print(f\"(After cropping) cropped_measurement_tensor.shape: {cropped_measurement_tensor.shape}\")\n",
    "\n",
    "        roi_coords_in_cropped_tensor = {\n",
    "            'all_points_x': [x - x_min for x in measurement_ROI['all_points_x']],\n",
    "            'all_points_y': [y - y_min for y in measurement_ROI['all_points_y']]\n",
    "        }\n",
    "\n",
    "        self.plot_tensor_and_polygon(cropped_measurement_tensor[:,:,:,100], roi_coords_in_cropped_tensor, \"(Cropped) Measurement Tensor\\n(Frame 100)\")\n",
    "\n",
    "        # ############# NORMALIZE 3D SEQUENCE (3D STANDARDIZATION) ###################\n",
    "\n",
    "        # Standardize the volume channel-wise\n",
    "        # Mean and std are calculated along the spatial and depth dimensions (H, W, D)\n",
    "        mean = cropped_measurement_tensor.mean(dim=(1, 2, 3), keepdim=True)  # Keep dimensions for broadcasting\n",
    "        std = cropped_measurement_tensor.std(dim=(1, 2, 3), keepdim=True)\n",
    "\n",
    "        # Standardize: (value - mean) / std\n",
    "        cropped_normalized_measurement_tensor = (cropped_measurement_tensor - mean) / std\n",
    "\n",
    "        print(\"(Before normalization/standardization) cropped_measurement_tensor.shape:\", cropped_measurement_tensor.shape)\n",
    "        print(\"(After normalization/standardization) cropped_normalized_measurement_tensor.shape:\", cropped_normalized_measurement_tensor.shape)\n",
    "\n",
    "        self.plot_tensor_and_polygon(cropped_normalized_measurement_tensor[:,:,:,100], roi_coords_in_cropped_tensor, \"(Cropped) 3D_Standardized Measurement Tensor\\n(Frame 100)\")\n",
    "\n",
    "        # ############# TEMPORAL COMPRESSION ###################\n",
    "\n",
    "        model_input_depth_dim = self.model_input_dim[2]\n",
    "\n",
    "        cropped_normalized_compressed_measurement_tensor = self.compress_depth_tensor(cropped_normalized_measurement_tensor, model_input_depth_dim)\n",
    "\n",
    "        print(f\"(Before compression) cropped_normalized_measurement_tensor.shape: {cropped_normalized_measurement_tensor.shape}\")\n",
    "        print(f\"(After compression) cropped_normalized_compressed_measurement_tensor.shape: {cropped_normalized_compressed_measurement_tensor.shape}\")\n",
    "\n",
    "        self.plot_tensor_and_polygon(cropped_normalized_compressed_measurement_tensor[:,:,:,15], roi_coords_in_cropped_tensor, f\"(Cropped) 3D_Standardized/Compressed_{model_input_depth_dim} Measurement Tensor\\n(Frame 15 After Compression)\")\n",
    "\n",
    "        return cropped_normalized_compressed_measurement_tensor\n",
    "\n",
    "    def plot_patches(self, patches, tensor_type, number_slices_height_dim, number_slices_width_dim, channel=None, depth_frame=None, cmap='RdBu'):\n",
    "        \"\"\"\n",
    "        Plots the patches of measurement or label tensors with adjusted ROI polygons.\n",
    "\n",
    "        Args:\n",
    "            patches (list): List of tuples [(patch_tensor, patch_coord, adjusted_roi)].\n",
    "            tensor_type (str): Type of tensor ('Measurement' or 'Label').\n",
    "            channel (int, optional): Channel index to visualize (for label tensors).\n",
    "            depth_frame (int, optional): Frame index to visualize (for measurement tensors).\n",
    "            cmap (str): Colormap for visualization.\n",
    "        \"\"\"\n",
    "        patches_per_row = number_slices_width_dim\n",
    "        n_rows = number_slices_height_dim\n",
    "        fig, axes = plt.subplots(n_rows, patches_per_row, figsize=(patches_per_row * 5, n_rows * 5))\n",
    "        axes = axes.flatten()  # Flatten axes for easy indexing\n",
    "\n",
    "        for idx, (patch_tensor, patch_coord, adjusted_roi) in enumerate(patches):\n",
    "            ax = axes[idx]\n",
    "\n",
    "            # Determine what to plot based on tensor type\n",
    "            if tensor_type == 'Measurement' and depth_frame is not None:\n",
    "                data = patch_tensor[0, :, :, depth_frame].cpu().numpy()  # Frame-specific\n",
    "                # Plot the patch tensor\n",
    "                im = ax.imshow(data, cmap=cmap)\n",
    "            elif tensor_type == 'Label' and channel is not None:\n",
    "                data = patch_tensor[channel, :, :].cpu().numpy()  # Channel-specific\n",
    "                # Plot the patch tensor\n",
    "                im = ax.imshow(data, cmap=cmap, vmin=0, vmax=1)\n",
    "            else:\n",
    "                raise ValueError(\"Specify 'depth_frame' for Measurement or 'channel' for Label tensors.\")\n",
    "\n",
    "\n",
    "            ax.set_title(f'{tensor_type} Patch Coord: ({patch_coord[0]}, {patch_coord[1]})')\n",
    "            ax.axis('off')\n",
    "            fig.colorbar(im, ax=ax, label='Pixel Value')\n",
    "\n",
    "            # Plot the adjusted polygon overlay\n",
    "            all_points_x = adjusted_roi['all_points_x']\n",
    "            all_points_y = adjusted_roi['all_points_y']\n",
    "            ax.plot(all_points_x + [all_points_x[0]], all_points_y + [all_points_y[0]], 'r-', linewidth=2)  # Close the polygon\n",
    "            ax.scatter(all_points_x, all_points_y, color='blue', zorder=5)  # Mark the vertices\n",
    "\n",
    "        # Hide unused axes\n",
    "        for idx in range(len(patches), len(axes)):\n",
    "            axes[idx].axis('off')\n",
    "\n",
    "        plt.tight_layout()  # Leave space for the title\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    def plot_tensor_and_polygon(self,tensor, roi, name_plot_tensor):\n",
    "        \"\"\"\n",
    "        Plot the mask and ROI polygon together for visualization.\n",
    "\n",
    "        Parameters:\n",
    "        - mask (torch.Tensor): The mask to be plotted.\n",
    "        - roi (dict): Dictionary with keys 'all_points_x' and 'all_points_y' representing the ROI polygon.\n",
    "        \"\"\"\n",
    "        if tensor.shape[0] == 1:\n",
    "            # Plotting the mask\n",
    "            fig, ax = plt.subplots(figsize=(6, 6))\n",
    "            im = ax.imshow(tensor[0].cpu().numpy(), cmap='RdBu')\n",
    "            plt.colorbar(im, ax=ax, label='Pixel Value')\n",
    "\n",
    "            # Correct the y-coordinates to match the image grid\n",
    "            all_points_x = roi['all_points_x']\n",
    "            all_points_y = roi['all_points_y']\n",
    "            plt.plot(all_points_x + [all_points_x[0]], all_points_y + [all_points_y[0]], 'r-', linewidth=2)  # Close the polygon\n",
    "            plt.scatter(all_points_x, all_points_y, color='blue', zorder=5)  # Mark the vertices\n",
    "            plt.suptitle(f'{name_plot_tensor} with ROI Polygon Overlay')\n",
    "            plt.show()\n",
    "        else:\n",
    "            fig, axes = plt.subplots(1, tensor.shape[0], figsize=(12, 6))\n",
    "            for i in range(tensor.shape[0]):\n",
    "                im = axes[i].imshow(tensor[i].cpu().numpy(), cmap='RdBu')\n",
    "                axes[i].set_title(f'Channel {i}')\n",
    "                fig.colorbar(im, ax=axes[i], label='Pixel Value')\n",
    "\n",
    "                # Add polygon overlay\n",
    "                all_points_x = roi['all_points_x']\n",
    "                all_points_y = roi['all_points_y']\n",
    "                axes[i].plot(all_points_x + [all_points_x[0]], all_points_y + [all_points_y[0]], 'r-', linewidth=2)  # Close the polygon\n",
    "                axes[i].scatter(all_points_x, all_points_y, color='blue', zorder=5)\n",
    "\n",
    "            plt.suptitle(f'{name_plot_tensor} with ROI Polygon Overlay')\n",
    "            plt.show()\n",
    "\n",
    "    def compress_depth_tensor(self, tensor, model_input_dim_depth):\n",
    "        num_channels, height, width, depth = tensor.shape\n",
    "\n",
    "        # Raise an exception if depth is smaller than model_input_dim_depth\n",
    "        if depth < model_input_dim_depth:\n",
    "            raise ValueError(f\"The depth of the input tensor ({depth}) must be greater than or equal to model_input_dim_depth ({model_input_dim_depth}).\")\n",
    "\n",
    "        block_size = depth // model_input_dim_depth\n",
    "        remainder = depth % model_input_dim_depth\n",
    "\n",
    "        print(f\"Block size: {block_size}\")\n",
    "        print(f\"Remainder: {remainder}\")\n",
    "\n",
    "        # Convert to NumPy\n",
    "        numpy_array = tensor.numpy()\n",
    "\n",
    "        # Apply Gaussian filter only along the last dimension (depth)\n",
    "        # sigma = block_size / 2 beacuse is the number of neighbours we look right and left\n",
    "        smoothed_numpy_array = gaussian_filter(numpy_array, sigma=(0, 0, 0, block_size / 2))\n",
    "\n",
    "        # Convert back to PyTorch tensor\n",
    "        smoothed_tensor = torch.tensor(smoothed_numpy_array, dtype=torch.float32)\n",
    "\n",
    "        # Generate fractional indices\n",
    "        indices = torch.linspace(0, depth - 1, model_input_dim_depth)\n",
    "        # print(f\"Indices shape: {indices.shape}\")\n",
    "        # print(f\"Indices: {indices}\")\n",
    "\n",
    "        # Round to nearest integer and clamp indices to valid range\n",
    "        indices = torch.clamp(indices.round().long(), 0, depth - 1)\n",
    "\n",
    "        # Select slices at these indices\n",
    "        reduced_tensor = smoothed_tensor[..., indices]\n",
    "\n",
    "        return reduced_tensor\n",
    "\n",
    "    def one_hot_encode(self, array):\n",
    "        # Get unique labels in the array\n",
    "        unique_labels = np.unique(array)\n",
    "        # Create a dictionary mapping each label to an index\n",
    "        label_to_index = {label: index for index, label in enumerate(unique_labels)}\n",
    "        # Shape for one-hot encoding: (height, width, num_classes)\n",
    "        one_hot_shape = array.shape + (len(unique_labels),)\n",
    "        # Initialize one-hot encoded array\n",
    "        one_hot_encoded = np.zeros(one_hot_shape, dtype=np.float32)\n",
    "        # Populate one-hot array\n",
    "        for label, index in label_to_index.items():\n",
    "            one_hot_encoded[..., index] = (array == label).astype(np.float32)\n",
    "        #Convert into tensor\n",
    "        one_hot_encoded = torch.tensor(one_hot_encoded, dtype=torch.float32)\n",
    "        # Shape for one-hot encoding: (num_classes, height, width)\n",
    "        one_hot_encoded = one_hot_encoded.permute(2, 0, 1)\n",
    "\n",
    "        return one_hot_encoded, label_to_index\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.is_inference_mode:\n",
    "            return len(self.metadata_dict_with_files_selected)\n",
    "        else:\n",
    "            return len(self.preprocessed_patches_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.is_inference_mode:\n",
    "            sample_id = list(self.metadata_dict_with_files_selected.keys())[idx]\n",
    "            y = y_filename = self.preprocessed_info_dict[sample_id][\"label\"][\"cropped_label_tensor_filename\"]\n",
    "            x = patches_info_dict = self.preprocessed_info_dict[sample_id][\"measurement\"][f\"depth[{self.model_input_dim[2]}]\"][\"patches\"][f\"patch_size[{self.patch_size}]\"][f\"overlap[{self.overlap_key}]\"]\n",
    "            return x, y\n",
    "        else:\n",
    "            x_filename = self.preprocessed_patches_dataset[idx][\"patch_measurement_info\"][\"patch_path\"]\n",
    "            y_filename = self.preprocessed_patches_dataset[idx][\"patch_label_info\"][\"patch_path\"]\n",
    "\n",
    "            x = torch.load(x_filename, weights_only=True)\n",
    "            y = torch.load(y_filename, weights_only=True)\n",
    "\n",
    "            return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8cOsPFnwzZ3l"
   },
   "source": [
    "# MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "45qX1_AS4DSJ",
    "outputId": "80e3711d-98d5-4f26-c8a0-ba162a7e24f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All imports succeeded!\n"
     ]
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import itertools\n",
    "from collections.abc import Sequence\n",
    "from typing import Final\n",
    "\n",
    "# Third-party imports\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.checkpoint as checkpoint\n",
    "from torch.nn import LayerNorm\n",
    "from typing import Type\n",
    "from einops import rearrange\n",
    "\n",
    "from monai.networks.blocks import MLPBlock as Mlp\n",
    "from monai.networks.blocks import PatchEmbed, UnetOutBlock, UnetrBasicBlock, UnetrUpBlock\n",
    "from monai.networks.layers import DropPath, trunc_normal_\n",
    "from monai.utils import ensure_tuple_rep, look_up_option\n",
    "\n",
    "print(\"All imports succeeded!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "VooONFUYvj4f"
   },
   "outputs": [],
   "source": [
    "class WindowAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Window based multi-head self attention module with relative position bias based on: \"Liu et al.,\n",
    "    Swin Transformer: Hierarchical Vision Transformer using Shifted Windows\n",
    "    <https://arxiv.org/abs/2103.14030>\"\n",
    "    https://github.com/microsoft/Swin-Transformer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        num_heads: int,\n",
    "        window_size: Sequence[int],\n",
    "        qkv_bias: bool = False,\n",
    "        attn_drop: float = 0.0,\n",
    "        proj_drop: float = 0.0,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dim: Number of input feature channels.\n",
    "            num_heads: Number of attention heads.\n",
    "            window_size: Size of the local window for attention.\n",
    "            qkv_bias: If True, adds a learnable bias to query, key, value projections.\n",
    "            attn_drop: Dropout rate for attention weights.\n",
    "            proj_drop: Dropout rate for output projection.\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.dim = dim  # Dimension of input features\n",
    "        self.window_size = window_size  # Size of the attention window\n",
    "        self.num_heads = num_heads  # Number of attention heads\n",
    "\n",
    "        # Dimension per attention head\n",
    "        head_dim = dim // num_heads\n",
    "        # Scaling factor for attention scores to prevent large values during softmax\n",
    "        self.scale = head_dim**-0.5\n",
    "        # Check for meshgrid arguments compatibility\n",
    "        mesh_args = torch.meshgrid.__kwdefaults__\n",
    "\n",
    "        # Handle 3D window sizes (e.g., for 3D volumes)\n",
    "        if len(self.window_size) == 3:\n",
    "            # Create a parameter table for relative position biases\n",
    "            self.relative_position_bias_table = nn.Parameter(\n",
    "                torch.zeros(\n",
    "                    (2 * self.window_size[0] - 1) * (2 * self.window_size[1] - 1) * (2 * self.window_size[2] - 1),\n",
    "                    num_heads,\n",
    "                )\n",
    "            )\n",
    "            # Create coordinate grids for relative position computation\n",
    "            coords_d = torch.arange(self.window_size[0])\n",
    "            coords_h = torch.arange(self.window_size[1])\n",
    "            coords_w = torch.arange(self.window_size[2])\n",
    "            # Generate coordinate grids with indexing support\n",
    "            if mesh_args is not None:\n",
    "                coords = torch.stack(torch.meshgrid(coords_d, coords_h, coords_w, indexing=\"ij\"))\n",
    "            else:\n",
    "                coords = torch.stack(torch.meshgrid(coords_d, coords_h, coords_w))\n",
    "            coords_flatten = torch.flatten(coords, 1)  # Flatten the coordinates for easier computation\n",
    "            # Compute relative coordinates between each point\n",
    "            relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]\n",
    "            relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Permute dimensions for indexing\n",
    "            # Adjust relative coordinates for bias table indexing\n",
    "            relative_coords[:, :, 0] += self.window_size[0] - 1\n",
    "            relative_coords[:, :, 1] += self.window_size[1] - 1\n",
    "            relative_coords[:, :, 2] += self.window_size[2] - 1\n",
    "            # Map to flattened indices\n",
    "            relative_coords[:, :, 0] *= (2 * self.window_size[1] - 1) * (2 * self.window_size[2] - 1)\n",
    "            relative_coords[:, :, 1] *= 2 * self.window_size[2] - 1\n",
    "\n",
    "        else: # Handle other input tensors\n",
    "            raise ValueError(\"Unsupported dimensions. Expected input to have length of 3 dimensions.\")\n",
    "\n",
    "        # Register the relative position index as a buffer\n",
    "        relative_position_index = relative_coords.sum(-1)\n",
    "        self.register_buffer(\"relative_position_index\", relative_position_index)\n",
    "\n",
    "        # Define linear layers for query, key, value projections\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        # Dropout layers for attention and output projection\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "        # Initialize relative position bias table with truncated normal distribution\n",
    "        trunc_normal_(self.relative_position_bias_table, std=0.02)\n",
    "        # Softmax layer for attention normalization\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"\"\"\n",
    "        Forward pass for window-based self-attention.\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor of shape (num_windows, num_tokens, embed_dim).\n",
    "            mask: Attention mask to restrict certain positions.\n",
    "\n",
    "        Returns:\n",
    "            Tensor of shape (batch_size, num_tokens, embed_dim) after self-attention.\n",
    "        \"\"\"\n",
    "        b, n, c = x.shape\n",
    "        # Compute query, key, and value projections and reshape them\n",
    "        qkv = self.qkv(x).reshape(b, n, 3, self.num_heads, c // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]  # Separate into query, key, and value tensors\n",
    "        q = q * self.scale  # Scale query for better numerical stability\n",
    "        attn = q @ k.transpose(-2, -1)  # Compute dot-product attention scores\n",
    "\n",
    "        # Add relative position bias to attention scores\n",
    "        relative_position_bias = self.relative_position_bias_table[\n",
    "            self.relative_position_index.clone()[:n, :n].reshape(-1)\n",
    "        ].reshape(n, n, -1)\n",
    "        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()\n",
    "        attn = attn + relative_position_bias.unsqueeze(0)  # Broadcast bias across batch and heads\n",
    "\n",
    "        # Apply attention mask if provided\n",
    "        if mask is not None:\n",
    "            nw = mask.shape[0]  # Number of windows\n",
    "            attn = attn.view(b // nw, nw, self.num_heads, n, n) + mask.unsqueeze(1).unsqueeze(0)\n",
    "            attn = attn.view(-1, self.num_heads, n, n)\n",
    "            attn = self.softmax(attn)  # Apply softmax to normalize attention scores\n",
    "        else:\n",
    "            attn = self.softmax(attn)  # Apply softmax to normalize attention scores\n",
    "\n",
    "        # Apply dropout to attention weights\n",
    "        attn = self.attn_drop(attn).to(v.dtype)\n",
    "        # Compute attention-weighted sum of values\n",
    "        x = (attn @ v).transpose(1, 2).reshape(b, n, c)\n",
    "        # Apply linear projection and dropout to the output\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def window_partition(x, window_size):\n",
    "    \"\"\"Window partition operation based on: \"Liu et al.,\n",
    "    Swin Transformer: Hierarchical Vision Transformer using Shifted Windows\"\n",
    "    <https://arxiv.org/abs/2103.14030>\"\n",
    "    https://github.com/microsoft/Swin-Transformer\n",
    "\n",
    "    This function partitions an input tensor into smaller windows based on the specified window size.\n",
    "    This is used in Swin Transformer models to divide the input into regions for applying self-attention efficiently.\n",
    "\n",
    "    Args:\n",
    "        x (Tensor): Input tensor of shape (batch_size, depth, height, width, channels) for 3D input data.\n",
    "        window_size (Sequence[int]): The size of each local window for partitioning.\n",
    "                                     It should be a tuple specifying the size for each spatial dimension (depth, height, width).\n",
    "\n",
    "    Returns:\n",
    "        Tensor: Partitioned tensor of shape (num_windows, window_size_product, channels) where `num_windows`\n",
    "                is the total number of windows, and `window_size_product` is the product of the window dimensions.\n",
    "\n",
    "    Example:\n",
    "        >>> import torch\n",
    "        >>> # Example 3D tensor with batch size 1, depth 8, height 8, width 8, and 3 channels\n",
    "        >>> x = torch.arange(1, 1 * 8 * 8 * 8 * 3 + 1).view(1, 8, 8, 8, 3)\n",
    "        >>> window_size = (4, 4, 4)\n",
    "        >>> windows = window_partition(x, window_size)\n",
    "        >>> print(\"Shape of partitioned windows:\", windows.shape)\n",
    "        >>> # Output shape: (8, 4*4*4, 3) -> (num_windows, window_size_product, channels)\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the shape of the input tensor\n",
    "    x_shape = x.size()\n",
    "\n",
    "    # Handle 5D input tensors (e.g., 3D input data with channels and batch dimensions)\n",
    "    if len(x_shape) == 5:\n",
    "        b, d, h, w, c = x_shape  # Extract batch, depth, height, width, and channels\n",
    "\n",
    "        # Reshape the input tensor so that each dimension becomes divisible by the window size\n",
    "        # This effectively creates smaller windows within the tensor\n",
    "        x = x.view(\n",
    "            b,\n",
    "            d // window_size[0],  # Number of windows along the depth dimension\n",
    "            window_size[0],       # Size of each window along the depth dimension\n",
    "            h // window_size[1],  # Number of windows along the height dimension\n",
    "            window_size[1],       # Size of each window along the height dimension\n",
    "            w // window_size[2],  # Number of windows along the width dimension\n",
    "            window_size[2],       # Size of each window along the width dimension\n",
    "            c                     # Channels (kept the same)\n",
    "        )\n",
    "\n",
    "        # Rearrange the dimensions to bring window dimensions next to each other and flatten each window\n",
    "        # Permute moves the dimensions around to the specified order, making it ready for further processing\n",
    "        windows = (\n",
    "            x.permute(0, 1, 3, 5, 2, 4, 6, 7)  # Change order of dimensions for easier window processing\n",
    "             .contiguous()                     # Ensures that data is stored contiguously in memory\n",
    "             .view(-1, window_size[0] * window_size[1] * window_size[2], c)  # Flatten each window\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported dimensions. Expected input to have length of 5 dimensions (b, d, h, w, c).\")\n",
    "\n",
    "    return windows\n",
    "\n",
    "\n",
    "def window_reverse(windows, window_size, dims):\n",
    "    \"\"\"\n",
    "    Window reverse operation based on: \"Liu et al.,\n",
    "    Swin Transformer: Hierarchical Vision Transformer using Shifted Windows\"\n",
    "    <https://arxiv.org/abs/2103.14030>\n",
    "    https://github.com/microsoft/Swin-Transformer\n",
    "\n",
    "    This function reverses the window partitioning process and reconstructs the original spatial dimensions\n",
    "    from the smaller windows. It reassembles the partitioned windows back into their original spatial arrangement.\n",
    "\n",
    "    Args:\n",
    "        windows: Tensor representing partitioned windows. Shape typically is (num_windows, window_size_product, channels).\n",
    "        window_size: Size of the local window (e.g., (depth, height, width) for 3D).\n",
    "        dims: Dimension values of the original spatial dimensions (before window partitioning).\n",
    "\n",
    "    Returns:\n",
    "        Tensor of shape corresponding to the original dimensions before window partitioning.\n",
    "\n",
    "    Example:\n",
    "        >>> import torch\n",
    "        >>> # Example 3D tensor with batch size 1, depth 8, height 8, width 8, and 3 channels (partitioned into windows)\n",
    "        >>> windows = torch.randn(8, 4*4*4, 3)  # 8 windows, each with size 4x4x4 and 3 channels\n",
    "        >>> window_size = (4, 4, 4)\n",
    "        >>> dims = (1, 8, 8, 8)  # Original dimensions: (batch_size, depth, height, width)\n",
    "        >>> x = window_reverse(windows, window_size, dims)\n",
    "        >>> print(\"Shape of reconstructed tensor:\", x.shape)\n",
    "        >>> # Output shape: (1, 8, 8, 8, 3)\n",
    "    \"\"\"\n",
    "\n",
    "    # Handle 3D data (e.g., 5D input tensor with shape (batch_size, depth, height, width, channels))\n",
    "    if len(dims) == 4:\n",
    "        b, d, h, w = dims  # Unpack batch size and spatial dimensions (depth, height, width)\n",
    "\n",
    "        # Reshape windows to form a structured tensor with individual window dimensions reassembled\n",
    "        x = windows.view(\n",
    "            b,\n",
    "            d // window_size[0],  # Number of windows along the depth dimension\n",
    "            h // window_size[1],  # Number of windows along the height dimension\n",
    "            w // window_size[2],  # Number of windows along the width dimension\n",
    "            window_size[0],  # Depth of each window\n",
    "            window_size[1],  # Height of each window\n",
    "            window_size[2],  # Width of each window\n",
    "            -1,  # Number of channels (kept the same)\n",
    "        )\n",
    "\n",
    "        # Permute dimensions to restore the original spatial arrangement by rearranging window dimensions\n",
    "        # The order of permutation restores the windows to their original tensor layout\n",
    "        x = x.permute(0, 1, 4, 2, 5, 3, 6, 7).contiguous().view(b, d, h, w, -1)\n",
    "\n",
    "    else:\n",
    "        # Raise an error if unsupported dimensions are provided\n",
    "        raise ValueError(\"Unsupported dimensions. Expected input to have length of 4 dimensions (b, d, h, w).\")\n",
    "\n",
    "    # Return the tensor with original spatial dimensions restored\n",
    "    return x\n",
    "\n",
    "\n",
    "def get_window_size(x_size, window_size, shift_size=None):\n",
    "    \"\"\"Computing window size based on: \"Liu et al.,\n",
    "    Swin Transformer: Hierarchical Vision Transformer using Shifted Windows\n",
    "    <https://arxiv.org/abs/2103.14030>\"\n",
    "    https://github.com/microsoft/Swin-Transformer\n",
    "\n",
    "    Computing window size and optional shift size adjustments based on the input size.\n",
    "\n",
    "    This function adjusts the window size and shift size based on the dimensions of the input (`x_size`).\n",
    "    If the input size for a specific dimension is smaller than or equal to the corresponding window size,\n",
    "    the function sets the window size to the input size and the shift size to zero for that dimension.\n",
    "\n",
    "    Args:\n",
    "        x_size (tuple): The input size as a tuple of dimensions (e.g., height, width, depth).\n",
    "        window_size (tuple): The local window size for each dimension.\n",
    "        shift_size (tuple, optional): The amount to shift the window. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Adjusted window size. If `shift_size` is provided, also returns the adjusted shift size.\n",
    "\n",
    "    Example:\n",
    "        >>> x_size = (10, 20, 15)\n",
    "        >>> window_size = (7, 7, 7)\n",
    "        >>> shift_size = (3, 3, 3)\n",
    "        >>> get_window_size(x_size, window_size, shift_size)\n",
    "        ((7, 7, 7), (0, 3, 3))\n",
    "    \"\"\"\n",
    "    # Create a mutable list from the provided window_size for adjustments\n",
    "    use_window_size = list(window_size)\n",
    "    if shift_size is not None:\n",
    "        # Create a mutable list from the provided shift_size for adjustments\n",
    "        use_shift_size = list(shift_size)\n",
    "\n",
    "    # Iterate over each dimension of the input size\n",
    "    for i in range(len(x_size)):\n",
    "        # If the input size in the current dimension is less than or equal to the window size\n",
    "        if x_size[i] <= window_size[i]:\n",
    "            # Adjust the window size to match the input size\n",
    "            use_window_size[i] = x_size[i]\n",
    "            if shift_size is not None:\n",
    "                # Set the shift size to 0 for this dimension since the window size matches input size\n",
    "                use_shift_size[i] = 0\n",
    "\n",
    "    # Return adjusted window size and optionally adjusted shift size\n",
    "    if shift_size is None:\n",
    "        return tuple(use_window_size)\n",
    "    else:\n",
    "        return tuple(use_window_size), tuple(use_shift_size)\n",
    "\n",
    "\n",
    "def compute_mask(dims, window_size, shift_size, device):\n",
    "    \"\"\"\n",
    "    Computing region masks based on: \"Liu et al.,\n",
    "    Swin Transformer: Hierarchical Vision Transformer using Shifted Windows\n",
    "    <https://arxiv.org/abs/2103.14030>\"\n",
    "    https://github.com/microsoft/Swin-TransformerComputing region masks based on: \"Liu et al., Swin Transformer: Hierarchical Vision Transformer using Shifted Windows\"\n",
    "    https://arxiv.org/abs/2103.14030\n",
    "\n",
    "    This function divides the input tensor into regions, assigns a unique integer label to each region, and creates an\n",
    "    attention mask (`attn_mask`) by comparing labels within and across windows. This mask restricts attention computations\n",
    "    to only valid elements, optimizing efficiency and adhering to the Swin Transformer's local attention mechanism.\n",
    "\n",
    "    Args:\n",
    "        dims: dimension values.\n",
    "        window_size: local window size.\n",
    "        shift_size: shift size.\n",
    "        device: device.\n",
    "    \"\"\"\n",
    "\n",
    "    cnt = 0\n",
    "\n",
    "    if len(dims) == 3:\n",
    "        # For a 3D input tensor (depth, height, width), create an initial mask filled with zeros\n",
    "        d, h, w = dims\n",
    "        img_mask = torch.zeros((1, d, h, w, 1), device=device)\n",
    "\n",
    "        # Dividing the input tensor into 3D regions using slices for depth, height, and width dimensions\n",
    "        # Each dimension is divided into three slices based on the window and shift sizes.\n",
    "        # Example window_size = (7, 7, 7), shift_size = (3, 3, 3) will divide each dimension as:\n",
    "        # - slice(-window_size[0]):  Covers elements from the beginning up to index, in this case, -7 (not inclusive)\n",
    "        # - slice(-window_size[0], -shift_size[0]): Covers elements between indices `-7` to `-3`\n",
    "        # - slice(-shift_size[0], None): Covers the last `3` elements\n",
    "        for d in slice(-window_size[0]), slice(-window_size[0], -shift_size[0]), slice(-shift_size[0], None):\n",
    "            for h in slice(-window_size[1]), slice(-window_size[1], -shift_size[1]), slice(-shift_size[1], None):\n",
    "                for w in slice(-window_size[2]), slice(-window_size[2], -shift_size[2]), slice(-shift_size[2], None):\n",
    "                    # Assign a unique integer label to each region within the 3D space\n",
    "                    img_mask[:, d, h, w, :] = cnt\n",
    "                    cnt += 1\n",
    "    elif len(dims) == 2:\n",
    "        raise ValueError(\"2D input is not supported. Please provide a 3D input with dimensions (d, h, w).\")\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported dimensions. Expected input to have length of 3 dimensions (d, h, w).\")\n",
    "\n",
    "    # The `img_mask` tensor, which contains unique integer labels for different regions of the input tensor,\n",
    "    # is now partitioned into smaller windows of size specified by `window_size` using the `window_partition` function.\n",
    "    # This function divides the tensor spatially into separate non-overlapping windows for localized processing,\n",
    "    # facilitating efficient computation of self-attention in the Swin Transformer by focusing within each window.\n",
    "    # mask_windows => tensor of shape (num_windows, window_size_product, 1)\n",
    "    mask_windows = window_partition(img_mask, window_size)\n",
    "\n",
    "    # Since `img_mask` initially had an extra singleton dimension (i.e., shape (1, d, h, w, 1) for 3D data),\n",
    "    # we remove this last dimension using `squeeze(-1)`.\n",
    "    # This operation reduces the dimensionality of `mask_windows` by eliminating the singleton dimension,\n",
    "    # resulting in a tensor that contains the labels of regions in each window.\n",
    "    # mask_windows => tensor of shape (num_windows, window_size_product)\n",
    "    mask_windows = mask_windows.squeeze(-1)\n",
    "\n",
    "    # Create an attention mask for controlling the attention mechanism in the Swin Transformer.\n",
    "    # The attention mask is generated by comparing the labels of elements in different windows:\n",
    "    # - `mask_windows.unsqueeze(1)` expands the dimensions of `mask_windows` for broadcasting so that\n",
    "    #   each window label can be compared with every other window label.\n",
    "    # - `mask_windows.unsqueeze(2)` similarly expands the dimensions of `mask_windows` for element-wise comparisons.\n",
    "\n",
    "    # The subtraction operation (`mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)`) generates a tensor\n",
    "    # that contains zero values when elements belong to the same window and non-zero values otherwise.\n",
    "    # This tensor effectively encodes information about which elements can attend to each other:\n",
    "    # - Elements with zero values (same window labels) can attend to each other.\n",
    "    # - Elements with non-zero values (different window labels) cannot attend to each other.\n",
    "\n",
    "    # The mask is further refined using `masked_fill`:\n",
    "    # - `masked_fill(attn_mask != 0, float(-100.0))` sets large negative values (-100.0) for elements\n",
    "    #   that belong to different windows, effectively blocking attention between them by making their\n",
    "    #   attention scores very low (close to negative infinity).\n",
    "    # - `masked_fill(attn_mask == 0, float(0.0))` sets zero values for elements within the same window,\n",
    "    #   allowing attention between them without modification of their scores.\n",
    "    attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)\n",
    "    attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))\n",
    "\n",
    "\n",
    "    return attn_mask\n",
    "\n",
    "\n",
    "class SwinTransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Swin Transformer block based on: \"Liu et al.,\n",
    "    Swin Transformer: Hierarchical Vision Transformer using Shifted Windows\n",
    "    <https://arxiv.org/abs/2103.14030>\"\n",
    "    https://github.com/microsoft/Swin-Transformer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        num_heads: int,\n",
    "        window_size: Sequence[int],\n",
    "        shift_size: Sequence[int],\n",
    "        mlp_ratio: float = 4.0,\n",
    "        qkv_bias: bool = True,\n",
    "        drop: float = 0.0,\n",
    "        attn_drop: float = 0.0,\n",
    "        drop_path: float = 0.0,\n",
    "        act_layer: str = \"GELU\",\n",
    "        norm_layer: type[LayerNorm] = nn.LayerNorm,\n",
    "        use_checkpoint: bool = False,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dim: Number of input feature channels.\n",
    "            num_heads: Number of attention heads in the multi-head self-attention mechanism.\n",
    "            window_size: Size of the local window for attention computations.\n",
    "            shift_size: Size of the shift applied to the window during the shifted-window mechanism.\n",
    "            mlp_ratio: Ratio of the hidden dimension size in the MLP to the embedding dimension size.\n",
    "            qkv_bias: Boolean indicating whether to add a bias term to the query, key, and value tensors.\n",
    "            drop: Dropout rate for the final output projection.\n",
    "            attn_drop: Dropout rate for the attention scores.\n",
    "            drop_path: Drop path (stochastic depth) rate.\n",
    "            act_layer: Activation function used in the MLP layers (e.g., GELU).\n",
    "            norm_layer: Normalization layer applied before and after attention (default: LayerNorm).\n",
    "            use_checkpoint: If True, use gradient checkpointing to save memory during training.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.num_heads = num_heads\n",
    "        self.window_size = window_size\n",
    "        self.shift_size = shift_size\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "        self.use_checkpoint = use_checkpoint\n",
    "\n",
    "        # Layer normalization before self-attention\n",
    "        self.norm1 = norm_layer(dim)\n",
    "\n",
    "        # Window-based multi-head self-attention mechanism\n",
    "        self.attn = WindowAttention(\n",
    "            dim,\n",
    "            window_size=self.window_size,\n",
    "            num_heads=num_heads,\n",
    "            qkv_bias=qkv_bias,\n",
    "            attn_drop=attn_drop,\n",
    "            proj_drop=drop,\n",
    "        )\n",
    "\n",
    "        # Optional drop path (stochastic depth)\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()\n",
    "\n",
    "        # Second normalization layer\n",
    "        self.norm2 = norm_layer(dim)\n",
    "\n",
    "        # MLP block with one hidden layer\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(hidden_size=dim, mlp_dim=mlp_hidden_dim, act=act_layer, dropout_rate=drop, dropout_mode=\"swin\")\n",
    "\n",
    "    def forward_part1(self, x, mask_matrix):\n",
    "        \"\"\"\n",
    "        Applies the first part of the forward pass in the Swin Transformer block, including:\n",
    "          - 1. Layer normalization\n",
    "          - 2. Handling input shape and padding\n",
    "          - 3. Applying window partitioning and shifted window self-attention\n",
    "          - 4. Reversing window operations\n",
    "          - 5. Removing padding (if applied)\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input tensor of shape (batch_size, depth, height, width, channels) or 4D equivalent.\n",
    "            mask_matrix (Tensor): Precomputed attention mask to control self-attention behavior.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Output tensor after applying all transformations.\n",
    "        \"\"\"\n",
    "        # Get the shape of the input tensor (could be 5D or 4D)\n",
    "        x_shape = x.size()\n",
    "\n",
    "        # Apply layer normalization to stabilize and optimize the learning process\n",
    "        x = self.norm1(x)\n",
    "\n",
    "        # Handle 5D input tensors (e.g., 3D images)\n",
    "        if len(x_shape) == 5:\n",
    "            b, d, h, w, c = x.shape  # Unpack the dimensions: batch size, depth, height, width, and channels\n",
    "\n",
    "            # Calculate effective window size and shift size based on input dimensions\n",
    "            window_size, shift_size = get_window_size((d, h, w), self.window_size, self.shift_size)\n",
    "\n",
    "            # Calculate padding needed to make dimensions divisible by the window size\n",
    "            # No padding on the left/top/front sides (pad_l, pad_t, pad_d0 are zero)\n",
    "            pad_l = pad_t = pad_d0 = 0\n",
    "            # Calculate padding for the right/bottom/back sides to ensure divisibility\n",
    "            pad_d1 = (window_size[0] - d % window_size[0]) % window_size[0]  # Depth dimension padding\n",
    "            pad_b = (window_size[1] - h % window_size[1]) % window_size[1]  # Height dimension padding\n",
    "            pad_r = (window_size[2] - w % window_size[2]) % window_size[2]  # Width dimension padding\n",
    "\n",
    "            # Apply padding to the input tensor to ensure its dimensions are divisible by the window size\n",
    "            # Padding order: (width padding right, width padding left, height padding bottom, height padding top, ...)\n",
    "            x = F.pad(x, (0, 0, pad_l, pad_r, pad_t, pad_b, pad_d0, pad_d1))\n",
    "\n",
    "            # Update dimension variables to reflect changes after padding\n",
    "            _, dp, hp, wp, _ = x.shape  # dp, hp, wp are the new depth, height, and width after padding\n",
    "            dims = [b, dp, hp, wp]  # Store updated dimensions\n",
    "\n",
    "        else:  # Handle other input tensors (e.g., 2D input data with channels and batch dimensions)\n",
    "            raise ValueError(\"Unsupported dimensions. Expected input to have length of 5 dimensions (b, d, h, w, c).\")\n",
    "\n",
    "        # Check if any shift is required (shift_size > 0)\n",
    "        if any(i > 0 for i in shift_size):\n",
    "            # Apply negative rolling shift along depth, height, and width if input is 5D\n",
    "            if len(x_shape) == 5:\n",
    "                # Roll (shift) elements along depth, height, and width dimensions (CYCLE SHIFT)\n",
    "                shifted_x = torch.roll(x, shifts=(-shift_size[0], -shift_size[1], -shift_size[2]), dims=(1, 2, 3))\n",
    "            # Set attention mask to the precomputed mask matrix for shifted attention\n",
    "            attn_mask = mask_matrix\n",
    "        else:\n",
    "            # No shift is needed, retain original input\n",
    "            shifted_x = x\n",
    "            attn_mask = None  # No attention mask is required for non-shifted windows\n",
    "\n",
    "        # Partition the (shifted) input tensor into windows for applying window-based self-attention\n",
    "        x_windows = window_partition(shifted_x, window_size)\n",
    "        # After window partitioning:\n",
    "        # - For 3D input, x_windows has shape (num_windows, window_d * window_h * window_w, channels)\n",
    "        # Here, `num_windows` is the number of windows formed, `window_d`, `window_h`, `window_w` are\n",
    "        # window dimensions, and `channels` is the number of feature channels.\n",
    "\n",
    "        # Apply window-based self-attention mechanism using `self.attn`\n",
    "        # This computes self-attention independently within each window\n",
    "        attn_windows = self.attn(x_windows, mask=attn_mask)\n",
    "        # After applying attention:\n",
    "        # - attn_windows retains the shape (num_windows, window_area, channels) where `window_area` is the product of window dimensions (e.g., window_d * window_h * window_w for 3D).\n",
    "        # - The attention mechanism is performed independently within each window, and the shape of the output remains consistent.\n",
    "\n",
    "        # attn_windows has shape (num_windows, window_area, channels) after attention\n",
    "        # Reverse window partitioning by reshaping attention windows back to original shape per window\n",
    "        attn_windows = attn_windows.view(-1, *(window_size + (c,)))\n",
    "        # After reshaping:\n",
    "        # - attn_windows now has shape (num_windows, window_d, window_h, window_w, channels) for 3D input\n",
    "\n",
    "        # Restore spatial structure of the original input using window reversing\n",
    "        shifted_x = window_reverse(attn_windows, window_size, dims)\n",
    "        # After window_reverse:\n",
    "        # - shifted_x is restored to its spatial structure with shape (batch_size, depth, height, width, channels) for 3D\n",
    "\n",
    "\n",
    "        # If a shift was applied earlier, roll back the shift to restore original structure\n",
    "        if any(i > 0 for i in shift_size):\n",
    "            if len(x_shape) == 5:\n",
    "                # Apply positive rolling shift to revert the previous shift operation (REVERSE CYCLE SHIFT)\n",
    "                x = torch.roll(shifted_x, shifts=(shift_size[0], shift_size[1], shift_size[2]), dims=(1, 2, 3))\n",
    "        else:\n",
    "            # No shift was applied, retain the tensor as is\n",
    "            x = shifted_x\n",
    "\n",
    "        # Remove padding if any was applied earlier to restore original dimensions\n",
    "        if len(x_shape) == 5:\n",
    "            # Check if any padding was added in depth, height, or width dimensions\n",
    "            if pad_d1 > 0 or pad_r > 0 or pad_b > 0:\n",
    "                # Slice out the padded regions to return to original dimensions\n",
    "                x = x[:, :d, :h, :w, :].contiguous()\n",
    "\n",
    "        return x  # Return the processed tensor\n",
    "\n",
    "\n",
    "    def forward_part2(self, x):\n",
    "        \"\"\"\n",
    "        Applies a series of operations on the input tensor 'x', including:\n",
    "          - 1. Normalization\n",
    "          - 2. Multi-layer perceptron (MLP) Transformation\n",
    "          - 3. Drop Path Regularization\n",
    "        This function contributes to the forward pass (second part) of the Swin Transformer block.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input tensor to be processed.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Output tensor after applying normalization, MLP, and drop path regularization.\n",
    "        \"\"\"\n",
    "        # Normalize the input tensor 'x' across the last dimension\n",
    "        # Layer normalization helps stabilize and optimize the learning process\n",
    "        x = self.norm2(x)\n",
    "\n",
    "        # Pass the normalized tensor through a Multi-Layer Perceptron (MLP)\n",
    "        # The MLP typically includes linear transformations, non-linear activations, and optional dropout\n",
    "        x = self.mlp(x)\n",
    "\n",
    "        # Apply drop path regularization (also known as stochastic depth)\n",
    "        # Drop path regularization randomly drops entire layers or paths during training to improve generalization\n",
    "        # In this case, if dropped, the output of this function will be nullified, effectively skipping the contribution of this part during training\n",
    "        return self.drop_path(x)\n",
    "\n",
    "\n",
    "\n",
    "    def load_from(self, weights, n_block, layer):\n",
    "        \"\"\"\n",
    "        Load weights from a pre-trained Swin Transformer model.\n",
    "        This method copies specific parameters from a state_dict into the corresponding layers of this block.\n",
    "        \"\"\"\n",
    "        root = f\"module.{layer}.0.blocks.{n_block}.\"\n",
    "        block_names = [\n",
    "            \"norm1.weight\",\n",
    "            \"norm1.bias\",\n",
    "            \"attn.relative_position_bias_table\",\n",
    "            \"attn.relative_position_index\",\n",
    "            \"attn.qkv.weight\",\n",
    "            \"attn.qkv.bias\",\n",
    "            \"attn.proj.weight\",\n",
    "            \"attn.proj.bias\",\n",
    "            \"norm2.weight\",\n",
    "            \"norm2.bias\",\n",
    "            \"mlp.fc1.weight\",\n",
    "            \"mlp.fc1.bias\",\n",
    "            \"mlp.fc2.weight\",\n",
    "            \"mlp.fc2.bias\",\n",
    "        ]\n",
    "        with torch.no_grad():\n",
    "            # Copy each relevant parameter from the weights\n",
    "            self.norm1.weight.copy_(weights[\"state_dict\"][root + block_names[0]])\n",
    "            self.norm1.bias.copy_(weights[\"state_dict\"][root + block_names[1]])\n",
    "            self.attn.relative_position_bias_table.copy_(weights[\"state_dict\"][root + block_names[2]])\n",
    "            self.attn.relative_position_index.copy_(weights[\"state_dict\"][root + block_names[3]])\n",
    "            self.attn.qkv.weight.copy_(weights[\"state_dict\"][root + block_names[4]])\n",
    "            self.attn.qkv.bias.copy_(weights[\"state_dict\"][root + block_names[5]])\n",
    "            self.attn.proj\n",
    "\n",
    "    def forward(self, x, mask_matrix):\n",
    "        \"\"\"\n",
    "        Forward pass of the SwinTransformerBlock.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor.\n",
    "            mask_matrix (torch.Tensor): Attention mask matrix to restrict computations.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor after applying the Swin Transformer block.\n",
    "        \"\"\"\n",
    "        # Save a copy of the input tensor as a shortcut (residual connection)\n",
    "        shortcut = x\n",
    "\n",
    "        # Check if gradient checkpointing is used; if so, compute the first part of the forward pass\n",
    "        if self.use_checkpoint:\n",
    "            # Use PyTorch's checkpointing to save memory during training by re-computing forward pass during backward pass\n",
    "            x = checkpoint.checkpoint(self.forward_part1, x, mask_matrix, use_reentrant=False)\n",
    "        else:\n",
    "            # Regular first part of the forward pass without checkpointing\n",
    "            x = self.forward_part1(x, mask_matrix)\n",
    "\n",
    "        # Add the residual connection and apply drop path regularization\n",
    "        x = shortcut + self.drop_path(x)\n",
    "\n",
    "        # Perform the second part of the forward pass, with optional checkpointing for memory savings\n",
    "        if self.use_checkpoint:\n",
    "            x = x + checkpoint.checkpoint(self.forward_part2, x, use_reentrant=False)\n",
    "        else:\n",
    "            # If not using checkpointing, directly compute the second part of the forward pass\n",
    "            x = x + self.forward_part2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class BasicLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Basic Swin Transformer layer in one stage based on: \"Liu et al.,\n",
    "    Swin Transformer: Hierarchical Vision Transformer using Shifted Windows\n",
    "    <https://arxiv.org/abs/2103.14030>\"\n",
    "    https://github.com/microsoft/Swin-Transformer\n",
    "\n",
    "    This class implements a single stage of Swin Transformer, which operates on input data\n",
    "    using window-based self-attention and shift mechanisms to enhance spatial interactions.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        depth: int,\n",
    "        num_heads: int,\n",
    "        window_size: Sequence[int],\n",
    "        drop_path: list,\n",
    "        mlp_ratio: float = 4.0,\n",
    "        qkv_bias: bool = False,\n",
    "        drop: float = 0.0,\n",
    "        attn_drop: float = 0.0,\n",
    "        norm_layer: type[LayerNorm] = nn.LayerNorm,\n",
    "        downsample: nn.Module | None = None,\n",
    "        use_checkpoint: bool = False,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initializes the BasicLayer for a Swin Transformer block stage.\n",
    "\n",
    "        Args:\n",
    "            dim (int): Number of feature channels for the input.\n",
    "            depth (int): Number of Swin Transformer blocks in this stage.\n",
    "            num_heads (int): Number of attention heads used in the multi-head attention module.\n",
    "            window_size (Sequence[int]): Size of the local attention window (e.g., [7, 7, 7]).\n",
    "            drop_path (list): List containing the stochastic depth rates for each block.\n",
    "            mlp_ratio (float): Ratio of MLP hidden dimensions to the input dimension.\n",
    "            qkv_bias (bool): If True, adds a learnable bias to query, key, and value tensors.\n",
    "            drop (float): Dropout rate applied to MLP layers.\n",
    "            attn_drop (float): Dropout rate applied to attention weights.\n",
    "            norm_layer (LayerNorm): Normalization layer used in the blocks.\n",
    "            downsample (nn.Module | None): Optional downsampling module applied at the end of the layer.\n",
    "            use_checkpoint (bool): If True, enables gradient checkpointing to reduce memory usage.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Set window size, shift size, and no-shift size for the layer\n",
    "        self.window_size = window_size\n",
    "        self.shift_size = tuple(i // 2 for i in window_size)  # Shift size is half of the window size\n",
    "        self.no_shift = tuple(0 for i in window_size)  # No shift is represented by zeros\n",
    "        self.depth = depth  # Number of Swin Transformer blocks in this stage\n",
    "        self.use_checkpoint = use_checkpoint  # Use checkpointing for memory efficiency\n",
    "\n",
    "        # Create a list of Swin Transformer blocks for this stage\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [\n",
    "                SwinTransformerBlock(\n",
    "                    dim=dim,  # Number of feature channels\n",
    "                    num_heads=num_heads,  # Number of attention heads\n",
    "                    window_size=self.window_size,  # Window size for attention\n",
    "                    shift_size=self.no_shift if (i % 2 == 0) else self.shift_size,  # Alternate between no shift and shift\n",
    "                    mlp_ratio=mlp_ratio,  # MLP hidden dimension ratio\n",
    "                    qkv_bias=qkv_bias,  # Add bias to query, key, value tensors if True\n",
    "                    drop=drop,  # Dropout rate for MLP\n",
    "                    attn_drop=attn_drop,  # Dropout rate for attention\n",
    "                    drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,  # Drop path rate for stochastic depth\n",
    "                    norm_layer=norm_layer,  # Normalization layer\n",
    "                    use_checkpoint=use_checkpoint,  # Use gradient checkpointing\n",
    "                )\n",
    "                for i in range(depth)  # Create `depth` number of blocks\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Initialize optional downsampling module if provided\n",
    "        self.downsample = downsample\n",
    "        if callable(self.downsample):  # Check if downsample is callable\n",
    "            self.downsample = downsample(dim=dim, norm_layer=norm_layer, spatial_dims=len(self.window_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the BasicLayer.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input tensor of shape (b, c, d, h, w) for 3D data.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Output tensor after applying Swin Transformer blocks and optional downsampling.\n",
    "        \"\"\"\n",
    "        # Get the shape of the input tensor (batch size, channels, depth, height, width)\n",
    "        x_shape = x.size()\n",
    "\n",
    "        # Handle 3D input tensors (e.g., volumetric data with batch size, channels, depth, height, width)\n",
    "        if len(x_shape) == 5:\n",
    "            b, c, d, h, w = x_shape  # Extract dimensions from input shape\n",
    "\n",
    "            # Determines the effective window_size and shift_size based on the dimensions\n",
    "            # (d, h, w) of the input tensor and pre-defined window size and shift size values from the instance.\n",
    "\n",
    "            window_size, shift_size = get_window_size((d, h, w), self.window_size, self.shift_size)\n",
    "\n",
    "            x = rearrange(x, \"b c d h w -> b d h w c\")\n",
    "\n",
    "            # dp, hp, and wp values are used to define the \"padded\" dimensions of the input,\n",
    "            # ensuring compatibility with window-based operation\n",
    "            dp = int(np.ceil(d / window_size[0])) * window_size[0]\n",
    "            hp = int(np.ceil(h / window_size[1])) * window_size[1]\n",
    "            wp = int(np.ceil(w / window_size[2])) * window_size[2]\n",
    "\n",
    "            # The `compute_mask` function generates a mask that partitions the input tensor into distinct regions,\n",
    "            # based on specified dimensions (`[dp, hp, wp]`), a `window_size`, and a `shift_size`.\n",
    "            # This mask is used to control the attention mechanism during computation, ensuring that elements\n",
    "            # only attend to others within the same window or shifted region.\n",
    "            #\n",
    "            # Key details:\n",
    "            # - `dims = [dp, hp, wp]` are the padded input dimensions (depth, height, and width).\n",
    "            # - `window_size` specifies the size of each window for partitioning, e.g., (7, 7, 7).\n",
    "            # - `shift_size` specifies the window shift amount, used to enhance spatial interactions.\n",
    "            # - `x.device` indicates where the computation occurs (CPU/GPU).\n",
    "            #\n",
    "            # The function divides the input tensor into regions, assigns a unique integer label to each region,\n",
    "            # and creates an attention mask (`attn_mask`) by comparing labels within and across windows.\n",
    "            # This mask restricts attention computations to only valid elements, optimizing efficiency\n",
    "            # and adhering to the Swin Transformer's local attention mechanism.\n",
    "\n",
    "            attn_mask = compute_mask([dp, hp, wp], window_size, shift_size, x.device)\n",
    "\n",
    "            # Apply each Swin Transformer block in sequence to the input tensor\n",
    "            for blk in self.blocks:\n",
    "                x = blk(x, attn_mask)\n",
    "\n",
    "            # Reshape the output back to its original shape with updated channels\n",
    "            # Before reshaping, 'x' has dimensions (b, dp, hp, wp, c), where dp, hp, and wp may include padding\n",
    "            # After reshaping, 'x' returns to its original spatial dimensions (b, d, h, w, -1), removing any padding\n",
    "            # -1 means that the last dimension size is infered by dividing the total number of elements by (b * d * h * w).\n",
    "            # It adjusts the feature dimension (channel dimension) to accommodate transformations done by the Swin Transformer blocks.\n",
    "            x = x.view(b, d, h, w, -1)\n",
    "\n",
    "            # Apply optional downsampling if a downsample module is defined\n",
    "            if self.downsample is not None:\n",
    "                x = self.downsample(x)\n",
    "\n",
    "            # Rearrange tensor back to original format (b, c, d, h, w)\n",
    "            x = rearrange(x, \"b d h w c -> b c d h w\")\n",
    "\n",
    "        else:\n",
    "            # Raise an error if the input tensor does not have the expected number of dimensions\n",
    "            raise ValueError(\"Unsupported dimensions. Expected input to have length of 5 dimensions (b, d, h, w, c).\")\n",
    "\n",
    "        return x  # Return the processed tensor\n",
    "\n",
    "\n",
    "class PatchMergingV2(nn.Module):\n",
    "    \"\"\"\n",
    "    Patch merging layer based on: \"Liu et al.,\n",
    "    Swin Transformer: Hierarchical Vision Transformer using Shifted Windows\n",
    "    <https://arxiv.org/abs/2103.14030>\"\n",
    "    https://github.com/microsoft/Swin-Transformer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim: int, norm_layer: Type[LayerNorm] = nn.LayerNorm, spatial_dims: int = 3) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dim: number of feature channels.\n",
    "            norm_layer: normalization layer.\n",
    "            spatial_dims: number of spatial dims.\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        if spatial_dims == 3:\n",
    "            self.reduction = nn.Linear(8 * dim, 2 * dim, bias=False)\n",
    "            self.norm = norm_layer(8 * dim)\n",
    "        else:\n",
    "          raise ValueError(f\"expecting 3D dim, got {dim}.\")\n",
    "    def forward(self, x):\n",
    "\n",
    "        x_shape = x.size()\n",
    "        if len(x_shape) == 5:\n",
    "            b, d, h, w, c = x_shape\n",
    "            pad_input = (h % 2 == 1) or (w % 2 == 1) or (d % 2 == 1)\n",
    "            if pad_input:\n",
    "                x = F.pad(x, (0, 0, 0, w % 2, 0, h % 2, 0, d % 2))\n",
    "            x0 = x[:, 0::2, 0::2, 0::2, :]\n",
    "            x1 = x[:, 1::2, 0::2, 0::2, :]\n",
    "            x2 = x[:, 0::2, 1::2, 0::2, :]\n",
    "            x3 = x[:, 0::2, 0::2, 1::2, :]\n",
    "            x4 = x[:, 1::2, 0::2, 1::2, :]\n",
    "            x5 = x[:, 1::2, 1::2, 0::2, :]\n",
    "            x6 = x[:, 0::2, 1::2, 1::2, :]\n",
    "            x7 = x[:, 1::2, 1::2, 1::2, :]\n",
    "            x = torch.cat([x0, x1, x2, x3, x4, x5, x6, x7], -1)\n",
    "\n",
    "        else:\n",
    "          raise ValueError(f\"expecting 5D x, got {x.shape}.\")\n",
    "\n",
    "        x = self.norm(x)\n",
    "        x = self.reduction(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class PatchMerging(PatchMergingV2):\n",
    "    \"\"\"The `PatchMerging` module previously defined in v0.9.0.\"\"\"\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_shape = x.size()\n",
    "        if len(x_shape) == 4:\n",
    "            return super().forward(x)\n",
    "        if len(x_shape) != 5:\n",
    "            raise ValueError(f\"expecting 5D x, got {x.shape}.\")\n",
    "        b, d, h, w, c = x_shape\n",
    "        pad_input = (h % 2 == 1) or (w % 2 == 1) or (d % 2 == 1)\n",
    "        if pad_input:\n",
    "            x = F.pad(x, (0, 0, 0, w % 2, 0, h % 2, 0, d % 2))\n",
    "        x0 = x[:, 0::2, 0::2, 0::2, :]\n",
    "        x1 = x[:, 1::2, 0::2, 0::2, :]\n",
    "        x2 = x[:, 0::2, 1::2, 0::2, :]\n",
    "        x3 = x[:, 0::2, 0::2, 1::2, :]\n",
    "        x4 = x[:, 1::2, 0::2, 1::2, :]\n",
    "        x5 = x[:, 0::2, 1::2, 0::2, :]\n",
    "        x6 = x[:, 0::2, 0::2, 1::2, :]\n",
    "        x7 = x[:, 1::2, 1::2, 1::2, :]\n",
    "        x = torch.cat([x0, x1, x2, x3, x4, x5, x6, x7], -1)\n",
    "        x = self.norm(x)\n",
    "        x = self.reduction(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "MERGING_MODE = {\"merging\": PatchMerging, \"mergingv2\": PatchMergingV2}\n",
    "\n",
    "class SwinTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Swin Transformer based on: \"Liu et al.,\n",
    "    Swin Transformer: Hierarchical Vision Transformer using Shifted Windows\n",
    "    <https://arxiv.org/abs/2103.14030>\"\n",
    "    https://github.com/microsoft/Swin-Transformer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_chans: int,\n",
    "        embed_dim: int,\n",
    "        window_size: Sequence[int],\n",
    "        patch_size: Sequence[int],\n",
    "        depths: Sequence[int],\n",
    "        num_heads: Sequence[int],\n",
    "        mlp_ratio: float = 4.0,\n",
    "        qkv_bias: bool = True,\n",
    "        drop_rate: float = 0.0,\n",
    "        attn_drop_rate: float = 0.0,\n",
    "        drop_path_rate: float = 0.0,\n",
    "        norm_layer: type[LayerNorm] = nn.LayerNorm,\n",
    "        patch_norm: bool = False,\n",
    "        use_checkpoint: bool = False,\n",
    "        spatial_dims: int = 3,\n",
    "        downsample=\"merging\",\n",
    "        use_v2=False,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initializes the Swin Transformer model.\n",
    "\n",
    "        Args:\n",
    "            in_chans: Number of input channels.\n",
    "            embed_dim: Dimension of linear projection output channels.\n",
    "            window_size: Local window size used for window-based attention.\n",
    "            patch_size: Size of input patches.\n",
    "            depths: Number of layers in each transformer stage.\n",
    "            num_heads: Number of attention heads in each stage.\n",
    "            mlp_ratio: Ratio of MLP hidden dimension to embedding dimension.\n",
    "            qkv_bias: Boolean indicating whether to add a learnable bias to query, key, and value tensors.\n",
    "            drop_rate: Dropout rate applied to the input embeddings.\n",
    "            attn_drop_rate: Dropout rate specific to the attention mechanism.\n",
    "            drop_path_rate: Rate for stochastic depth (drop path).\n",
    "            norm_layer: Normalization layer type.\n",
    "            patch_norm: Whether to add normalization after patch embedding.\n",
    "            use_checkpoint: Enables gradient checkpointing for reduced memory usage.\n",
    "            spatial_dims: Number of spatial dimensions (e.g., 3 for 3D data).\n",
    "            downsample: Module used for downsampling between stages.\n",
    "            use_v2: Boolean indicating whether to use an updated version with residual convolutional blocks.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_layers = len(depths)  # Number of stages in the transformer\n",
    "        self.embed_dim = embed_dim  # Embedding dimension size\n",
    "        self.patch_norm = patch_norm  # Whether to normalize after patch embedding\n",
    "        self.window_size = window_size  # Size of the attention window\n",
    "        self.patch_size = patch_size  # Patch size for embedding input\n",
    "\n",
    "        # Initialize the patch embedding layer\n",
    "        self.patch_embed = PatchEmbed(\n",
    "            patch_size=patch_size,\n",
    "            in_chans=in_chans,\n",
    "            embed_dim=embed_dim,\n",
    "            norm_layer=norm_layer if self.patch_norm else None,  # Apply normalization if specified\n",
    "            spatial_dims=spatial_dims,\n",
    "        )\n",
    "\n",
    "        # Dropout applied to positionally encoded input embeddings\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "\n",
    "        # Calculate drop path rate schedule for each layer using linear interpolation\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]\n",
    "\n",
    "        self.use_v2 = use_v2  # Boolean to check if using version 2\n",
    "        self.layers1 = nn.ModuleList()  # List for the first stage's layers\n",
    "        self.layers2 = nn.ModuleList()  # List for the second stage's layers\n",
    "        self.layers3 = nn.ModuleList()  # List for the third stage's layers\n",
    "        self.layers4 = nn.ModuleList()  # List for the fourth stage's layers\n",
    "\n",
    "        # If using version 2, initialize additional layers with residual convolutional blocks\n",
    "        if self.use_v2:\n",
    "            self.layers1c = nn.ModuleList()\n",
    "            self.layers2c = nn.ModuleList()\n",
    "            self.layers3c = nn.ModuleList()\n",
    "            self.layers4c = nn.ModuleList()\n",
    "\n",
    "        # Set up the downsampling module\n",
    "        down_sample_mod = look_up_option(downsample, MERGING_MODE) if isinstance(downsample, str) else downsample\n",
    "\n",
    "        # Loop through each stage and initialize layers\n",
    "        for i_layer in range(self.num_layers):\n",
    "            # Create a BasicLayer instance for each stage\n",
    "            layer = BasicLayer(\n",
    "                dim=int(embed_dim * 2**i_layer),  # Double the dimension for each stage\n",
    "                depth=depths[i_layer],  # Number of layers in this stage\n",
    "                num_heads=num_heads[i_layer],  # Number of attention heads\n",
    "                window_size=self.window_size,  # Size of the attention window\n",
    "                drop_path=dpr[sum(depths[:i_layer]) : sum(depths[: i_layer + 1])],  # Drop path rate for this stage\n",
    "                mlp_ratio=mlp_ratio,  # MLP hidden dimension to embedding dimension ratio\n",
    "                qkv_bias=qkv_bias,  # Bias for query, key, and value tensors\n",
    "                drop=drop_rate,  # General dropout rate\n",
    "                attn_drop=attn_drop_rate,  # Attention dropout rate\n",
    "                norm_layer=norm_layer,  # Normalization layer type\n",
    "                downsample=down_sample_mod,  # Downsampling module\n",
    "                use_checkpoint=use_checkpoint,  # Enable gradient checkpointing for memory efficiency\n",
    "            )\n",
    "\n",
    "            # Append the layer to the appropriate module list\n",
    "            if i_layer == 0:\n",
    "                self.layers1.append(layer)\n",
    "            elif i_layer == 1:\n",
    "                self.layers2.append(layer)\n",
    "            elif i_layer == 2:\n",
    "                self.layers3.append(layer)\n",
    "            elif i_layer == 3:\n",
    "                self.layers4.append(layer)\n",
    "\n",
    "            # Add corresponding residual convolutional layers if using version 2\n",
    "            if self.use_v2:\n",
    "                layerc = UnetrBasicBlock(\n",
    "                    spatial_dims=spatial_dims,\n",
    "                    in_channels=embed_dim * 2**i_layer,\n",
    "                    out_channels=embed_dim * 2**i_layer,\n",
    "                    kernel_size=3,\n",
    "                    stride=1,\n",
    "                    norm_name=\"instance\",\n",
    "                    res_block=True,\n",
    "                )\n",
    "                if i_layer == 0:\n",
    "                    self.layers1c.append(layerc)\n",
    "                elif i_layer == 1:\n",
    "                    self.layers2c.append(layerc)\n",
    "                elif i_layer == 2:\n",
    "                    self.layers3c.append(layerc)\n",
    "                elif i_layer == 3:\n",
    "                    self.layers4c.append(layerc)\n",
    "\n",
    "        # Calculate the number of features after the final stage\n",
    "        self.num_features = int(embed_dim * 2 ** (self.num_layers - 1))\n",
    "\n",
    "    def proj_out(self, x, normalize=False):\n",
    "        \"\"\"\n",
    "        Applies projection and optional normalization to the input tensor.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input tensor.\n",
    "            normalize (bool): Whether to apply layer normalization.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Projected and normalized tensor.\n",
    "        \"\"\"\n",
    "        if normalize:\n",
    "            x_shape = x.shape\n",
    "            ch = int(x_shape[1])  # Number of channels\n",
    "            if len(x_shape) == 5:  # If input is 5D (3D spatial data)\n",
    "                x = rearrange(x, \"b c d h w -> b d h w c\")  # Rearrange dimensions for normalization\n",
    "                x = F.layer_norm(x, [ch])  # Apply layer normalization\n",
    "                x = rearrange(x, \"b d h w c -> b c d h w\")  # Rearrange back to original dimensions\n",
    "            else:\n",
    "                # Handle other input dimensions (e.g., 2D data)\n",
    "                raise ValueError(\"Unsupported dimensions. Expected input to have length of 5 dimensions (b, d, h, w, c).\")\n",
    "        return x\n",
    "\n",
    "    def forward(self, x, normalize=True):\n",
    "        \"\"\"\n",
    "        Forward pass for the Swin Transformer.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input tensor.\n",
    "            normalize (bool): Whether to apply normalization after projection.\n",
    "\n",
    "        Returns:\n",
    "            List[Tensor]: Output tensors from each stage.\n",
    "        \"\"\"\n",
    "        # Apply patch embedding and dropout to the input\n",
    "        x0 = self.patch_embed(x)\n",
    "        x0 = self.pos_drop(x0)\n",
    "\n",
    "        # Apply projection and normalization\n",
    "        x0_out = self.proj_out(x0, normalize)\n",
    "\n",
    "        # Forward pass through each stage, conditionally using residual convolutional layers if specified\n",
    "        if self.use_v2:\n",
    "            x0 = self.layers1c[0](x0.contiguous())\n",
    "        x1 = self.layers1[0](x0.contiguous())\n",
    "        x1_out = self.proj_out(x1, normalize)\n",
    "        if self.use_v2:\n",
    "            x1 = self.layers2c[0](x1.contiguous())\n",
    "        x2 = self.layers2[0](x1.contiguous())\n",
    "        x2_out = self.proj_out(x2, normalize)\n",
    "        if self.use_v2:\n",
    "            x2 = self.layers3c[0](x2.contiguous())\n",
    "        x3 = self.layers3[0](x2.contiguous())\n",
    "        x3_out = self.proj_out(x3, normalize)\n",
    "        if self.use_v2:\n",
    "            x3 = self.layers4c[0](x3.contiguous())\n",
    "        x4 = self.layers4[0](x3.contiguous())\n",
    "        x4_out = self.proj_out(x4, normalize)\n",
    "\n",
    "        # Return outputs from each stage\n",
    "        return [x0_out, x1_out, x2_out, x3_out, x4_out]\n",
    "\n",
    "\n",
    "class SwinUNETR(nn.Module):\n",
    "    \"\"\"\n",
    "    Swin UNETR based on: \"Hatamizadeh et al.,\n",
    "    Swin UNETR: Swin Transformers for Semantic Segmentation of Brain Tumors in MRI Images\n",
    "    <https://arxiv.org/abs/2201.01266>\"\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        depths: Sequence[int] = (2, 2, 2, 2),\n",
    "        num_heads: Sequence[int] = (3, 6, 12, 24),\n",
    "        feature_size: int = 24,\n",
    "        norm_name: tuple | str = \"instance\",\n",
    "        drop_rate: float = 0.0,\n",
    "        attn_drop_rate: float = 0.0,\n",
    "        dropout_path_rate: float = 0.0,\n",
    "        normalize: bool = True,\n",
    "        use_checkpoint: bool = False,\n",
    "        spatial_dims: int = 3,\n",
    "        downsample=\"merging\",\n",
    "        use_v2=False,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_channels: dimension of input channels.\n",
    "            out_channels: dimension of output channels.\n",
    "            feature_size: dimension of network feature size.\n",
    "            depths: number of layers in each stage.\n",
    "            num_heads: number of attention heads.\n",
    "            norm_name: feature normalization type and arguments.\n",
    "            drop_rate: dropout rate.\n",
    "            attn_drop_rate: attention dropout rate.\n",
    "            dropout_path_rate: drop path rate.\n",
    "            normalize: normalize output intermediate features in each stage.\n",
    "            use_checkpoint: use gradient checkpointing for reduced memory usage.\n",
    "            spatial_dims: number of spatial dimensions (e.g. 3 for 3D data).\n",
    "            downsample: module used for downsampling, available options are `\"mergingv2\"`, `\"merging\"`.\n",
    "            use_v2: using swinunetr_v2, which adds a residual convolution block at the beginning of each swin stage.\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # Ensure sizes match spatial dimensions\n",
    "        patch_size = ensure_tuple_rep(2, spatial_dims)\n",
    "        window_size = ensure_tuple_rep(7, spatial_dims)\n",
    "\n",
    "        # Ensure valid spatial dimensions\n",
    "        if not (spatial_dims == 3):\n",
    "            raise ValueError(\"Spatial dimension should be 3.\")\n",
    "\n",
    "        # Validate rates between 0 and 1\n",
    "        if not (0 <= drop_rate <= 1):\n",
    "            raise ValueError(\"Dropout rate should be between 0 and 1.\")\n",
    "        if not (0 <= attn_drop_rate <= 1):\n",
    "            raise ValueError(\"Attention dropout rate should be between 0 and 1.\")\n",
    "        if not (0 <= dropout_path_rate <= 1):\n",
    "            raise ValueError(\"Drop path rate should be between 0 and 1.\")\n",
    "\n",
    "        # Ensure feature size is divisible by 12 for the multi-head attention mechanism\n",
    "        if feature_size % 12 != 0:\n",
    "            raise ValueError(\"Feature size should be divisible by 12.\")\n",
    "\n",
    "        self.normalize = normalize\n",
    "\n",
    "        # Define the Swin Transformer-based encoder (`SwinTransformer`) to be used in this UNETR\n",
    "        self.swinViT = SwinTransformer(\n",
    "            in_chans=in_channels,\n",
    "            embed_dim=feature_size,\n",
    "            window_size=window_size,\n",
    "            patch_size=patch_size,\n",
    "            depths=depths,\n",
    "            num_heads=num_heads,\n",
    "            mlp_ratio=4.0,\n",
    "            qkv_bias=True,\n",
    "            drop_rate=drop_rate,\n",
    "            attn_drop_rate=attn_drop_rate,\n",
    "            drop_path_rate=dropout_path_rate,\n",
    "            norm_layer=nn.LayerNorm,\n",
    "            use_checkpoint=use_checkpoint,\n",
    "            spatial_dims=spatial_dims,\n",
    "            downsample=look_up_option(downsample, MERGING_MODE) if isinstance(downsample, str) else downsample,\n",
    "            use_v2=use_v2,\n",
    "        )\n",
    "\n",
    "        # Encoder stages - Use `UnetrBasicBlock` for encoding input features\n",
    "        # These layers transform the input tensor into feature maps at different resolutions\n",
    "        self.encoder1 = UnetrBasicBlock(\n",
    "            spatial_dims=spatial_dims,\n",
    "            in_channels=in_channels,\n",
    "            out_channels=feature_size,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            norm_name=norm_name,\n",
    "            res_block=True,\n",
    "        )\n",
    "\n",
    "        self.encoder2 = UnetrBasicBlock(\n",
    "            spatial_dims=spatial_dims,\n",
    "            in_channels=feature_size,\n",
    "            out_channels=feature_size,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            norm_name=norm_name,\n",
    "            res_block=True,\n",
    "        )\n",
    "\n",
    "        self.encoder3 = UnetrBasicBlock(\n",
    "            spatial_dims=spatial_dims,\n",
    "            in_channels=2 * feature_size,\n",
    "            out_channels=2 * feature_size,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            norm_name=norm_name,\n",
    "            res_block=True,\n",
    "        )\n",
    "\n",
    "        self.encoder4 = UnetrBasicBlock(\n",
    "            spatial_dims=spatial_dims,\n",
    "            in_channels=4 * feature_size,\n",
    "            out_channels=4 * feature_size,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            norm_name=norm_name,\n",
    "            res_block=True,\n",
    "        )\n",
    "\n",
    "        self.encoder10 = UnetrBasicBlock(\n",
    "            spatial_dims=spatial_dims,\n",
    "            in_channels=16 * feature_size,\n",
    "            out_channels=16 * feature_size,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            norm_name=norm_name,\n",
    "            res_block=True,\n",
    "        )\n",
    "\n",
    "        # Decoder stages - Use `UnetrUpBlock` for decoding feature maps to higher resolutions\n",
    "        # These layers upsample the feature maps to reconstruct the original spatial resolution\n",
    "        self.decoder5 = UnetrUpBlock(\n",
    "            spatial_dims=spatial_dims,\n",
    "            in_channels=16 * feature_size,\n",
    "            out_channels=8 * feature_size,\n",
    "            kernel_size=3,\n",
    "            upsample_kernel_size=2,\n",
    "            norm_name=norm_name,\n",
    "            res_block=True,\n",
    "        )\n",
    "\n",
    "        self.decoder4 = UnetrUpBlock(\n",
    "            spatial_dims=spatial_dims,\n",
    "            in_channels=feature_size * 8,\n",
    "            out_channels=feature_size * 4,\n",
    "            kernel_size=3,\n",
    "            upsample_kernel_size=2,\n",
    "            norm_name=norm_name,\n",
    "            res_block=True,\n",
    "        )\n",
    "\n",
    "        self.decoder3 = UnetrUpBlock(\n",
    "            spatial_dims=spatial_dims,\n",
    "            in_channels=feature_size * 4,\n",
    "            out_channels=feature_size * 2,\n",
    "            kernel_size=3,\n",
    "            upsample_kernel_size=2,\n",
    "            norm_name=norm_name,\n",
    "            res_block=True,\n",
    "        )\n",
    "\n",
    "        self.decoder2 = UnetrUpBlock(\n",
    "            spatial_dims=spatial_dims,\n",
    "            in_channels=feature_size * 2,\n",
    "            out_channels=feature_size,\n",
    "            kernel_size=3,\n",
    "            upsample_kernel_size=2,\n",
    "            norm_name=norm_name,\n",
    "            res_block=True,\n",
    "        )\n",
    "\n",
    "        self.decoder1 = UnetrUpBlock(\n",
    "            spatial_dims=spatial_dims,\n",
    "            in_channels=feature_size,\n",
    "            out_channels=feature_size,\n",
    "            kernel_size=3,\n",
    "            upsample_kernel_size=2,\n",
    "            norm_name=norm_name,\n",
    "            res_block=True,\n",
    "        )\n",
    "\n",
    "        # Output block that takes the final feature map and converts it to desired output channels\n",
    "        self.out = UnetOutBlock(spatial_dims=spatial_dims, in_channels=feature_size, out_channels=out_channels)\n",
    "\n",
    "    def load_from(self, weights):\n",
    "        # Load weights from a pretrained model\n",
    "        with torch.no_grad():\n",
    "            self.swinViT.patch_embed.proj.weight.copy_(weights[\"state_dict\"][\"module.patch_embed.proj.weight\"])\n",
    "            self.swinViT.patch_embed.proj.bias.copy_(weights[\"state_dict\"][\"module.patch_embed.proj.bias\"])\n",
    "            for bname, block in self.swinViT.layers1[0].blocks.named_children():\n",
    "                block.load_from(weights, n_block=bname, layer=\"layers1\")\n",
    "            # Load weights for downsampling layers and other components\n",
    "            self.swinViT.layers1[0].downsample.reduction.weight.copy_(\n",
    "                weights[\"state_dict\"][\"module.layers1.0.downsample.reduction.weight\"]\n",
    "            )\n",
    "            self.swinViT.layers1[0].downsample.norm.weight.copy_(\n",
    "                weights[\"state_dict\"][\"module.layers1.0.downsample.norm.weight\"]\n",
    "            )\n",
    "            self.swinViT.layers1[0].downsample.norm.bias.copy_(\n",
    "                weights[\"state_dict\"][\"module.layers1.0.downsample.norm.bias\"]\n",
    "            )\n",
    "            for bname, block in self.swinViT.layers2[0].blocks.named_children():\n",
    "                block.load_from(weights, n_block=bname, layer=\"layers2\")\n",
    "            self.swinViT.layers2[0].downsample.reduction.weight.copy_(\n",
    "                weights[\"state_dict\"][\"module.layers2.0.downsample.reduction.weight\"]\n",
    "            )\n",
    "            self.swinViT.layers2[0].downsample.norm.weight.copy_(\n",
    "                weights[\"state_dict\"][\"module.layers2.0.downsample.norm.weight\"]\n",
    "            )\n",
    "            self.swinViT.layers2[0].downsample.norm.bias.copy_(\n",
    "                weights[\"state_dict\"][\"module.layers2.0.downsample.norm.bias\"]\n",
    "            )\n",
    "            # Repeating for layers3 and layers4\n",
    "\n",
    "    def forward(self, x_in):\n",
    "        \"\"\"\n",
    "        Forward pass for the SwinUNETR model.\n",
    "\n",
    "        Args:\n",
    "            x_in: Input tensor, typically with shape (batch, channels, depth, height, width).\n",
    "\n",
    "        Returns:\n",
    "            logits: Output predictions after applying the Swin Transformer and decoding layers.\n",
    "        \"\"\"\n",
    "\n",
    "        # Pass input through the Swin Transformer encoder\n",
    "        hidden_states_out = self.swinViT(x_in, self.normalize)\n",
    "\n",
    "        # Apply encoder blocks to extract features at multiple resolutions\n",
    "        enc0 = self.encoder1(x_in)\n",
    "        enc1 = self.encoder2(hidden_states_out[0])\n",
    "        enc2 = self.encoder3(hidden_states_out[1])\n",
    "        enc3 = self.encoder4(hidden_states_out[2])\n",
    "\n",
    "        # Apply the decoder blocks in a hierarchical manner to reconstruct the image\n",
    "        dec4 = self.encoder10(hidden_states_out[4])\n",
    "        dec3 = self.decoder5(dec4, hidden_states_out[3])\n",
    "        dec2 = self.decoder4(dec3, enc3)\n",
    "        dec1 = self.decoder3(dec2, enc2)\n",
    "        dec0 = self.decoder2(dec1, enc1)\n",
    "\n",
    "        # Generate final output from the last upsampled decoder output\n",
    "        out = self.decoder1(dec0, enc0)\n",
    "        logits = self.out(out)\n",
    "\n",
    "        return logits\n",
    "\n",
    "\n",
    "class SwinUNETR_AIRT(torch.nn.Module):\n",
    "    def __init__(self, input_dimensions, in_channels, out_classes, **kwargs):\n",
    "        super(SwinUNETR_AIRT, self).__init__()\n",
    "\n",
    "        self.ensure_all_dimensions_divisible(input_dimensions)\n",
    "\n",
    "        self.model = SwinUNETR(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_classes,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "        self.conv_layer = nn.Conv3d(input_dimensions[2], 1, kernel_size=1, stride=1, padding=0, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # batch_size, in_channels, height, width, depth\n",
    "        x = x.permute(0, 1, 4, 2, 3)\n",
    "        # batch_size, in_channels, depth, height, width\n",
    "        x = self.model(x)\n",
    "        # batch_size, out_classes, depth, height, width\n",
    "        x = x.permute(0, 2, 1, 3, 4)\n",
    "        # batch_size, depth, out_classes, height, width\n",
    "        x = self.conv_layer(x)\n",
    "        # batch_size, 1, out_classes, height, width\n",
    "        x = x.squeeze(1)\n",
    "        # batch_size, out_classes, height, width\n",
    "        return x\n",
    "\n",
    "    def ensure_all_dimensions_divisible(self, input_dimensions, divisor=32):\n",
    "        \"\"\"\n",
    "        Ensure that all dimensions in input_dimensions are divisible by the given divisor.\n",
    "        If not, raise an exception.\n",
    "\n",
    "        Args:\n",
    "            input_dimensions (tuple): Input dimensions (e.g., (128, 128, 128)).\n",
    "            divisor (int): The number to ensure divisibility by (default is 32).\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If any dimension is not divisible by the divisor.\n",
    "        \"\"\"\n",
    "        for dim in input_dimensions:\n",
    "            if dim % divisor != 0:\n",
    "                raise ValueError(\n",
    "                    f\"Dimension {dim} is not divisible by {divisor}. All dimensions must be divisible.\"\n",
    "                )\n",
    "        print(\"All dimensions are divisible by\", divisor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LR_SCHEDULER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2020 - 2021 MONAI Consortium\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "import math\n",
    "import warnings\n",
    "from typing import List\n",
    "\n",
    "from torch.optim import Adam, Optimizer\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "\n",
    "\n",
    "class LinearWarmupCosineAnnealingLR(_LRScheduler):\n",
    "    def __init__(\n",
    "        self,\n",
    "        optimizer: Optimizer,\n",
    "        warmup_epochs: int,\n",
    "        max_epochs: int,\n",
    "        warmup_start_lr: float = 0.0,\n",
    "        eta_min: float = 0.0,\n",
    "        last_epoch: int = -1,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            optimizer (Optimizer): Wrapped optimizer.\n",
    "            warmup_epochs (int): Maximum number of iterations for linear warmup\n",
    "            max_epochs (int): Maximum number of iterations\n",
    "            warmup_start_lr (float): Learning rate to start the linear warmup. Default: 0.\n",
    "            eta_min (float): Minimum learning rate. Default: 0.\n",
    "            last_epoch (int): The index of last epoch. Default: -1.\n",
    "        \"\"\"\n",
    "        self.warmup_epochs = warmup_epochs\n",
    "        self.max_epochs = max_epochs\n",
    "        self.warmup_start_lr = warmup_start_lr\n",
    "        self.eta_min = eta_min\n",
    "\n",
    "        super(LinearWarmupCosineAnnealingLR, self).__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self) -> List[float]:\n",
    "        \"\"\"\n",
    "        Compute learning rate using chainable form of the scheduler\n",
    "        \"\"\"\n",
    "        if not self._get_lr_called_within_step:\n",
    "            warnings.warn(\n",
    "                \"To get the last learning rate computed by the scheduler, \" \"please use `get_last_lr()`.\", UserWarning\n",
    "            )\n",
    "\n",
    "        if self.last_epoch == 0:\n",
    "            return [self.warmup_start_lr] * len(self.base_lrs)\n",
    "        elif self.last_epoch < self.warmup_epochs:\n",
    "            return [\n",
    "                group[\"lr\"] + (base_lr - self.warmup_start_lr) / (self.warmup_epochs - 1)\n",
    "                for base_lr, group in zip(self.base_lrs, self.optimizer.param_groups)\n",
    "            ]\n",
    "        elif self.last_epoch == self.warmup_epochs:\n",
    "            return self.base_lrs\n",
    "        elif (self.last_epoch - 1 - self.max_epochs) % (2 * (self.max_epochs - self.warmup_epochs)) == 0:\n",
    "            return [\n",
    "                group[\"lr\"]\n",
    "                + (base_lr - self.eta_min) * (1 - math.cos(math.pi / (self.max_epochs - self.warmup_epochs))) / 2\n",
    "                for base_lr, group in zip(self.base_lrs, self.optimizer.param_groups)\n",
    "            ]\n",
    "\n",
    "        return [\n",
    "            (1 + math.cos(math.pi * (self.last_epoch - self.warmup_epochs) / (self.max_epochs - self.warmup_epochs)))\n",
    "            / (\n",
    "                1\n",
    "                + math.cos(\n",
    "                    math.pi * (self.last_epoch - self.warmup_epochs - 1) / (self.max_epochs - self.warmup_epochs)\n",
    "                )\n",
    "            )\n",
    "            * (group[\"lr\"] - self.eta_min)\n",
    "            + self.eta_min\n",
    "            for group in self.optimizer.param_groups\n",
    "        ]\n",
    "\n",
    "    def _get_closed_form_lr(self) -> List[float]:\n",
    "        \"\"\"\n",
    "        Called when epoch is passed as a param to the `step` function of the scheduler.\n",
    "        \"\"\"\n",
    "        if self.last_epoch < self.warmup_epochs:\n",
    "            return [\n",
    "                self.warmup_start_lr + self.last_epoch * (base_lr - self.warmup_start_lr) / (self.warmup_epochs - 1)\n",
    "                for base_lr in self.base_lrs\n",
    "            ]\n",
    "\n",
    "        return [\n",
    "            self.eta_min\n",
    "            + 0.5\n",
    "            * (base_lr - self.eta_min)\n",
    "            * (1 + math.cos(math.pi * (self.last_epoch - self.warmup_epochs) / (self.max_epochs - self.warmup_epochs)))\n",
    "            for base_lr in self.base_lrs\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qhfDmrpq3vTJ"
   },
   "source": [
    "# LIGHTNING MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ut52X3M24DSL",
    "outputId": "34e7fcb0-2e14-4d61-c7b3-823e6ca477a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All imports succeeded!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, random_split, Dataset\n",
    "import pytorch_lightning as pl\n",
    "import torch.nn.functional as F\n",
    "from monai.losses import DiceLoss\n",
    "from monai.metrics import ConfusionMatrixMetric, MeanIoU, DiceMetric\n",
    "import numpy as np\n",
    "from monai.inferers import sliding_window_inference\n",
    "from functools import partial\n",
    "from fractions import Fraction\n",
    "\n",
    "print(\"All imports succeeded!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "0CZsIk23Rc_q"
   },
   "outputs": [],
   "source": [
    "# Define a PyTorch Lightning model wrapper\n",
    "class SwinUNETR_AIRT_LightningModel(pl.LightningModule):\n",
    "    def __init__(self, patch_dimensions, **kwargs):\n",
    "        super(SwinUNETR_AIRT_LightningModel, self).__init__()\n",
    "\n",
    "        ###################### MODEL INSTANTIATION ###########################\n",
    "\n",
    "        self.patch_dimensions = patch_dimensions\n",
    "\n",
    "        self.model = SwinUNETR_AIRT(input_dimensions=self.patch_dimensions, in_channels=1, out_classes=2, **kwargs).to(\"cuda\")\n",
    "\n",
    "        ###################### LOGGING PARAMETERS ###########################\n",
    "\n",
    "        self.enable_batch_logging_into_console = False  # Boolean flag for controlling batch logging into console\n",
    "        self.enable_training_epoch_logging_into_console = True  # Boolean flag for controlling training epoch logging into console\n",
    "        self.enable_validation_epoch_logging_into_console = True  # Boolean flag for controlling validation epoch logging into console\n",
    "\n",
    "        ###################### LOSS & OTHER METRICS ###########################\n",
    "\n",
    "        self.include_background_in_loss_and_metrics = False  # Boolean flag for controlling inclusion of background in loss and metrics\n",
    "\n",
    "        # DiceLoss ask that the model's output must have at least two channels.\n",
    "        # The first channel is assumed to represent the background class,\n",
    "        # while subsequent channels represent different foreground classes.\n",
    "        #\n",
    "        # If include_background=False, the Dice loss computation will exclude the first channel\n",
    "        # (i.e., it will not compute the Dice score for the first class).\n",
    "        # The loss will be computed only for the rest of the channels.\n",
    "        #\n",
    "        # DiceLoss does not apply any activation function by default (this is our case)\n",
    "        # Therefore, we have to apply the activation function (in our case, softmax) before computing the loss\n",
    "        # over the model's output (y_hat)\n",
    "        #\n",
    "        # y_hat (prediction) is expected to be a multiple-channel tensor containing,\n",
    "        # in each channel/class, the probability corresponding to the channel/class for each pixel/voxel (i.e. the probability that pixel belongs to the class represented by that channel)\n",
    "        # (I.e. y_hat shape: (OUTPUT_CHANNELS, HEIGHT, WIDTH) )\n",
    "        # The first channel is assumed to represent the background class, if include_background=True.\n",
    "        #\n",
    "        # y (ground truth) is expected to be a multiple-channel tensor where each channel represents\n",
    "        # a class, and the grid corresponding to that channel has to be one-hot encoded (just 1s and 0s) representing when\n",
    "        # the corrresponding pixel is labeled with that class (1) or not (0).\n",
    "        # (I.e. y shape: (OUTPUT_CHANNELS, HEIGHT, WIDTH) )\n",
    "        # The first channel is assumed to represent the background class, if include_background=True.\n",
    "\n",
    "        self.loss_fn = DiceLoss(include_background=self.include_background_in_loss_and_metrics)\n",
    "\n",
    "        # When using include_background=True in MeanIoU,\n",
    "        # the model's output must have at least two channels.\n",
    "        # The first channel is assumed to represent the background class,\n",
    "        # while subsequent channels represent different foreground classes.\n",
    "        #\n",
    "        # Since reduction=\"mean\", MeanIoU will return the average IoU score across all classes,\n",
    "        # including the background.\n",
    "        #\n",
    "        # y_hat (prediction) is expected to be a multiple-channel tensor. It must be one-hot format and first dim is batch. The values should be binarized.\n",
    "        # (I.e. y_hat shape: (OUTPUT_CHANNELS, HEIGHT, WIDTH) )\n",
    "        # The first channel is assumed to represent the background class, if include_background=True.\n",
    "        #\n",
    "        # y (ground truth) is expected to be a multiple-channel tensor where each channel represents\n",
    "        # a class, and the grid corresponding to that channel has to be one-hot encoded (just 1s and 0s) representing when\n",
    "        # the corrresponding pixel is labeled with that class (1) or not (0).\n",
    "        # (I.e. y shape: (OUTPUT_CHANNELS, HEIGHT, WIDTH) )\n",
    "        # The first channel is assumed to represent the background class, if include_background=True.\n",
    "\n",
    "        self.mean_iou_metric = MeanIoU(include_background=self.include_background_in_loss_and_metrics, reduction=\"none\")\n",
    "\n",
    "        # When using include_background=True in DiceMetric,\n",
    "        # the model's output must have at least two channels.\n",
    "        # The first channel is assumed to represent the background class,\n",
    "        # while subsequent channels represent different foreground classes.\n",
    "        #\n",
    "        # Since reduction=\"mean\", DiceMetric will return the average Dice coefficent across all classes,\n",
    "        # including the background (since include_background=True).\n",
    "        #\n",
    "        # y_hat (prediction) is expected to be a multiple-channel tensor. It must be one-hot format and first dim is batch. The values should be binarized.\n",
    "        # (I.e. y_hat shape: (OUTPUT_CHANNELS, HEIGHT, WIDTH) )\n",
    "        # The first channel is assumed to represent the background class, if include_background=True.\n",
    "        #\n",
    "        # y (ground truth) is expected to be a multiple-channel tensor where each channel represents\n",
    "        # a class, and the grid corresponding to that channel has to be one-hot encoded (just 1s and 0s) representing when\n",
    "        # the corrresponding pixel is labeled with that class (1) or not (0).\n",
    "        # (I.e. y shape: (OUTPUT_CHANNELS, HEIGHT, WIDTH) )\n",
    "        # The first channel is assumed to represent the background class, if include_background=True.\n",
    "\n",
    "        self.dice_metric = DiceMetric(include_background=self.include_background_in_loss_and_metrics, reduction=\"none\")\n",
    "\n",
    "        self.training_batch_losses_in_epoch = []  # Store losses for training batches in training epoch\n",
    "\n",
    "        self.validation_batch_losses_in_epoch = []  # Store losses for validation batches in validation epoch\n",
    "        self.validation_batch_mean_ious_in_epoch = []  # Store mean_ious for validation batches in validation epoch\n",
    "        self.validation_batch_dice_coeffs_in_epoch = []  # Store dice_coeffs for validation batches in validation epoch\n",
    "\n",
    "    def forward(self, x):\n",
    "        pred = self.model(x)\n",
    "        return pred\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # Optional logging for debugging\n",
    "        if self.enable_batch_logging_into_console and self.enable_training_epoch_logging_into_console:\n",
    "            print(f\"Training: Epoch {self.current_epoch + 1}, Batch {batch_idx}\")\n",
    "    \n",
    "        # Unpack the batch\n",
    "        x, y = batch  # x shape: (batch_size, input_channels, height, width, depth), y shape: (batch_size, output_channels, height, width)\n",
    "    \n",
    "        # Forward pass through the model\n",
    "        y_hat = self(x)  # y_hat shape: (batch_size, output_channels, height, width, 1)\n",
    "    \n",
    "        # Apply post-processing (e.g., softmax for probabilities)\n",
    "        y_hat_probabilities = F.softmax(y_hat, dim=1)\n",
    "    \n",
    "        # Compute loss using the provided loss function\n",
    "        loss = self.loss_fn(y_hat_probabilities, y)\n",
    "    \n",
    "        # Log training loss\n",
    "        self.log(\n",
    "            'train_loss', \n",
    "            loss, \n",
    "            on_step=True, \n",
    "            on_epoch=True, \n",
    "            prog_bar=True, \n",
    "            batch_size=self.trainer.train_dataloader.batch_size\n",
    "        )\n",
    "    \n",
    "        # Keep track of batch losses for custom epoch-level logging\n",
    "        self.training_batch_losses_in_epoch.append(loss)\n",
    "    \n",
    "        return loss  # Return loss for Lightning to handle the backward pass\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        \n",
    "        if self.enable_batch_logging_into_console and self.enable_validation_epoch_logging_into_console:\n",
    "            print(f\"Validation: Epoch {self.current_epoch + 1}, Batch {batch_idx}\")\n",
    "\n",
    "        x, y = batch  # x format: list of dict_patches (one per sample in the batch), y format: list of label_paths (one per sample in the batch)\n",
    "\n",
    "        batch_y = []\n",
    "        batch_y_hat_probabilities = []\n",
    "        losses_batch = []  # Use a Python list to collect losses\n",
    "\n",
    "        #############################   COMPUTING PREDICTIONS   #############################################\n",
    "\n",
    "        for sample_x, sample_y in zip(x, y):\n",
    "            y = torch.load(sample_y, weights_only=True).to(self.device)  # Move to correct device\n",
    "            y = y.unsqueeze(0)  # Add 1 dimension to match model output format\n",
    "\n",
    "            # Create a tensor with all values set to NaN\n",
    "            y_hat = torch.empty_like(y).fill_(float('nan')).to(self.device)\n",
    "            patches_dict_info = sample_x\n",
    "\n",
    "            for patch_key in list(patches_dict_info.keys()):\n",
    "                patch_coordinates = patches_dict_info[patch_key][\"patch_coord\"]\n",
    "                patch_path = patches_dict_info[patch_key][\"patch_path\"]\n",
    "                patch_tensor = torch.load(patch_path, weights_only=True).to(self.device)\n",
    "                patch_tensor = patch_tensor.unsqueeze(0)  # Match model input format\n",
    "                patch_prediction = self(patch_tensor)\n",
    "\n",
    "                # Fill y_hat with patch predictions\n",
    "                for i in range(patch_coordinates[0], patch_coordinates[0] + self.patch_dimensions[0]):\n",
    "                    for j in range(patch_coordinates[1], patch_coordinates[1] + self.patch_dimensions[1]):\n",
    "                        is_pixel_without_prediction = torch.isnan(y_hat[:, :, i, j]).any()\n",
    "                        if is_pixel_without_prediction:\n",
    "                            y_hat[:, :, i, j] = patch_prediction[:, :, i - patch_coordinates[0], j - patch_coordinates[1]]\n",
    "\n",
    "            # Apply softmax to probabilities (POST-PROCESSING)\n",
    "            y_hat_probabilities = F.softmax(y_hat, dim=1)\n",
    "\n",
    "            # Compute loss and store it in the list\n",
    "            loss = self.loss_fn(y_hat_probabilities, y)\n",
    "            losses_batch.append(loss.item())\n",
    "\n",
    "            batch_y_hat_probabilities.append(y_hat_probabilities)\n",
    "            batch_y.append(y)\n",
    "\n",
    "        #############################   LOSS   #############################################\n",
    "        \n",
    "        # Calculate average loss for the batch\n",
    "        batch_loss_avg = torch.tensor(losses_batch, device=self.device).mean()\n",
    "        self.log('val_loss', batch_loss_avg, on_step=True, on_epoch=True, prog_bar=True, batch_size=self.trainer.val_dataloaders.batch_size)\n",
    "\n",
    "        if self.enable_batch_logging_into_console and self.enable_validation_epoch_logging_into_console:\n",
    "            print(f\"- Dice Loss: {batch_loss_avg:.6f}\")\n",
    "\n",
    "        self.validation_batch_losses_in_epoch.append(batch_loss_avg)\n",
    "\n",
    "        ############################   METRICS   ############################################\n",
    "\n",
    "        # Concatenate the list of samples along the batch dimension\n",
    "        batch_y_hat_probabilities= torch.cat(batch_y_hat_probabilities, dim=0)\n",
    "        batch_y = torch.cat(batch_y, dim=0)\n",
    "\n",
    "        # Convert predictions to one-hot encoding\n",
    "        batch_y_hat_one_hot = torch.zeros_like(batch_y_hat_probabilities)\n",
    "        max_indices = torch.argmax(batch_y_hat_probabilities, dim=1)\n",
    "        batch_y_hat_one_hot.scatter_(dim=1, index=max_indices.unsqueeze(1), value=1)\n",
    "\n",
    "        # Compute Mean IoU for the current batch\n",
    "        batch_mean_iou = self.mean_iou_metric(y_pred=batch_y_hat_one_hot, y=batch_y)  # Directly call MeanIoU\n",
    "        batch_mean_iou_avg = batch_mean_iou.mean()  # Average IoU across classes for the batch\n",
    "        self.validation_batch_mean_ious_in_epoch.append(batch_mean_iou_avg)\n",
    "        self.log('val_mean_iou', batch_mean_iou_avg, on_step=True, on_epoch=True, prog_bar=True, batch_size=self.trainer.val_dataloaders.batch_size)\n",
    "\n",
    "        # Compute Dice Metric (Dice Coeff) for the current batch\n",
    "        batch_dice = self.dice_metric(y_pred=batch_y_hat_one_hot, y=batch_y)  # Directly call DiceMetric\n",
    "        batch_dice_avg = batch_dice.mean()  # Average Dice across classes for the batch\n",
    "        self.validation_batch_dice_coeffs_in_epoch.append(batch_dice_avg)\n",
    "        self.log('val_dice', batch_dice_avg, on_step=True, on_epoch=True, prog_bar=True, batch_size=self.trainer.val_dataloaders.batch_size)\n",
    "\n",
    "        # Return the average batch loss\n",
    "        return batch_loss_avg\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        ## TO DO\n",
    "        x = 0\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-2)\n",
    "        scheduler = LinearWarmupCosineAnnealingLR(\n",
    "            optimizer, warmup_epochs=np.ceil(0.05*self.trainer.max_epochs), max_epochs=self.trainer.max_epochs\n",
    "        )\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "    # Overriding hooks for demonstration\n",
    "    def on_train_epoch_start(self):\n",
    "\n",
    "        if self.enable_training_epoch_logging_into_console:\n",
    "            # Start of an epoch\n",
    "            print(f\"{'=' * 40}\")\n",
    "            print(f\"Starting training epoch {self.current_epoch + 1}...\")\n",
    "            print(f\"{'=' * 40}\")\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        if self.enable_training_epoch_logging_into_console:\n",
    "            # End of an epoch\n",
    "            print(f\"{'=' * 40}\")\n",
    "            print(f\"Finished training epoch {self.current_epoch + 1}\")\n",
    "            print(f\"{'=' * 40}\")\n",
    "\n",
    "            # Average Dice Loss for the Epoch\n",
    "            average_loss_for_epoch = torch.stack(self.training_batch_losses_in_epoch).mean()\n",
    "            print(f\"==> Average Dice Loss for Training Epoch (Patch Level) {self.current_epoch + 1}: {average_loss_for_epoch:.6f}\")\n",
    "            print()\n",
    "\n",
    "        # Clearing list for the next epoch\n",
    "        self.training_batch_losses_in_epoch.clear()\n",
    "\n",
    "\n",
    "    def on_validation_epoch_start(self):\n",
    "        \n",
    "        if self.enable_validation_epoch_logging_into_console:\n",
    "            # Start of an epoch\n",
    "            print(f\"{'=' * 40}\")\n",
    "            print(f\"Starting validation epoch ...\")\n",
    "            print(f\"{'=' * 40}\")\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        if self.enable_validation_epoch_logging_into_console:\n",
    "            print(f\"{'=' * 40}\")\n",
    "            print(f\"Finished validation epoch\")\n",
    "            print(f\"{'=' * 40}\")\n",
    "\n",
    "        avg_loss = torch.stack(self.validation_batch_losses_in_epoch).mean()\n",
    "        print(f\"==> Average Dice Loss (include_background=[{self.include_background_in_loss_and_metrics}]) for Validation Epoch {self.current_epoch + 1}: {avg_loss:.6f}\")\n",
    "        print()\n",
    "        self.validation_batch_losses_in_epoch.clear()\n",
    "\n",
    "        average_mean_iou_epoch = torch.stack(self.validation_batch_mean_ious_in_epoch).mean()\n",
    "        print(f\"==> Average Mean IoU (Mean between all classes - include_background=[{self.include_background_in_loss_and_metrics}]) for Validation Epoch {self.current_epoch + 1}: {average_mean_iou_epoch:.6f}\")\n",
    "        print()\n",
    "        self.validation_batch_mean_ious_in_epoch.clear()\n",
    "\n",
    "\n",
    "        average_dice_coeff_epoch = torch.stack(self.validation_batch_dice_coeffs_in_epoch).mean()\n",
    "        print(f\"==> Average Dice Coefficient (Mean between all classes - include_background=[{self.include_background_in_loss_and_metrics}]) for Validation Epoch {self.current_epoch + 1}: {average_dice_coeff_epoch:.6f}\")\n",
    "        self.validation_batch_dice_coeffs_in_epoch.clear()\n",
    "\n",
    "        print(f\"{'=' * 40}\")\n",
    "\n",
    "\n",
    "    def on_test_epoch_start(self):\n",
    "\n",
    "        # Start of an epoch\n",
    "        print(f\"{'=' * 40}\")\n",
    "        print(f\"Starting testing epoch...\")\n",
    "        print(f\"{'=' * 40}\")\n",
    "\n",
    "        ## STILL TO DO ##\n",
    "        ## STILL TO DO ##\n",
    "        ## STILL TO DO ##\n",
    "\n",
    "\n",
    "    def on_test_epoch_end(self):\n",
    "        # End of an epoch\n",
    "        print(f\"{'=' * 40}\")\n",
    "        print(f\"Finished testing epoch\")\n",
    "        print(f\"{'=' * 40}\")\n",
    "\n",
    "        ## STILL TO DO ##\n",
    "        ## STILL TO DO ##\n",
    "        ## STILL TO DO ##\n",
    "\n",
    "    # Function to compute the smallest valid overlap value\n",
    "    def calculate_overlap(self, model_input_dims, model_output_dims, image_spatial_dim):\n",
    "        overlap_values = []\n",
    "        for in_dim_model, out_dim_model, dim_image in zip(model_input_dims,model_output_dims,image_spatial_dim):\n",
    "            smallest_overlap = None\n",
    "            zoom_scale = out_dim_model / in_dim_model\n",
    "            for stride in range(1, in_dim_model + 1):  # Stride must be <= model_input_spatial_dim\n",
    "                if (dim_image - in_dim_model) % stride == 0:\n",
    "                    overlap = 1 - (stride / in_dim_model)\n",
    "                    if 0 <= overlap < 1 and (overlap*in_dim_model*zoom_scale).is_integer():\n",
    "                        if smallest_overlap is None or overlap < smallest_overlap:\n",
    "                            smallest_overlap = overlap\n",
    "            if smallest_overlap is None:\n",
    "                raise ValueError(f\"No possible overlap value for dimension with model_input_spatial_dim={in_dim_model} \"\n",
    "                                f\"and image_spatial_dim={dim_image}.\")\n",
    "            overlap_values.append(smallest_overlap)\n",
    "        return overlap_values\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "UKS8T-C6tvXs",
    "outputId": "348339e8-259a-4cd4-acae-2d719f0c5568"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The directory '/home/adrian/AIRT_Segmentation_Project/experimenting/data/train_data/preprocessed_files' exists.\n",
      "The json file '/home/adrian/AIRT_Segmentation_Project/experimenting/data/train_data/preprocessed_files/preprocessed_info.json' exists.\n",
      "The label and measurement corresponding to 'R_003' sample have already been processed at least once.\n",
      "The measurement corresponding to 'R_003' sample have already been processed with depth [64] at least once.\n",
      "The label corresponding to 'R_003' sample have already been processed with patch size [128x128] at least once.\n",
      "The measurement corresponding to 'R_003' sample have already been processed with depth [64] and patch size [128x128] at least once.\n",
      "The label corresponding to 'R_003' sample have already been processed with patch size [128x128] and overlap [0_10x0_15]\n",
      "The measurement corresponding to 'R_003' sample have already been processed with depth [64], patch size [128x128] and overlap [0_10x0_15]\n",
      "The label and measurement corresponding to 'Z_003' sample have already been processed at least once.\n",
      "The measurement corresponding to 'Z_003' sample have already been processed with depth [64] at least once.\n",
      "The label corresponding to 'Z_003' sample have already been processed with patch size [128x128] at least once.\n",
      "The measurement corresponding to 'Z_003' sample have already been processed with depth [64] and patch size [128x128] at least once.\n",
      "The label corresponding to 'Z_003' sample have already been processed with patch size [128x128] and overlap [0_10x0_15]\n",
      "The measurement corresponding to 'Z_003' sample have already been processed with depth [64], patch size [128x128] and overlap [0_10x0_15]\n",
      "The label and measurement corresponding to 'R_004' sample have already been processed at least once.\n",
      "The measurement corresponding to 'R_004' sample have already been processed with depth [64] at least once.\n",
      "The label corresponding to 'R_004' sample have already been processed with patch size [128x128] at least once.\n",
      "The measurement corresponding to 'R_004' sample have already been processed with depth [64] and patch size [128x128] at least once.\n",
      "The label corresponding to 'R_004' sample have already been processed with patch size [128x128] and overlap [0_10x0_15]\n",
      "The measurement corresponding to 'R_004' sample have already been processed with depth [64], patch size [128x128] and overlap [0_10x0_15]\n",
      "The label and measurement corresponding to 'Z_004' sample have already been processed at least once.\n",
      "The measurement corresponding to 'Z_004' sample have already been processed with depth [64] at least once.\n",
      "The label corresponding to 'Z_004' sample have already been processed with patch size [128x128] at least once.\n",
      "The measurement corresponding to 'Z_004' sample have already been processed with depth [64] and patch size [128x128] at least once.\n",
      "The label corresponding to 'Z_004' sample have already been processed with patch size [128x128] and overlap [0_10x0_15]\n",
      "The measurement corresponding to 'Z_004' sample have already been processed with depth [64], patch size [128x128] and overlap [0_10x0_15]\n",
      "The JSON file '/home/adrian/AIRT_Segmentation_Project/experimenting/data/train_data/preprocessed_files/preprocessed_info.json' has been updated.\n",
      "The directory '/home/adrian/AIRT_Segmentation_Project/experimenting/data/train_data/preprocessed_files' exists.\n",
      "The json file '/home/adrian/AIRT_Segmentation_Project/experimenting/data/train_data/preprocessed_files/preprocessed_info.json' exists.\n",
      "The label and measurement corresponding to 'R_002' sample have already been processed at least once.\n",
      "The measurement corresponding to 'R_002' sample have already been processed with depth [64] at least once.\n",
      "The label corresponding to 'R_002' sample have already been processed with patch size [128x128] at least once.\n",
      "The measurement corresponding to 'R_002' sample have already been processed with depth [64] and patch size [128x128] at least once.\n",
      "The label corresponding to 'R_002' sample have already been processed with patch size [128x128] and overlap [0_00x0_00]\n",
      "The measurement corresponding to 'R_002' sample have already been processed with depth [64], patch size [128x128] and overlap [0_00x0_00]\n",
      "The label and measurement corresponding to 'Z_002' sample have already been processed at least once.\n",
      "The measurement corresponding to 'Z_002' sample have already been processed with depth [64] at least once.\n",
      "The label corresponding to 'Z_002' sample have already been processed with patch size [128x128] at least once.\n",
      "The measurement corresponding to 'Z_002' sample have already been processed with depth [64] and patch size [128x128] at least once.\n",
      "The label corresponding to 'Z_002' sample have already been processed with patch size [128x128] and overlap [0_00x0_00]\n",
      "The measurement corresponding to 'Z_002' sample have already been processed with depth [64], patch size [128x128] and overlap [0_00x0_00]\n",
      "The JSON file '/home/adrian/AIRT_Segmentation_Project/experimenting/data/train_data/preprocessed_files/preprocessed_info.json' has been updated.\n"
     ]
    }
   ],
   "source": [
    "metadata_dict_with_files_selected_training = {\n",
    "    \"R_003\": {\n",
    "        \"3D_thermal_sequence_filename\": \"R_003.mat\",\n",
    "        \"label_filename\": \"R_003_binary.png\",\n",
    "        \"stratified_group\": \"A\",\n",
    "        \"ROI\": {\n",
    "          \"all_points_x\": [\n",
    "            75,\n",
    "            80,\n",
    "            250,\n",
    "            255,\n",
    "            75\n",
    "          ],\n",
    "          \"all_points_y\": [\n",
    "            32,\n",
    "            215,\n",
    "            214,\n",
    "            31,\n",
    "            32\n",
    "          ]\n",
    "        }\n",
    "    },\n",
    "    \"Z_003\": {\n",
    "        \"3D_thermal_sequence_filename\": \"Z_003.mat\",\n",
    "        \"label_filename\": \"Z_003_binary.png\",\n",
    "        \"stratified_group\": \"B\",\n",
    "        \"ROI\": {\n",
    "          \"all_points_x\": [\n",
    "            79,\n",
    "            84,\n",
    "            255,\n",
    "            259,\n",
    "            79\n",
    "          ],\n",
    "          \"all_points_y\": [\n",
    "            32,\n",
    "            215,\n",
    "            215,\n",
    "            32,\n",
    "            32\n",
    "          ]\n",
    "        }\n",
    "    },\n",
    "    \"R_004\": {\n",
    "        \"3D_thermal_sequence_filename\": \"R_004.mat\",\n",
    "        \"label_filename\": \"R_004_binary.png\",\n",
    "        \"stratified_group\": \"A\",\n",
    "        \"ROI\": {\n",
    "          \"all_points_x\": [\n",
    "            74,\n",
    "            80,\n",
    "            251,\n",
    "            254,\n",
    "            74\n",
    "          ],\n",
    "          \"all_points_y\": [\n",
    "            32,\n",
    "            215,\n",
    "            215,\n",
    "            32,\n",
    "            32\n",
    "          ]\n",
    "        }\n",
    "    },\n",
    "    \"Z_004\": {\n",
    "        \"3D_thermal_sequence_filename\": \"Z_004.mat\",\n",
    "        \"label_filename\": \"Z_004_binary.png\",\n",
    "        \"stratified_group\": \"B\",\n",
    "        \"ROI\": {\n",
    "          \"all_points_x\": [\n",
    "            78,\n",
    "            84,\n",
    "            255,\n",
    "            259,\n",
    "            78\n",
    "          ],\n",
    "          \"all_points_y\": [\n",
    "            31,\n",
    "            215,\n",
    "            214,\n",
    "            31,\n",
    "            31\n",
    "          ]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "metadata_dict_with_files_selected_validation = {\n",
    "    \"R_002\": {\n",
    "        \"3D_thermal_sequence_filename\": \"R_002.mat\",\n",
    "        \"label_filename\": \"R_002_binary.png\",\n",
    "        \"stratified_group\": \"A\",\n",
    "        \"ROI\": {\n",
    "          \"all_points_x\": [\n",
    "            74,\n",
    "            80,\n",
    "            250,\n",
    "            254,\n",
    "            74\n",
    "          ],\n",
    "          \"all_points_y\": [\n",
    "            32,\n",
    "            214,\n",
    "            214,\n",
    "            32,\n",
    "            32\n",
    "          ]\n",
    "        }\n",
    "    },\n",
    "    \"Z_002\": {\n",
    "       \"3D_thermal_sequence_filename\": \"Z_002.mat\",\n",
    "       \"label_filename\": \"Z_002_binary.png\",\n",
    "       \"stratified_group\": \"B\",\n",
    "       \"ROI\": {\n",
    "         \"all_points_x\": [\n",
    "            77,\n",
    "            83,\n",
    "            255,\n",
    "            258,\n",
    "            77\n",
    "          ],\n",
    "          \"all_points_y\": [\n",
    "            30,\n",
    "            214,\n",
    "            214,\n",
    "            32,\n",
    "            30\n",
    "          ]\n",
    "        }\n",
    "    }\n",
    "\n",
    "}\n",
    "\n",
    "data_dir = \"data/train_data\"\n",
    "patch_dims = (128,128,64)\n",
    "overlap_training = [0.10,0.15] #e.g. [0.25,0.35] -> 0.25 overlap in the height dimension & 0.35 overlap in the width one\n",
    "overlap_inference = [0,0]\n",
    "\n",
    "def custom_collate(batch):\n",
    "    # Collect x (dictionaries) into a list\n",
    "    batch_x = [item[0] for item in batch]\n",
    "    # Collect y (strings) into a list\n",
    "    batch_y = [item[1] for item in batch]\n",
    "\n",
    "    return batch_x, batch_y\n",
    "\n",
    "# Instantiate the dataset\n",
    "train_dataset = SwinUnetr3DDataset(\n",
    "    is_inference_mode = False,\n",
    "    metadata_dict_with_files_selected=metadata_dict_with_files_selected_training,\n",
    "    data_dir=data_dir,\n",
    "    model_input_dim=patch_dims,\n",
    "    overlap = overlap_training\n",
    ")\n",
    "\n",
    "val_dataset = SwinUnetr3DDataset(\n",
    "    is_inference_mode = True,\n",
    "    metadata_dict_with_files_selected=metadata_dict_with_files_selected_validation,\n",
    "    data_dir=data_dir,\n",
    "    model_input_dim=patch_dims,\n",
    "    overlap = overlap_inference\n",
    ")\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, collate_fn=custom_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nY8E6ofC3-XQ",
    "outputId": "4d46d3f8-9fbf-4516-f66b-ce21c8c2f2f4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assigned execution_id: 13\n",
      "Using accelerator: <pytorch_lightning.accelerators.cuda.CUDAAccelerator object at 0x7f739f302ef0>\n",
      "Number of devices used: 1\n"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "import os\n",
    "\n",
    "# Path to the parent directory\n",
    "parent_dir = \"executions/train_val\"\n",
    "\n",
    "# List all directories in the parent directory\n",
    "directories = [d for d in os.listdir(parent_dir) if os.path.isdir(os.path.join(parent_dir, d))]\n",
    "\n",
    "# Assign identifier\n",
    "if not directories:\n",
    "    execution_id = 0\n",
    "else:\n",
    "    execution_id = len(directories)\n",
    "\n",
    "print(f\"Assigned execution_id: {execution_id}\")\n",
    "\n",
    "formatted_overlap_training_dim_0 = f\"{overlap_training[0]:.2f}\"\n",
    "formatted_overlap_training_dim_1 = f\"{overlap_training[1]:.2f}\"\n",
    "\n",
    "formatted_overlap_inference_dim_0 = f\"{overlap_inference[0]:.2f}\"\n",
    "formatted_overlap_inference_dim_1 = f\"{overlap_inference[1]:.2f}\"\n",
    "\n",
    "formatted_patch_dims = f\"{patch_dims[0]}x{patch_dims[1]}x{patch_dims[2]}\" #e.g. 64x64\n",
    "formatted_overlap_training = f\"{formatted_overlap_training_dim_0.replace('.', '_')}x{formatted_overlap_training_dim_1.replace('.', '_')}\" #e.g. 0_25x0_35\n",
    "formatted_overlap_inference = f\"{formatted_overlap_inference_dim_0.replace('.', '_')}x{formatted_overlap_inference_dim_1.replace('.', '_')}\" #e.g. 0_25x0_35\n",
    "\n",
    "experiment_config_id = f\"id={execution_id}-patch_dims=[{formatted_patch_dims}]-overlap_training=[{formatted_overlap_training}]-overlap_inference=[{formatted_overlap_inference}]\"\n",
    " \n",
    "# Model checkpoint callback\n",
    "execution_dir = os.path.join(parent_dir, experiment_config_id)\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='val_loss',\n",
    "    dirpath=execution_dir,\n",
    "    filename=\"best-checkpoint-{epoch:02d}-{val_loss:.2f}\",\n",
    "    save_top_k=1,\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "# Early stopping callback\n",
    "early_stopping_callback = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "# Define logger\n",
    "logger = CSVLogger(\n",
    "    save_dir=execution_dir,\n",
    "    name=\"\",\n",
    "    version=\"\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    max_epochs=200,\n",
    "    callbacks=[checkpoint_callback, early_stopping_callback],\n",
    "    log_every_n_steps=1, # log every n batches\n",
    "    logger=logger\n",
    ")\n",
    "\n",
    "print(f\"Using accelerator: {trainer.accelerator}\")\n",
    "print(f\"Number of devices used: {trainer.num_devices}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING & PLOTTING RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "e99501e206864fa6834e3285c7ee0b40",
      "e7c3dca7ea534bd19894c0a7d350189c",
      "c3f90ce9999f478fb032d933f0910376",
      "df390551f9ba405e98c7afb5b337da0c",
      "aca1da82bdfb4ed39c2a5e19439b9cf4",
      "6dc74a03a32243faaedc32ce99ae1186",
      "14e7eba41ad54db3b9e81442a0f3fcb5",
      "147a9512d7e14fc1bb5df198c8645072",
      "04a768248f854759a430185e0ecc760f",
      "25c749615a744af69d020c21077b063f",
      "156907f41ad54247b0faeebaf54db22d",
      "6eb5de561af54373821567026e5cdf67",
      "238467bc46154011b1ae98d030ac9fd4",
      "09b1ad60c74f46a2b6bb7a55cc9076a6",
      "fe0a0addb86649f280e95cd3455bbdd1",
      "c39ca81b222d4bd8b562dbc6b3472d47",
      "aba2b3234543468192b95a151139570f",
      "cc4a23eaf9d3404f8aebc62549a21851",
      "ed210ac45b50410b8406a03920ca70aa",
      "c74a4aff322d4c1baaea6319c68fc013",
      "4c4468dccb4e4857a4f59b352f6e2d65",
      "02996bb2eaac478fb1f8c39104591d9e",
      "7958baf0d57f4a299a532d6d1903848c",
      "49ae2980e69142e394b308d863f4060a",
      "2631c25082444c3baeebddf48da847fc",
      "04d29d904f2e49759fe4590a893604de",
      "e27eca274bb34cc7a065e00530bed0cb",
      "60a46c23903e46c49e75afe50c9a6c04",
      "b0ef657450784488aa2b046bc5ee577e",
      "a883fddfa800437789bc05acafbe921e",
      "c803d63da62f496ab6f8ccf8dfb313c0",
      "c8790884269a4ffdbe155525796bf676",
      "762eb6299af34b079b2b7d1404f9d8ab",
      "3af8446178204d4a8ce6d014e8d0d646",
      "4219267821184a5088c0c88a34d2f4c9",
      "76e033fd323048699969847140de85a4",
      "f3001fa8500d4a22bce7661eb0ae204a",
      "1f891dfa066e4eb08a7730dd770611f6",
      "9eb5072d61774bd18d894ecf77e81e12",
      "c2fdd7e266254984bb2e2b54ef9d7f2d",
      "5edabcfcaf1c4ef7a8736160884b5502",
      "9175ad14ddad400b87d1805a79d9f993",
      "fbc2f79c24ce45e29f64221d00481a83",
      "5516845c13664350a6c3ef8e6e90d0a3",
      "48f9dd734e1747d1836b789279d7c289",
      "ae47ae991a6b4371b3ec9aa489d2ffcc",
      "32afa45db0c443c288be348601cace6d",
      "359daf43863c4a33bd4569de5210f46d",
      "a4f0addb2ce84e29bc6db32fceee5968",
      "e5b81fab1daa433da14aeb73714dc66b",
      "623aefc5dd734c92b6ac65635fe8f6a7",
      "0b80357856dd497b977f1c62b4dd54d5",
      "13296b0129ed4d04aa301f2b07662d4a",
      "790968fc3c26487dada3256743598c46",
      "d2e5b4dbfedd4cfd9a7faa638d159a69",
      "eae96f67f37a4ebabb31f19173e3152b",
      "9ae0bd932f4840f2a11132046dfed2ec",
      "9a43948433da47ad8193944880f8581b",
      "0ba419f910fd451e8d20d7ee21ac229a",
      "acee2b3e64e04397b19a5980ee9693f4",
      "cf09714929914e15b8ca92ccd79fa9f9",
      "ba0c9cb9084940d681b93b4ccf9370a7",
      "6b7bd8095d2c4ad18c659bea3867c9f9",
      "e5e8f803089547f6aac87be176b6b586",
      "bba67b7b4b0c409686babdc2cb7455bd",
      "a678d62c00414bb783d8663d5d8b8a9a",
      "6fd65b20690a4571a0f40daeca6553ea",
      "8dd441669acd480aa67478ce06c32a54",
      "f6af8c83bd474f8a8b89143f1fd9062d",
      "1dad2ea4a59a44158e4232c43205d677",
      "70d1f021f6094454841277b5d31e53f3",
      "9b3c8c7bad644443a272536a4dd484ec",
      "73da76a42f87466bb5ad5720acdb594c",
      "6f21451a4fb045b4b1643da6f1a482db",
      "cb4d7afcc95643369cd3095d204080aa",
      "e3e6e22c70de42b1be6d703f07f57ed9",
      "19269604ec5f47a39c76a3442b98f36f",
      "5b229f4741d44a61829647791c4a53cc",
      "efc5ae14e3894efb851b894a6d8bcd77",
      "adc73e09733a4821a9f6699b1832d7db",
      "fcdddf668b284457ae120ad2d26effda",
      "f79040d7c6db49f48963aa24c40a4d8a",
      "6c6265ad504a4dd6b29b5b44e9c8e88e",
      "e5f0aedfb973415a89b8efeae1cea88a",
      "23cafe3cfb504ab78b65feb113802c75",
      "0d1043b563a2421e93da1b9dde15b83e",
      "fa07b61c9040475aaefe4a3a2eb9797a",
      "01a404fef4cb481ba0eb2affe9c9f0a3",
      "afe77aa595c74197b1ffb77c69d5ae44",
      "4e1afd85300c4533a5ac8c929f63a4c6",
      "6584f713501f49cd9e47d028abd8eb49",
      "07e78019d0fe490bacfa494fed3c0639",
      "5f2899d0eae84a26819267d8381ab98b",
      "fc916d29fc52475c953a3e22781ff313",
      "1944dda3e99a4b73bab485d1ebcece46",
      "ea11c07a8df54393b5fedd153ea790ce",
      "57d1b45a22454941ad98dfc2565bd6ba",
      "27b9a12b35814c6896695b933b42d203",
      "57354716ebe845398c024c78dcc23412",
      "2623d6ae12a144c5bbf067210b00ea22",
      "aa3065868cc24baeabfe417f6521c35e",
      "77c4c406303d4574becdd7e61e58094c",
      "8d14f4ab98d14e2fad1274fe5c772c69",
      "6631195574944c0184cbeeb57d09dc62",
      "462e6c8fc880423592439e448a60895c",
      "2c715b96110849c482142eec37bdf4ef",
      "45e456e35e98476cbb0c05b6438f987c",
      "b457bb71f5d34ab9af0bdf88a52b8d4f",
      "afc5b6291f8a43299cf95408d85301a0",
      "3fb12ce622d44f518cddc8cfabcf1877",
      "addfe20913dc4f86a6fd9226e5328f40",
      "7a371b13ada04a2184458c7f5a1b94ef",
      "8e1507e834a34d2f95e4d8d237b9d70b",
      "e4852d0c53db482d91f0b3573e933ef6",
      "7d8588f03d4d4dce82ac7f9dc2408cd6",
      "0fd8a83a43d441e182dd14fd46942786",
      "885d1a21842f4addb048d45a12840cf9",
      "7f983c2eb21744a5b8ef53576a2a988b",
      "7cf0d78d353e4736b9c26555b4fd5c8d",
      "a21af7833aa74b9daef348fa40e2a769",
      "361ba0ee48264d1d932edc7b0f95fd88",
      "9d801c19d3a644e2ac1a6d042ece3a37",
      "6e1a9bdb343f49fb9eb696f75e35551e",
      "269c42edb5e24fada8cbfbbff3d6a766",
      "e0f001637c7e471586388621b5c63bb7",
      "5f8af3e19b414671b202568469256021",
      "a3d67c0e328a40ed9a30df9185cbffbb",
      "16c67939d1b3438d9e4a577ed12e2ba1",
      "4ee3a8a753cb4c28a2f04e14fec40b4c",
      "df570219617f436d8d0432bf23047a5a",
      "3ce6ffb88d8549bbb053ff4146beb070",
      "7bceda8bc2104bf796f28f29e2effcfb"
     ]
    },
    "id": "cvDjmick3moF",
    "outputId": "bfe3c793-541e-498f-a37f-f9d833cb01b7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type           | Params | Mode \n",
      "---------------------------------------------------\n",
      "0 | model   | SwinUNETR_AIRT | 15.7 M | train\n",
      "1 | loss_fn | DiceLoss       | 0      | train\n",
      "---------------------------------------------------\n",
      "15.7 M    Trainable params\n",
      "0         Non-trainable params\n",
      "15.7 M    Total params\n",
      "62.812    Total estimated model params size (MB)\n",
      "273       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All dimensions are divisible by 32\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |                                                                            | 0/? [00:00<?, …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Starting validation epoch ...\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adrian/AIRT_Segmentation_Project/experimenting/.env_experimenting/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Finished validation epoch\n",
      "========================================\n",
      "==> Average Dice Loss for Validation Epoch 1: 0.970917\n",
      "\n",
      "==> Average Mean IoU (Mean between all classes - include_background=[False]) for Validation Epoch 1: 0.008448\n",
      "\n",
      "==> Average Dice Coefficient (Mean between all classes - include_background=[False]) for Validation Epoch 1: 0.016755\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adrian/AIRT_Segmentation_Project/experimenting/.env_experimenting/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff24b8675ae2482aa4912917a8ac7d69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                   | 0/? [00:00<?, …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Starting training epoch 1...\n",
      "========================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                 | 0/? [00:00<?, …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Starting validation epoch ...\n",
      "========================================\n",
      "========================================\n",
      "Finished validation epoch\n",
      "========================================\n",
      "==> Average Dice Loss for Validation Epoch 1: 0.970917\n",
      "\n",
      "==> Average Mean IoU (Mean between all classes - include_background=[False]) for Validation Epoch 1: 0.008448\n",
      "\n",
      "==> Average Dice Coefficient (Mean between all classes - include_background=[False]) for Validation Epoch 1: 0.016755\n",
      "========================================\n",
      "========================================\n",
      "Finished training epoch 1\n",
      "========================================\n",
      "==> Average Dice Loss for Training Epoch (Patch Level) 1: 0.930655\n",
      "\n",
      "========================================\n",
      "Starting training epoch 2...\n",
      "========================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                 | 0/? [00:00<?, …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Starting validation epoch ...\n",
      "========================================\n",
      "========================================\n",
      "Finished validation epoch\n",
      "========================================\n",
      "==> Average Dice Loss for Validation Epoch 2: 0.962368\n",
      "\n",
      "==> Average Mean IoU (Mean between all classes - include_background=[False]) for Validation Epoch 2: 0.017872\n",
      "\n",
      "==> Average Dice Coefficient (Mean between all classes - include_background=[False]) for Validation Epoch 2: 0.035050\n",
      "========================================\n",
      "========================================\n",
      "Finished training epoch 2\n",
      "========================================\n",
      "==> Average Dice Loss for Training Epoch (Patch Level) 2: 0.923115\n",
      "\n",
      "========================================\n",
      "Starting training epoch 3...\n",
      "========================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                 | 0/? [00:00<?, …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Starting validation epoch ...\n",
      "========================================\n",
      "========================================\n",
      "Finished validation epoch\n",
      "========================================\n",
      "==> Average Dice Loss for Validation Epoch 3: 0.951444\n",
      "\n",
      "==> Average Mean IoU (Mean between all classes - include_background=[False]) for Validation Epoch 3: 0.022932\n",
      "\n",
      "==> Average Dice Coefficient (Mean between all classes - include_background=[False]) for Validation Epoch 3: 0.044627\n",
      "========================================\n",
      "========================================\n",
      "Finished training epoch 3\n",
      "========================================\n",
      "==> Average Dice Loss for Training Epoch (Patch Level) 3: 0.906587\n",
      "\n",
      "========================================\n",
      "Starting training epoch 4...\n",
      "========================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                 | 0/? [00:00<?, …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Starting validation epoch ...\n",
      "========================================\n",
      "========================================\n",
      "Finished validation epoch\n",
      "========================================\n",
      "==> Average Dice Loss for Validation Epoch 4: 0.917003\n",
      "\n",
      "==> Average Mean IoU (Mean between all classes - include_background=[False]) for Validation Epoch 4: 0.072812\n",
      "\n",
      "==> Average Dice Coefficient (Mean between all classes - include_background=[False]) for Validation Epoch 4: 0.132094\n",
      "========================================\n",
      "========================================\n",
      "Finished training epoch 4\n",
      "========================================\n",
      "==> Average Dice Loss for Training Epoch (Patch Level) 4: 0.876301\n",
      "\n",
      "========================================\n",
      "Starting training epoch 5...\n",
      "========================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                 | 0/? [00:00<?, …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Starting validation epoch ...\n",
      "========================================\n",
      "========================================\n",
      "Finished validation epoch\n",
      "========================================\n",
      "==> Average Dice Loss for Validation Epoch 5: 0.776817\n",
      "\n",
      "==> Average Mean IoU (Mean between all classes - include_background=[False]) for Validation Epoch 5: 0.271917\n",
      "\n",
      "==> Average Dice Coefficient (Mean between all classes - include_background=[False]) for Validation Epoch 5: 0.426007\n",
      "========================================\n",
      "========================================\n",
      "Finished training epoch 5\n",
      "========================================\n",
      "==> Average Dice Loss for Training Epoch (Patch Level) 5: 0.746669\n",
      "\n",
      "========================================\n",
      "Starting training epoch 6...\n",
      "========================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                 | 0/? [00:00<?, …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Starting validation epoch ...\n",
      "========================================\n",
      "========================================\n",
      "Finished validation epoch\n",
      "========================================\n",
      "==> Average Dice Loss for Validation Epoch 6: 0.503985\n",
      "\n",
      "==> Average Mean IoU (Mean between all classes - include_background=[False]) for Validation Epoch 6: 0.480150\n",
      "\n",
      "==> Average Dice Coefficient (Mean between all classes - include_background=[False]) for Validation Epoch 6: 0.631167\n",
      "========================================\n",
      "========================================\n",
      "Finished training epoch 6\n",
      "========================================\n",
      "==> Average Dice Loss for Training Epoch (Patch Level) 6: 0.452092\n",
      "\n",
      "========================================\n",
      "Starting training epoch 7...\n",
      "========================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                 | 0/? [00:00<?, …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Starting validation epoch ...\n",
      "========================================\n",
      "========================================\n",
      "Finished validation epoch\n",
      "========================================\n",
      "==> Average Dice Loss for Validation Epoch 7: 0.399774\n",
      "\n",
      "==> Average Mean IoU (Mean between all classes - include_background=[False]) for Validation Epoch 7: 0.546342\n",
      "\n",
      "==> Average Dice Coefficient (Mean between all classes - include_background=[False]) for Validation Epoch 7: 0.706622\n",
      "========================================\n",
      "========================================\n",
      "Finished training epoch 7\n",
      "========================================\n",
      "==> Average Dice Loss for Training Epoch (Patch Level) 7: 0.323646\n",
      "\n",
      "========================================\n",
      "Starting training epoch 8...\n",
      "========================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                 | 0/? [00:00<?, …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Starting validation epoch ...\n",
      "========================================\n",
      "========================================\n",
      "Finished validation epoch\n",
      "========================================\n",
      "==> Average Dice Loss for Validation Epoch 8: 0.321407\n",
      "\n",
      "==> Average Mean IoU (Mean between all classes - include_background=[False]) for Validation Epoch 8: 0.575267\n",
      "\n",
      "==> Average Dice Coefficient (Mean between all classes - include_background=[False]) for Validation Epoch 8: 0.729604\n",
      "========================================\n",
      "========================================\n",
      "Finished training epoch 8\n",
      "========================================\n",
      "==> Average Dice Loss for Training Epoch (Patch Level) 8: 0.249702\n",
      "\n",
      "========================================\n",
      "Starting training epoch 9...\n",
      "========================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                 | 0/? [00:00<?, …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Starting validation epoch ...\n",
      "========================================\n",
      "========================================\n",
      "Finished validation epoch\n",
      "========================================\n",
      "==> Average Dice Loss for Validation Epoch 9: 0.442130\n",
      "\n",
      "==> Average Mean IoU (Mean between all classes - include_background=[False]) for Validation Epoch 9: 0.440922\n",
      "\n",
      "==> Average Dice Coefficient (Mean between all classes - include_background=[False]) for Validation Epoch 9: 0.608968\n",
      "========================================\n",
      "========================================\n",
      "Finished training epoch 9\n",
      "========================================\n",
      "==> Average Dice Loss for Training Epoch (Patch Level) 9: 0.248425\n",
      "\n",
      "========================================\n",
      "Starting training epoch 10...\n",
      "========================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                 | 0/? [00:00<?, …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Starting validation epoch ...\n",
      "========================================\n",
      "========================================\n",
      "Finished validation epoch\n",
      "========================================\n",
      "==> Average Dice Loss for Validation Epoch 10: 0.339654\n",
      "\n",
      "==> Average Mean IoU (Mean between all classes - include_background=[False]) for Validation Epoch 10: 0.562318\n",
      "\n",
      "==> Average Dice Coefficient (Mean between all classes - include_background=[False]) for Validation Epoch 10: 0.719848\n",
      "========================================\n",
      "========================================\n",
      "Finished training epoch 10\n",
      "========================================\n",
      "==> Average Dice Loss for Training Epoch (Patch Level) 10: 0.199591\n",
      "\n",
      "========================================\n",
      "Starting training epoch 11...\n",
      "========================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                 | 0/? [00:00<?, …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Starting validation epoch ...\n",
      "========================================\n",
      "========================================\n",
      "Finished validation epoch\n",
      "========================================\n",
      "==> Average Dice Loss for Validation Epoch 11: 0.248566\n",
      "\n",
      "==> Average Mean IoU (Mean between all classes - include_background=[False]) for Validation Epoch 11: 0.647863\n",
      "\n",
      "==> Average Dice Coefficient (Mean between all classes - include_background=[False]) for Validation Epoch 11: 0.785997\n",
      "========================================\n",
      "========================================\n",
      "Finished training epoch 11\n",
      "========================================\n",
      "==> Average Dice Loss for Training Epoch (Patch Level) 11: 0.161946\n",
      "\n",
      "========================================\n",
      "Starting training epoch 12...\n",
      "========================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                 | 0/? [00:00<?, …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Starting validation epoch ...\n",
      "========================================\n",
      "========================================\n",
      "Finished validation epoch\n",
      "========================================\n",
      "==> Average Dice Loss for Validation Epoch 12: 0.302322\n",
      "\n",
      "==> Average Mean IoU (Mean between all classes - include_background=[False]) for Validation Epoch 12: 0.558291\n",
      "\n",
      "==> Average Dice Coefficient (Mean between all classes - include_background=[False]) for Validation Epoch 12: 0.708964\n",
      "========================================\n",
      "========================================\n",
      "Finished training epoch 12\n",
      "========================================\n",
      "==> Average Dice Loss for Training Epoch (Patch Level) 12: 0.153445\n",
      "\n",
      "========================================\n",
      "Starting training epoch 13...\n",
      "========================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                 | 0/? [00:00<?, …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Starting validation epoch ...\n",
      "========================================\n",
      "========================================\n",
      "Finished validation epoch\n",
      "========================================\n",
      "==> Average Dice Loss for Validation Epoch 13: 0.225127\n",
      "\n",
      "==> Average Mean IoU (Mean between all classes - include_background=[False]) for Validation Epoch 13: 0.673504\n",
      "\n",
      "==> Average Dice Coefficient (Mean between all classes - include_background=[False]) for Validation Epoch 13: 0.804689\n",
      "========================================\n",
      "========================================\n",
      "Finished training epoch 13\n",
      "========================================\n",
      "==> Average Dice Loss for Training Epoch (Patch Level) 13: 0.132633\n",
      "\n",
      "========================================\n",
      "Starting training epoch 14...\n",
      "========================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                 | 0/? [00:00<?, …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Starting validation epoch ...\n",
      "========================================\n",
      "========================================\n",
      "Finished validation epoch\n",
      "========================================\n",
      "==> Average Dice Loss for Validation Epoch 14: 0.275142\n",
      "\n",
      "==> Average Mean IoU (Mean between all classes - include_background=[False]) for Validation Epoch 14: 0.590596\n",
      "\n",
      "==> Average Dice Coefficient (Mean between all classes - include_background=[False]) for Validation Epoch 14: 0.742514\n",
      "========================================\n",
      "========================================\n",
      "Finished training epoch 14\n",
      "========================================\n",
      "==> Average Dice Loss for Training Epoch (Patch Level) 14: 0.125259\n",
      "\n",
      "========================================\n",
      "Starting training epoch 15...\n",
      "========================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                 | 0/? [00:00<?, …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Starting validation epoch ...\n",
      "========================================\n",
      "========================================\n",
      "Finished validation epoch\n",
      "========================================\n",
      "==> Average Dice Loss for Validation Epoch 15: 0.221891\n",
      "\n",
      "==> Average Mean IoU (Mean between all classes - include_background=[False]) for Validation Epoch 15: 0.676437\n",
      "\n",
      "==> Average Dice Coefficient (Mean between all classes - include_background=[False]) for Validation Epoch 15: 0.806823\n",
      "========================================\n",
      "========================================\n",
      "Finished training epoch 15\n",
      "========================================\n",
      "==> Average Dice Loss for Training Epoch (Patch Level) 15: 0.122376\n",
      "\n",
      "========================================\n",
      "Starting training epoch 16...\n",
      "========================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                 | 0/? [00:00<?, …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Starting validation epoch ...\n",
      "========================================\n",
      "========================================\n",
      "Finished validation epoch\n",
      "========================================\n",
      "==> Average Dice Loss for Validation Epoch 16: 0.230453\n",
      "\n",
      "==> Average Mean IoU (Mean between all classes - include_background=[False]) for Validation Epoch 16: 0.657169\n",
      "\n",
      "==> Average Dice Coefficient (Mean between all classes - include_background=[False]) for Validation Epoch 16: 0.793039\n",
      "========================================\n",
      "========================================\n",
      "Finished training epoch 16\n",
      "========================================\n",
      "==> Average Dice Loss for Training Epoch (Patch Level) 16: 0.112847\n",
      "\n",
      "========================================\n",
      "Starting training epoch 17...\n",
      "========================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                 | 0/? [00:00<?, …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Starting validation epoch ...\n",
      "========================================\n",
      "========================================\n",
      "Finished validation epoch\n",
      "========================================\n",
      "==> Average Dice Loss for Validation Epoch 17: 0.305323\n",
      "\n",
      "==> Average Mean IoU (Mean between all classes - include_background=[False]) for Validation Epoch 17: 0.569104\n",
      "\n",
      "==> Average Dice Coefficient (Mean between all classes - include_background=[False]) for Validation Epoch 17: 0.725379\n",
      "========================================\n",
      "========================================\n",
      "Finished training epoch 17\n",
      "========================================\n",
      "==> Average Dice Loss for Training Epoch (Patch Level) 17: 0.099498\n",
      "\n",
      "========================================\n",
      "Starting training epoch 18...\n",
      "========================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                 | 0/? [00:00<?, …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Starting validation epoch ...\n",
      "========================================\n",
      "========================================\n",
      "Finished validation epoch\n",
      "========================================\n",
      "==> Average Dice Loss for Validation Epoch 18: 0.253195\n",
      "\n",
      "==> Average Mean IoU (Mean between all classes - include_background=[False]) for Validation Epoch 18: 0.612082\n",
      "\n",
      "==> Average Dice Coefficient (Mean between all classes - include_background=[False]) for Validation Epoch 18: 0.759170\n",
      "========================================\n",
      "========================================\n",
      "Finished training epoch 18\n",
      "========================================\n",
      "==> Average Dice Loss for Training Epoch (Patch Level) 18: 0.104901\n",
      "\n",
      "========================================\n",
      "Starting training epoch 19...\n",
      "========================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                 | 0/? [00:00<?, …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Starting validation epoch ...\n",
      "========================================\n",
      "========================================\n",
      "Finished validation epoch\n",
      "========================================\n",
      "==> Average Dice Loss for Validation Epoch 19: 0.208152\n",
      "\n",
      "==> Average Mean IoU (Mean between all classes - include_background=[False]) for Validation Epoch 19: 0.669567\n",
      "\n",
      "==> Average Dice Coefficient (Mean between all classes - include_background=[False]) for Validation Epoch 19: 0.801928\n",
      "========================================\n",
      "========================================\n",
      "Finished training epoch 19\n",
      "========================================\n",
      "==> Average Dice Loss for Training Epoch (Patch Level) 19: 0.097869\n",
      "\n",
      "========================================\n",
      "Starting training epoch 20...\n",
      "========================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                 | 0/? [00:00<?, …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Starting validation epoch ...\n",
      "========================================\n",
      "========================================\n",
      "Finished validation epoch\n",
      "========================================\n",
      "==> Average Dice Loss for Validation Epoch 20: 0.240430\n",
      "\n",
      "==> Average Mean IoU (Mean between all classes - include_background=[False]) for Validation Epoch 20: 0.626704\n",
      "\n",
      "==> Average Dice Coefficient (Mean between all classes - include_background=[False]) for Validation Epoch 20: 0.770405\n",
      "========================================\n",
      "========================================\n",
      "Finished training epoch 20\n",
      "========================================\n",
      "==> Average Dice Loss for Training Epoch (Patch Level) 20: 0.088231\n",
      "\n",
      "========================================\n",
      "Starting training epoch 21...\n",
      "========================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                 | 0/? [00:00<?, …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Starting validation epoch ...\n",
      "========================================\n",
      "========================================\n",
      "Finished validation epoch\n",
      "========================================\n",
      "==> Average Dice Loss for Validation Epoch 21: 0.211319\n",
      "\n",
      "==> Average Mean IoU (Mean between all classes - include_background=[False]) for Validation Epoch 21: 0.675489\n",
      "\n",
      "==> Average Dice Coefficient (Mean between all classes - include_background=[False]) for Validation Epoch 21: 0.806315\n",
      "========================================\n",
      "========================================\n",
      "Finished training epoch 21\n",
      "========================================\n",
      "==> Average Dice Loss for Training Epoch (Patch Level) 21: 0.084193\n",
      "\n",
      "========================================\n",
      "Starting training epoch 22...\n",
      "========================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "891298a928b04055987ae031b2a28a0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                 | 0/? [00:00<?, …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Starting validation epoch ...\n",
      "========================================\n",
      "========================================\n",
      "Finished validation epoch\n",
      "========================================\n",
      "==> Average Dice Loss for Validation Epoch 22: 0.220083\n",
      "\n",
      "==> Average Mean IoU (Mean between all classes - include_background=[False]) for Validation Epoch 22: 0.643997\n",
      "\n",
      "==> Average Dice Coefficient (Mean between all classes - include_background=[False]) for Validation Epoch 22: 0.783363\n",
      "========================================\n",
      "========================================\n",
      "Finished training epoch 22\n",
      "========================================\n",
      "==> Average Dice Loss for Training Epoch (Patch Level) 22: 0.083480\n",
      "\n",
      "========================================\n",
      "Starting training epoch 23...\n",
      "========================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac016c9eb141407cb51b82298581f9fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                 | 0/? [00:00<?, …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Starting validation epoch ...\n",
      "========================================\n",
      "========================================\n",
      "Finished validation epoch\n",
      "========================================\n",
      "==> Average Dice Loss for Validation Epoch 23: 0.196791\n",
      "\n",
      "==> Average Mean IoU (Mean between all classes - include_background=[False]) for Validation Epoch 23: 0.695430\n",
      "\n",
      "==> Average Dice Coefficient (Mean between all classes - include_background=[False]) for Validation Epoch 23: 0.820183\n",
      "========================================\n",
      "========================================\n",
      "Finished training epoch 23\n",
      "========================================\n",
      "==> Average Dice Loss for Training Epoch (Patch Level) 23: 0.068143\n",
      "\n",
      "========================================\n",
      "Starting training epoch 24...\n",
      "========================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78b00d164e5744cc98521e831fdeebfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                 | 0/? [00:00<?, …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Starting validation epoch ...\n",
      "========================================\n",
      "========================================\n",
      "Finished validation epoch\n",
      "========================================\n",
      "==> Average Dice Loss for Validation Epoch 24: 0.231606\n",
      "\n",
      "==> Average Mean IoU (Mean between all classes - include_background=[False]) for Validation Epoch 24: 0.634839\n",
      "\n",
      "==> Average Dice Coefficient (Mean between all classes - include_background=[False]) for Validation Epoch 24: 0.776618\n",
      "========================================\n",
      "========================================\n",
      "Finished training epoch 24\n",
      "========================================\n",
      "==> Average Dice Loss for Training Epoch (Patch Level) 24: 0.061307\n",
      "\n",
      "========================================\n",
      "Starting training epoch 25...\n",
      "========================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17263c9be6a3424cb64af0a82909414c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                 | 0/? [00:00<?, …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Starting validation epoch ...\n",
      "========================================\n",
      "========================================\n",
      "Finished validation epoch\n",
      "========================================\n",
      "==> Average Dice Loss for Validation Epoch 25: 0.249675\n",
      "\n",
      "==> Average Mean IoU (Mean between all classes - include_background=[False]) for Validation Epoch 25: 0.628702\n",
      "\n",
      "==> Average Dice Coefficient (Mean between all classes - include_background=[False]) for Validation Epoch 25: 0.771775\n",
      "========================================\n",
      "========================================\n",
      "Finished training epoch 25\n",
      "========================================\n",
      "==> Average Dice Loss for Training Epoch (Patch Level) 25: 0.075814\n",
      "\n",
      "========================================\n",
      "Starting training epoch 26...\n",
      "========================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60845629611b4b19997d24e507477e3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                 | 0/? [00:00<?, …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Starting validation epoch ...\n",
      "========================================\n",
      "========================================\n",
      "Finished validation epoch\n",
      "========================================\n",
      "==> Average Dice Loss for Validation Epoch 26: 0.224602\n",
      "\n",
      "==> Average Mean IoU (Mean between all classes - include_background=[False]) for Validation Epoch 26: 0.649163\n",
      "\n",
      "==> Average Dice Coefficient (Mean between all classes - include_background=[False]) for Validation Epoch 26: 0.787189\n",
      "========================================\n",
      "========================================\n",
      "Finished training epoch 26\n",
      "========================================\n",
      "==> Average Dice Loss for Training Epoch (Patch Level) 26: 0.062506\n",
      "\n",
      "========================================\n",
      "Starting training epoch 27...\n",
      "========================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9817dc89cd264084b0467df0e2d95edb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                 | 0/? [00:00<?, …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Starting validation epoch ...\n",
      "========================================\n",
      "========================================\n",
      "Finished validation epoch\n",
      "========================================\n",
      "==> Average Dice Loss for Validation Epoch 27: 0.234904\n",
      "\n",
      "==> Average Mean IoU (Mean between all classes - include_background=[False]) for Validation Epoch 27: 0.653971\n",
      "\n",
      "==> Average Dice Coefficient (Mean between all classes - include_background=[False]) for Validation Epoch 27: 0.790726\n",
      "========================================\n",
      "========================================\n",
      "Finished training epoch 27\n",
      "========================================\n",
      "==> Average Dice Loss for Training Epoch (Patch Level) 27: 0.068074\n",
      "\n",
      "========================================\n",
      "Starting training epoch 28...\n",
      "========================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7dd0cca515f497da0442ccd7ec39e12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                 | 0/? [00:00<?, …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Starting validation epoch ...\n",
      "========================================\n",
      "========================================\n",
      "Finished validation epoch\n",
      "========================================\n",
      "==> Average Dice Loss for Validation Epoch 28: 0.193883\n",
      "\n",
      "==> Average Mean IoU (Mean between all classes - include_background=[False]) for Validation Epoch 28: 0.693151\n",
      "\n",
      "==> Average Dice Coefficient (Mean between all classes - include_background=[False]) for Validation Epoch 28: 0.818498\n",
      "========================================\n",
      "========================================\n",
      "Finished training epoch 28\n",
      "========================================\n",
      "==> Average Dice Loss for Training Epoch (Patch Level) 28: 0.067490\n",
      "\n",
      "========================================\n",
      "Starting training epoch 29...\n",
      "========================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a58d7444e2e54d8a9074f0c23ad017d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                 | 0/? [00:00<?, …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Starting validation epoch ...\n",
      "========================================\n",
      "========================================\n",
      "Finished validation epoch\n",
      "========================================\n",
      "==> Average Dice Loss for Validation Epoch 29: 0.215519\n",
      "\n",
      "==> Average Mean IoU (Mean between all classes - include_background=[False]) for Validation Epoch 29: 0.663150\n",
      "\n",
      "==> Average Dice Coefficient (Mean between all classes - include_background=[False]) for Validation Epoch 29: 0.797461\n",
      "========================================\n",
      "========================================\n",
      "Finished training epoch 29\n",
      "========================================\n",
      "==> Average Dice Loss for Training Epoch (Patch Level) 29: 0.056715\n",
      "\n",
      "========================================\n",
      "Starting training epoch 30...\n",
      "========================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7777d0c7cca44fb90868e99cb2a1fbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                 | 0/? [00:00<?, …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Starting validation epoch ...\n",
      "========================================\n",
      "========================================\n",
      "Finished validation epoch\n",
      "========================================\n",
      "==> Average Dice Loss for Validation Epoch 30: 0.203008\n",
      "\n",
      "==> Average Mean IoU (Mean between all classes - include_background=[False]) for Validation Epoch 30: 0.679248\n",
      "\n",
      "==> Average Dice Coefficient (Mean between all classes - include_background=[False]) for Validation Epoch 30: 0.808703\n",
      "========================================\n",
      "========================================\n",
      "Finished training epoch 30\n",
      "========================================\n",
      "==> Average Dice Loss for Training Epoch (Patch Level) 30: 0.049586\n",
      "\n",
      "========================================\n",
      "Starting training epoch 31...\n",
      "========================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aad6db124eda4457b5380489ddb4be47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                 | 0/? [00:00<?, …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Starting validation epoch ...\n",
      "========================================\n",
      "========================================\n",
      "Finished validation epoch\n",
      "========================================\n",
      "==> Average Dice Loss for Validation Epoch 31: 0.225379\n",
      "\n",
      "==> Average Mean IoU (Mean between all classes - include_background=[False]) for Validation Epoch 31: 0.638987\n",
      "\n",
      "==> Average Dice Coefficient (Mean between all classes - include_background=[False]) for Validation Epoch 31: 0.779608\n",
      "========================================\n",
      "========================================\n",
      "Finished training epoch 31\n",
      "========================================\n",
      "==> Average Dice Loss for Training Epoch (Patch Level) 31: 0.042028\n",
      "\n",
      "========================================\n",
      "Starting training epoch 32...\n",
      "========================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d9ff7dc08444fe3966e6e278110d811",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                 | 0/? [00:00<?, …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Starting validation epoch ...\n",
      "========================================\n",
      "========================================\n",
      "Finished validation epoch\n",
      "========================================\n",
      "==> Average Dice Loss for Validation Epoch 32: 0.206133\n",
      "\n",
      "==> Average Mean IoU (Mean between all classes - include_background=[False]) for Validation Epoch 32: 0.670427\n",
      "\n",
      "==> Average Dice Coefficient (Mean between all classes - include_background=[False]) for Validation Epoch 32: 0.802538\n",
      "========================================\n",
      "========================================\n",
      "Finished training epoch 32\n",
      "========================================\n",
      "==> Average Dice Loss for Training Epoch (Patch Level) 32: 0.049008\n",
      "\n",
      "========================================\n",
      "Starting training epoch 33...\n",
      "========================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                 | 0/? [00:00<?, …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Starting validation epoch ...\n",
      "========================================\n",
      "========================================\n",
      "Finished validation epoch\n",
      "========================================\n",
      "==> Average Dice Loss for Validation Epoch 33: 0.201866\n",
      "\n",
      "==> Average Mean IoU (Mean between all classes - include_background=[False]) for Validation Epoch 33: 0.684848\n",
      "\n",
      "==> Average Dice Coefficient (Mean between all classes - include_background=[False]) for Validation Epoch 33: 0.812901\n",
      "========================================\n",
      "========================================\n",
      "Finished training epoch 33\n",
      "========================================\n",
      "==> Average Dice Loss for Training Epoch (Patch Level) 33: 0.043084\n",
      "\n",
      "========================================\n",
      "Starting training epoch 34...\n",
      "========================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                 | 0/? [00:00<?, …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Starting validation epoch ...\n",
      "========================================\n",
      "========================================\n",
      "Finished validation epoch\n",
      "========================================\n",
      "==> Average Dice Loss for Validation Epoch 34: 0.218190\n",
      "\n",
      "==> Average Mean IoU (Mean between all classes - include_background=[False]) for Validation Epoch 34: 0.659888\n",
      "\n",
      "==> Average Dice Coefficient (Mean between all classes - include_background=[False]) for Validation Epoch 34: 0.794663\n",
      "========================================\n",
      "========================================\n",
      "Finished training epoch 34\n",
      "========================================\n",
      "==> Average Dice Loss for Training Epoch (Patch Level) 34: 0.038804\n",
      "\n",
      "========================================\n",
      "Starting training epoch 35...\n",
      "========================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                 | 0/? [00:00<?, …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Starting validation epoch ...\n",
      "========================================\n",
      "========================================\n",
      "Finished validation epoch\n",
      "========================================\n",
      "==> Average Dice Loss for Validation Epoch 35: 0.231548\n",
      "\n",
      "==> Average Mean IoU (Mean between all classes - include_background=[False]) for Validation Epoch 35: 0.628221\n",
      "\n",
      "==> Average Dice Coefficient (Mean between all classes - include_background=[False]) for Validation Epoch 35: 0.771563\n",
      "========================================\n",
      "========================================\n",
      "Finished training epoch 35\n",
      "========================================\n",
      "==> Average Dice Loss for Training Epoch (Patch Level) 35: 0.038728\n",
      "\n",
      "========================================\n",
      "Starting training epoch 36...\n",
      "========================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                 | 0/? [00:00<?, …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Starting validation epoch ...\n",
      "========================================\n",
      "========================================\n",
      "Finished validation epoch\n",
      "========================================\n",
      "==> Average Dice Loss for Validation Epoch 36: 0.208571\n",
      "\n",
      "==> Average Mean IoU (Mean between all classes - include_background=[False]) for Validation Epoch 36: 0.673708\n",
      "\n",
      "==> Average Dice Coefficient (Mean between all classes - include_background=[False]) for Validation Epoch 36: 0.804743\n",
      "========================================\n",
      "========================================\n",
      "Finished training epoch 36\n",
      "========================================\n",
      "==> Average Dice Loss for Training Epoch (Patch Level) 36: 0.036669\n",
      "\n",
      "========================================\n",
      "Starting training epoch 37...\n",
      "========================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                 | 0/? [00:00<?, …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Starting validation epoch ...\n",
      "========================================\n",
      "========================================\n",
      "Finished validation epoch\n",
      "========================================\n",
      "==> Average Dice Loss for Validation Epoch 37: 0.238631\n",
      "\n",
      "==> Average Mean IoU (Mean between all classes - include_background=[False]) for Validation Epoch 37: 0.621206\n",
      "\n",
      "==> Average Dice Coefficient (Mean between all classes - include_background=[False]) for Validation Epoch 37: 0.766231\n",
      "========================================\n",
      "========================================\n",
      "Finished training epoch 37\n",
      "========================================\n",
      "==> Average Dice Loss for Training Epoch (Patch Level) 37: 0.031964\n",
      "\n",
      "========================================\n",
      "Starting training epoch 38...\n",
      "========================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                 | 0/? [00:00<?, …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Starting validation epoch ...\n",
      "========================================\n",
      "========================================\n",
      "Finished validation epoch\n",
      "========================================\n",
      "==> Average Dice Loss for Validation Epoch 38: 0.212981\n",
      "\n",
      "==> Average Mean IoU (Mean between all classes - include_background=[False]) for Validation Epoch 38: 0.655790\n",
      "\n",
      "==> Average Dice Coefficient (Mean between all classes - include_background=[False]) for Validation Epoch 38: 0.792115\n",
      "========================================\n",
      "========================================\n",
      "Finished training epoch 38\n",
      "========================================\n",
      "==> Average Dice Loss for Training Epoch (Patch Level) 38: 0.034557\n",
      "\n",
      "Training completed in 0h 8m 16s\n",
      "Plot saved at: training_validation/id=13-patch_dims=[128x128x64]-overlap_training=[0_10x0_15]-overlap_inference=[0_00x0_00]/loss-evol_id=13-patch_dims=[128x128x64]-overlap_training=[0_10x0_15]-overlap_inference=[0_00x0_00].png\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA2AAAAI4CAYAAAAMOIqKAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAADjgUlEQVR4nOzdd1hT1xsH8G/CCHsICKgIiBs37oUDxb23dY/WVUdtLbV1Vm3rrFqtWidq3bVaJ65frVpHHXVvcSMyZY+c3x8xt4QwAgJhfD/PwwOcnHvvm5PkJm/uue+VCSEEiIiIiIiIKNfJ9R0AERERERFRUcEEjIiIiIiIKI8wASMiIiIiIsojTMCIiIiIiIjyCBMwIiIiIiKiPMIEjIiIiIiIKI8wASMiIiIiIsojTMCIiIiIiIjyCBMwIiIiIiKiPFIgE7ANGzZAJpPhyZMnGfZzc3PD4MGD8ySmwurJkyeQyWRYsGBBrm3j1KlTkMlkOHXqlNQ2ePBguLm55do2c5L6+aj+efv2rb5DIh0tWbKkUDx2Ben1kl0zZsyATCbL1rK6vmfog3ofq/7ZtWuXvkMqciZMmCCNv4WFhb7DybZmzZqhWbNmeo1B/XzesGFDtpaPiorC8OHD4eTkBJlMhgkTJuRofIXN4MGDpedulSpV9B1OkbN3716N/felS5d0XrZAJmB5LSoqCtOnT0ebNm1QrFixDHcua9asgbe3NxwdHaFQKODu7o4hQ4bo9Y1/69atWLJkid62X1QsXrwY/v7+sLS0lNru3r2LiRMnomHDhjAxMUn3Q2BISAjmz5+Ppk2bwsHBATY2Nqhfvz62b9+e5rbu37+PPn36oFSpUjAzM0PFihUxa9YsxMTEZCv27du346OPPkK5cuUgk8nSfRO/ePEixo4dC09PT5ibm6N06dLo1asX7t27l2b/HTt2oH79+rCxsYGdnR28vb1x4MCBbMWodvnyZXTq1AnFihWDmZkZqlSpgqVLl6bbPzw8HMWLF0/zw22bNm3g7++Prl27flBMBMTExGDGjBkaX6SQ7kaOHAl/f3/UrVtXoz0+Ph5TpkxBiRIlYGpqinr16iEgICDL63/16hW+/PJLNG/eHJaWllpfeqV29uxZNG7cGGZmZnBycsKnn36KqKioLG9XLTw8HCNHjoSDgwPMzc3RvHlzXL58Ocvr0XWfCqi+hE354Uj988knn2j0GzBgAPz9/dGkSZPs3DXKQXPnzsWGDRswatQo+Pv7Y8CAAfoOKd+zt7eHv78/vvvuO63bcvJ1vG/fPtSqVQsmJiYoXbo0pk+fjqSkJK1+OfVaT2nt2rWoVKkSTExMUK5cOSxbtixb61Eqlfjhhx/g7u4OExMTVKtWDb/++muafW/fvo02bdrAwsICxYoVw4ABAxAcHKzRp3bt2vD398fIkSOzHowogJKSkkRsbKxQKpUZ9nN1dRWDBg364O09fvxYABClS5cWzZo1EwDE+vXr0+w7atQoMWjQILFgwQKxdu1a8fXXXwtHR0dhb28vXrx48cGxZEf79u2Fq6trtpZV3/f58+fnbFApnDx5UgAQJ0+elNoSEhJEXFxcrm0zJ61fv14AEI8fP07zNrlcLqpUqSJq1KiRbr/9+/cLIyMj0blzZ7FkyRKxfPly0bx5cwFATJs2TaPv06dPhY2NjXB1dRXz5s0Tq1atEoMHDxYARKdOnbJ1H7y9vYWFhYVo3ry5sLW1Fd7e3mn26969u3BychLjxo0Ta9asEbNnzxaOjo7C3NxcXL9+XaPv0qVLBQDRvn17sXLlSrF48WJRvXp1AUDs3r07W3EeOXJEGBsbi3r16olFixaJ1atXiylTpojPP/883WXGjRsnzM3NBQCxc+fONPtMnz5dABDBwcHZikvfBg0alO3XeE4JDg4WAMT06dNzZf2JiYkiNjY2W8vq+p6hD+p9bHrvKX369BGGhoZi8uTJYtWqVaJBgwbC0NBQnD59OkvbUe9ny5UrJxo0aKC1z03pypUrwsTERNSsWVOsXLlSTJ06VSgUCtGmTZss3juV5ORk0bBhQ2Fubi5mzJghli9fLipXriwsLS3FvXv3srQuXfepQqg+A9SoUUP4+/tr/Jw/fz7N/oMGDRLm5uZZvXv5hre3d7r77ryiVCpFbGysSEpKytby9erVE40aNcrhqAqvjPb9Ofk6PnjwoJDJZKJ58+Zi9erVYty4cUIul4tPPvlEo19OvtbVfv75ZwFAdO/eXaxevVoMGDBAABDfffddltf15ZdfCgBixIgRYvXq1aJ9+/YCgPj11181+j179kzY29sLDw8P8eOPP4o5c+YIW1tbUb16dREfH6+1XvXnwIsXL+ocS4FMwHSVUwlYXFycePXqlRBCiIsXL2b4ZpmWS5cuCQBi3rx5HxxLdhTEBKwgySgBCwkJEZGRkUIIIebPn59uv0ePHoknT55otCmVStGiRQuhUChEVFSU1D5nzhwBQNy4cUOj/8CBAwUAERoamuX78PTpU5GcnCyEEMLT0zPdN/EzZ85o7Xzu3bsnFAqF6N+/v0Z7uXLlRJ06dTQ+9EZERAgLC4tsJYoRERHC0dFRdO3aVYo1M9evXxeGhoZi1qxZhTIBUz8vCmIClvI5XZRllICdP39ea/8bGxsrPDw8RIMGDbK0ncjISBESEiKEEGLnzp0Z7nPbtm0rnJ2dRUREhNS2Zs0aAUAcOXIkS9sVQojt27drvf7evHkjbGxsRN++fbO0Ll33qUKoPgO0b99e53UX1AQsOjpaCJE/ErAP5e7unqXHLDPJycnZ/uKmIMho35+Tr+PKlSuL6tWri8TERKlt6tSpQiaTidu3b0ttOflaF0KImJgYYWdnp/Wc6N+/vzA3N8/S553nz58LIyMjMWbMGKlNqVSKJk2aiFKlSml8aTBq1ChhamoqAgMDpbaAgAABQKxatUpr3dlJwArkFMTU8/mFEPj222+l6VjNmzfHzZs3c2x7CoUCTk5O2V5efW5GeHh4pn3V9+3PP//Exx9/DDs7O1hZWWHgwIEICwvT6Pv777+jffv2KFGiBBQKBTw8PDB79mwkJydLfZo1a4YDBw4gMDBQmn6R8lyRuLg4zJgxA+XLl4eJiQmcnZ3RrVs3PHz4UCu21atXw8PDAwqFAnXq1MHFixezPBbPnz9Hly5dYG5ujuLFi2PixImIj4/X6pf6nJaU56L99NNPKFOmDMzMzNC6dWs8e/YMQgjMnj0bpUqVgqmpKTp37ozQ0FCNdV66dAm+vr6wt7eHqakp3N3dMXTo0Czfh6woVqyYxpTE9Li7u8PV1VWjTSaToUuXLoiPj8ejR4+k9sjISACAo6OjRn9nZ2fI5XIYGxsDANavXw+ZTIZ169Zp9Js7dy5kMhkOHjwotbm4uEAuz3x30LBhQ2n9auXKlYOnpydu376t0R4ZGSlN/VOzsrKChYUFTE1NAaheu82bN4eDgwPevHkj9UtISEDVqlXh4eGB6OhoAKqptEFBQZgzZw7kcjmio6OhVCozjHf8+PHo2rVrjk8tWrFiBTw9PaFQKFCiRAmMGTNG4/U9duxYWFhYpDkltG/fvnByctJ4nR46dAhNmjSBubk5LC0t0b59e6192ODBg2FhYYGHDx+iXbt2sLS0RP/+/dONccGCBWjYsCHs7OxgamoKLy+vNM8vkslkGDt2LLZs2YIKFSrAxMQEXl5e+PPPP3UejydPnsDBwQEAMHPmTGlfM2PGjExjP336NHr27InSpUtDoVDAxcUFEydORGxsrMY20joHTB373r17UaVKFSgUCnh6euLw4cMa/dI6B8zNzQ0dOnTAX3/9hbp168LExARlypTBpk2btO7fv//+C29vb5iamqJUqVL49ttvpddXbk4v37VrFwwMDDSmt5iYmGDYsGE4d+4cnj17pvO6LC0tUaxYsUz7RUZGIiAgAB999BGsrKyk9oEDB8LCwgI7duwAAMTGxqJixYqoWLGixmMVGhoKZ2dnNGzYUHqO79q1C46OjujWrZvUz8HBAb169cLvv/+e5ntAenTdp6aUkJAg7UdySlJSEmbPni29J7q5ueGrr77SuC8dOnRAmTJl0ly+QYMGqF27tkbb5s2b4eXlBVNTUxQrVgx9+vTReoybNWuGKlWq4J9//kHTpk1hZmaGr776Ks1tJCQkYNq0afDy8oK1tTXMzc3RpEkTnDx5UqNfyvfXxYsXw9XVFaampvD29saNGzeyNC5pnQOmfv2/ePECXbp0gYWFBRwcHDB58mTpOaI+F/zx48c4cOCAtA9Rv77i4+Mxffp0lC1bVtpPfPHFF1rPnZT7M/U+Wr0/ePHiBYYOHSqdIuLp6an1/qiOY8eOHZgzZw5KlSoFExMTtGzZEg8ePNC6v+fPn0e7du1ga2sLc3NzVKtWDT/++KNGnzt37qBHjx4oVqwYTExMULt2bezbty9L45pVur6OdXHr1i3cunULI0eOhKGhodQ+evRoCCE03ld0fa1Pnz4dcrkcx48f19jWyJEjYWxsjGvXrgEATp48iZCQEIwePVqj35gxYxAdHZ2lUxp+//13JCYmaqxLJpNh1KhReP78Oc6dOye17969Gx06dEDp0qWlNh8fH5QvXz5LY5cRw8y75H/Tpk3Dt99+i3bt2qFdu3a4fPkyWrdujYSEBI1+SqVS60N5eqytrWFkZJTtmEJCQpCcnIynT59i1qxZAICWLVvqvPzYsWNhY2ODGTNm4O7du1i5ciUCAwOlnQOg+lBhYWGBSZMmwcLCAidOnMC0adMQGRmJ+fPnAwCmTp2KiIgIPH/+HIsXLwYA6STj5ORkdOjQAcePH0efPn0wfvx4vHv3DgEBAbhx4wY8PDykeLZu3Yp3797h448/hkwmww8//IBu3brh0aNHOo9TbGwsWrZsiadPn+LTTz9FiRIl4O/vjxMnTug8Llu2bEFCQgLGjRuH0NBQ/PDDD+jVqxdatGiBU6dOYcqUKXjw4AGWLVuGyZMnSzvXN2/eoHXr1nBwcMCXX34JGxsbPHnyBHv27NFYf1hYmMYH4/SYmZnBzMxM57iz6/Xr1wBUc7zVmjVrhu+//x7Dhg3DzJkzYWdnh7Nnz2LlypX49NNPYW5uDgAYMmQI9uzZg0mTJqFVq1ZwcXHB9evXMXPmTAwbNgzt2rXLkRiFEAgKCoKnp6dGe7NmzbBr1y4sW7YMHTt2RFxcHJYtW4aIiAiMHz8eAKQEsVq1avjkk0+kx2P69Om4efMmTp06Jd2fY8eOwcrKSnoTv3fvHszNzTFgwAAsXrwYJiYmGtvfuXMnzp49i9u3b+foh+QZM2Zg5syZ8PHxwahRo6TX58WLF3HmzBkYGRmhd+/e+Omnn3DgwAH07NlTWjYmJgb79+/H4MGDYWBgAADw9/fHoEGD4Ovri++//x4xMTFYuXIlGjdujCtXrmh8EZGUlARfX180btwYCxYsyPA5+OOPP6JTp07o378/EhISsG3bNvTs2RN//PEH2rdvr9H3f//7H7Zv345PP/0UCoUCK1asQJs2bXDhwgWdTux2cHDAypUrMWrUKHTt2lV6861WrVqmse/cuRMxMTEYNWoU7OzscOHCBSxbtgzPnz/Hzp07M932X3/9hT179mD06NGwtLTE0qVL0b17dzx9+hR2dnYZLvvgwQP06NEDw4YNw6BBg7Bu3ToMHjwYXl5e0vP5xYsXaN68OWQyGfz8/GBubo5ffvkFCoVCa31RUVGIi4vLNGYjIyNYW1tn2u/KlSsoX768xgcoANJ5YlevXoWLi0um68mK69evIykpSSs5MDY2Ro0aNXDlyhUAgKmpKTZu3IhGjRph6tSpWLRoEQDVB6OIiAhs2LBBeo5fuXIFtWrV0vqSp27duli9ejXu3buHqlWr5uj9UDtx4gTMzMyQnJwMV1dXTJw4Udr/fIjhw4dj48aN6NGjBz777DOcP38e8+bNw+3bt/Hbb78BAHr37o2BAwfi4sWLqFOnjrRsYGAg/v77b+l9GgDmzJmDb775Br169cLw4cMRHByMZcuWoWnTprhy5QpsbGykviEhIWjbti369OmDjz76SOvLOLXIyEj88ssv6Nu3L0aMGIF3795h7dq18PX1xYULF1CjRg2N/ps2bcK7d+8wZswYxMXF4ccff0SLFi1w/fr1dLehq+TkZPj6+qJevXpYsGABjh07hoULF8LDwwOjRo1CpUqV4O/vj4kTJ6JUqVL47LPPAKj2LUqlEp06dcJff/2FkSNHolKlSrh+/ToWL16Me/fuYe/evRrbOnHiBHbs2IGxY8fC3t4ebm5uCAoKQv369aUEzcHBAYcOHcKwYcMQGRmpVezju+++g1wux+TJkxEREYEffvgB/fv3x/nz56U+AQEB6NChA5ydnTF+/Hg4OTnh9u3b+OOPP6Tn2M2bN9GoUSOULFkSX375JczNzbFjxw506dIFu3fvls4/zunPp7q+jnWh7pt6XSVKlECpUqU01qXra/3rr7/G/v37MWzYMFy/fh2WlpY4cuQI1qxZg9mzZ6N69eoZbtvLywtyuRxXrlzBRx99pPP9MDc3R6VKlbRiU9/euHFjvHjxAm/evNHaprpvyi+vP4jOx8rykZRTvt68eSOMjY1F+/btNaY6ffXVVwKAxhRE9VQPXX7Sm5qh6xREhUIhrcvOzk4sXbo0S/fNy8tLJCQkSO0//PCDACB+//13qS0mJkZr+Y8//liYmZlpnD+V3hTEdevWCQBi0aJFWrepx1I9ZnZ2dhqHen///XcBQOzfv1+n+yWEEEuWLBEAxI4dO6S26OhoUbZsWa0xT31YXR2Hg4ODCA8Pl9r9/PwEAK1D43379hXGxsbSOPz22286HR52dXXV6fmRcppVRlMQU8psukxqISEhonjx4qJJkyZat82ePVuYmppqxDR16lStfq9evRLFihUTrVq1EvHx8aJmzZqidOnSGlMSUstoCmJa/P39BQCxdu1ajfagoCDRsmVLjRjt7e3F2bNntdaxatUqAUBs3rxZ/P3338LAwEBMmDBBo0+1atWEmZmZMDMzE+PGjRO7d+8W48aNEwBEnz59NPrGxMSI0qVLCz8/PyHEf9NcP3QKonp/07p1a41pkMuXLxcAxLp164QQqtdPyZIlRffu3TWW37FjhwAg/vzzTyGEEO/evRM2NjZixIgRGv1ev34trK2tNdoHDRokAIgvv/xSK660pqGk3j8kJCSIKlWqiBYtWmi0qx+bS5cuSW2BgYHCxMREdO3aNcPxSCmjKYgZxZ7WfmzevHlCJpNpTP9QP0apYzc2NhYPHjyQ2q5duyYAiGXLlkltab1G1a919WMhhOrxVSgU4rPPPpPaxo0bJ2Qymbhy5YrUFhISIooVK6a1TvX9zOwn5esroymInp6eWo+XEELcvHlTABA///yz1m26yGgKovq2lOOi1rNnT+Hk5KTR5ufnJ+Ryufjzzz+lZZcsWaLRx9zcXAwdOlRrfQcOHBAAxOHDh7N1PzLbp3bs2FF8//33Yu/evWLt2rWiSZMmAoD44osv0uyv6xTEq1evCgBi+PDhGu2TJ08WAMSJEyeEEKpp06mfT0Ko3s9TPr+fPHkiDAwMxJw5czT6qadQp2z39vZO97FPPQUxKSlJa8p4WFiYcHR01Hg81M9BU1NT8fz5c6ldPQV24sSJmY5J6nWlfD6rXxezZs3S6FuzZk3h5eWl0ZbWtFF/f38hl8u1zntUnxt05swZqQ2AkMvl4ubNmxp9hw0bJpydncXbt2812vv06SOsra2l/ZD6vaJSpUoaY/fjjz8KANK5zklJScLd3V24urqKsLAwjXWm/CzasmVLUbVqVY3PZEqlUjRs2FCUK1dOa9yy+vk0vSmIWX0dZ0T9Onv69KnWbXXq1BH169eX/s/Ka/369evC2NhYDB8+XISFhYmSJUuK2rVra3yWGzNmjDAwMEgzLgcHB633/oy0b99elClTRqs9Ojpa4/1J/Tl/06ZNWn0///xzAUCrRkF2piAW+CNgx44dk46IpJyeMmHCBMydO1ejr5OTk87Vo9TZd3YdOnQIcXFxuH37NjZv3pzl6Q8jR47U+IZj1KhR+Oqrr3Dw4EF06tQJAKRpXADw7t07xMfHo0mTJli1ahXu3LmT6X3YvXs37O3tMW7cOK3bUk/16d27N2xtbaX/1VO6Uk6Ny8zBgwfh7OyMHj16SG1mZmYYOXIkvvjiC53W0bNnT41vjuvVqwcA+OijjzQOjderVw+//vorXrx4gTJlykjfHv7xxx+oXr16ut8ebdmyRWvqU1rSm1aSU5RKJfr374/w8PA0q/24ubmhadOm6N69O+zs7HDgwAHMnTsXTk5OGDt2rNTPyckJP/30E/r27YsmTZrg6tWrCAgI0PpGPbvu3LmDMWPGoEGDBhg0aJDGbWZmZqhQoQJKlSqFDh064N27d1i8eDG6deuG06dPo2zZslLfkSNHYs+ePRg3bhzs7e3h4eGh9fqNiopCTEwMPvnkE6nqYbdu3ZCQkIBVq1Zh1qxZKFeuHADVt5eJiYnpTs3JLvX+ZsKECRrf8I0YMQJfffUVDhw4gCFDhkAmk6Fnz55YtWoVoqKipKPO27dvR8mSJdG4cWMAqm9Qw8PD0bdvX40S+AYGBqhXr57WVCFAtS/QRcr9g/rIbpMmTdKs+NSgQQN4eXlJ/5cuXRqdO3fG/v37kZycLB3J+FBpxZ4yzujoaMTGxqJhw4YQQuDKlSsaU0DS4uPjo3G0vlq1arCystJp31S5cmWN6akODg6oUKGCxrKHDx9GgwYNNI4WFCtWDP3799d6bX7xxRc6fRubcl+akdjY2DSPtKmP9uqyr8oq9TrT225aU0P/+OMPDBo0CFFRUfD29sann36qtc68vh8AtKZ5DRkyBG3btsWiRYswbtw4lCpVKlvrVX8DPmnSJI32zz77DAsWLMCBAwfQvHlzWFlZoW3bttixYwfmz58vva9u374d9evXl57be/bsgVKpRK9evTT2A05OTihXrhxOnjypsS9TKBQYMmRIpnEaGBhIr12lUonw8HAolUrUrl07zap0Xbp0QcmSJaX/69ati3r16uHgwYPSEc4Pkbr6ZJMmTeDv75/pcjt37kSlSpVQsWJFjfFp0aIFANU0tYYNG0rt3t7eqFy5svS/EAK7d+9Gr169IITQWIevry+2bduGy5cvo1GjRlL7kCFDNKbbp/zMU6VKFVy5cgWPHz/G4sWLNY5OAv99fgoNDcWJEycwa9YsvHv3Du/evdPY7vTp0/HixQuULFkyxz+fZvV1/CHrUp8Woe6r62u9SpUqmDlzJvz8/PDvv//i7du3OHr0qMZnudjYWK3THj7kfugSW2b3N6N1ZUWBT8ACAwMBQPrgpebg4KD1JmdiYgIfH588iat58+YAgLZt26Jz586oUqUKLCwspA/H6qllatbW1hofRFLfHwsLCzg7O2tMpbp58ya+/vprnDhxQuMFAAARERGZxvjw4UNUqFBB48mentQfgtRjm/q8tIwEBgaibNmyWsldhQoVdF5H6jjUyVjqaTjqdnV83t7e6N69O2bOnInFixejWbNm6NKlC/r166fxIkq5A9ancePG4fDhw9i0aZPWznbbtm0YOXIk7t27J32A6NatG5RKJaZMmYK+fftqTL3q06cPNm/ejAMHDmDkyJFZmgqbkdevX6N9+/awtraWzlVJqWfPnjA0NMT+/fults6dO6NcuXKYOnWqVon9tWvXwsPDA/fv38fZs2c1Xg/Afx/U+/btq9Her18/rFq1CufOnUO5cuXw5MkTzJ8/Hz/99FO2r+kTGhqqMYXZ1NQU1tbW0v4m9XPW2NgYZcqUkW4HVF9aLFmyBPv27UO/fv0QFRWFgwcPStN4AdXlBID/PkikljpRNjQ01PlD4x9//IFvv/0WV69e1ThPIq1raaXe3wBA+fLlERMTg+Dg4A86B1YtvdifPn2KadOmYd++fVr7E132Y2klaLa2tjrtm3RZNjAwEA0aNNDql/ILBLXKlStrfPD7UKampmmeH6We5pj6NZJT2wSQ7nZTb9PY2Bjr1q1DnTp1YGJiIp0bl3qdeX0/0iKTyTBx4kQcOXIEp06dyjRZDg4O1piSbmFhAQsLCwQGBkIul2s9B5ycnGBjY6O1H9i7dy/OnTuHhg0b4uHDh/jnn380Lg1z//59CCHSfB0C0PrCsGTJkul+KE1t48aNWLhwIe7cuYPExESp3d3dXatvevuBnDjnxcTERDpPVE3X1+n9+/dx+/ZtreXVUp4/DGjft+DgYISHh2P16tVYvXq1TuvI7DOP+jz5jKZoP3jwAEIIfPPNN/jmm2/S3W7JkiVz/PNpVl/HObWurL7WP//8c2zbtg0XLlzA3LlztfafpqamWqcTfcj90CW2zO5vWvcjOwp8ApYVycnJWjX801OsWDGdd3CZ8fDwQM2aNbFlyxYpAXN2dtbos379+ixdNDo8PBze3t6wsrLCrFmz4OHhARMTE1y+fBlTpkzJtDhBVqX3DbgQIke3k904MotPfQ2ov//+G/v378eRI0cwdOhQLFy4EH///bf0QT31G2561G/EuWHmzJlYsWIFvvvuuzSvgbJixQrUrFlT68Nsp06dsGHDBly5ckVjRx4SEiJdHPDWrVtQKpU6FdzISEREBNq2bYvw8HCcPn0aJUqU0Lj90aNHOHz4sNabXbFixdC4cWOcOXNGa52nTp2SdnjXr1/X+tBbokQJ3Lx5U+tchOLFiwP4741x2rRpKFmyJJo1ayZ9YaH+wiM4OBhPnjxB6dKlMxyDbt264X//+5/0/6BBg7J8YdH69evDzc0NO3bsQL9+/bB//37Exsaid+/eUh/169Tf3z/NJCf1lyMKhUKnx+706dPo1KkTmjZtihUrVsDZ2RlGRkZYv349tm7dmqX7kVPSij05ORmtWrVCaGgopkyZgooVK8Lc3BwvXrzA4MGDddqPfci+Kaf3axERETp9I2tsbKxTQQxnZ2e8ePFCq/3Vq1cAoPW6ywnq9yb1NlJvN61tHjlyBIDqw8n9+/e1PgA7Ozunuz4gd+5HetRf1ulyvk2dOnU0kqnp06dLhWWAtL/MSK1jx44wMzPDjh070LBhQ+zYsQNyuVzj3FClUgmZTIZDhw6l+ZxM/V6j64e/zZs3Y/DgwejSpQs+//xzFC9eHAYGBpg3b16ahbZy04ccRVcqlahatWq6R+FSfwGbenzU+5GPPvpIa6aGWsrzVYGc2Teotzt58mT4+vqm2UedxOf059PsvI51WVfqsX716pXGtQuz+lp/9OiR9EXk9evX09x2cnIy3rx5I73XA6oCMyEhIVm+HydPnoQQQuO1mzq2zMauWLFiH3z0CygECZi6ctz9+/c1poUFBwdrfbPy7NmzNL/1ScvJkydz9IrysbGxGtl06kPNqQsY3L9/XzqKBqimX7169UoqnHDq1CmEhIRgz549aNq0qdTv8ePHWttO703Cw8MD58+fR2Ji4gcVHNGVq6srbty4ofXkv3v3bq5vW61+/fqoX78+5syZg61bt6J///7Ytm0bhg8fDkD7DTc9qd+Ic8pPP/2EGTNmYMKECZgyZUqafYKCgtKcwqT+djP1hRHHjBmDd+/eYd68efDz88OSJUu0ps5kRVxcHDp27Ih79+7h2LFjaX7jHxQUBABpJrOJiYlaMb569Qrjxo1D69atYWxsLL1hpawM6eXlhYCAALx48ULjCNTLly8BQPp29OnTp3jw4EGa00TV1Y/CwsK0po2ktHDhQo39h3rHrI7n7t27GutPSEjA48ePtb7B7NWrF3788UdERkZi+/btcHNzQ/369aXb1VPnihcvnqPffu7evRsmJiY4cuSIxhvF+vXr0+yvfgNM6d69ezAzM0v3W+fUdPkwmtr169dx7949bNy4EQMHDpTas3Oh4dzi6uqaZvWztNrGjx+PjRs3ZrpOb29vnS5YXaNGDZw8eRKRkZEaR0PVhQBSF1HICVWqVIGhoSEuXbqEXr16Se0JCQm4evWqRhugqhA5a9YsDBkyBFevXsXw4cNx/fp1janiNWrUwOnTp7W+/Dl//jzMzMxQvnz5HL8f6VFPL9XleZ16Srr6Ne/q6gqlUon79+9rnNAfFBSE8PBwjf2Wubk5OnTogJ07d2LRokXYvn07mjRpovHB0cPDA0IIuLu75+hY7Nq1C2XKlMGePXs0Xp/Tp09Ps396+4GUhYD0wcPDA9euXUPLli2ztZ9xcHCApaUlkpOTc2w/q95337hxI911qp8vRkZGmW43pz+fZvV1nBH1fubSpUsaydbLly/x/PlzjSqtWXmtK5VKDB48GFZWVtJpQz169NCooJhy2ykLh126dAlKpTJL+8AaNWrgl19+we3btzU+t6Ten5YsWRIODg7SF9cppVW8JrsKZBn6lHx8fGBkZIRly5ZpfDOR8vC+mnqOrS4/2TkHLCkpKc3D6RcuXMD169c1Kqr4+Pho/KQ+IrZ69WqN6QIrV65EUlIS2rZtC+C/b2dS3ueEhASsWLFCa/vm5uZpTuXp3r073r59i+XLl2vdlhtHttq1a4eXL19qlCyNiYlJd0pATgoLC9O6T+oXUcrEeMuWLTo9P1J+WMwp6ip0/fv3z3C+ffny5XHlyhXcu3dPo/3XX3+FXC7X+CZv165d2L59O7777jt8+eWX6NOnD77++mutZXWVnJyM3r1749y5c9i5c2eaU7MA1bd6crkc27dv1xj358+f4/Tp06hZs6ZG/xEjRkCpVGLt2rVYvXo1DA0NMWzYMI1l1W8Ya9eu1Vj2l19+gaGhofSG9O233+K3337T+Jk9ezYA1Tk6v/32m1RZMT1eXl4ar0/1ztrHxwfGxsZYunSpRmxr165FRESEVnXB3r17Iz4+Hhs3bsThw4e13vR8fX1hZWWFuXPnarze1XT9RjQ1AwMDyGQyjQT4yZMnWtXC1M6dO6dxTsizZ8/w+++/o3Xr1jp/c62uaqjL5TZSxglo7m+EEFplnPXJ19cX586dw9WrV6W20NBQbNmyRavvF198odP+Y+HChTptu0ePHkhOTtbYR8bHx2P9+vWoV69ejldABFTTt318fLB582aNc1b8/f0RFRWlceQmMTERgwcPRokSJfDjjz9iw4YNCAoKwsSJE7XuR1BQkEbV2bdv32Lnzp3o2LFjjnybnFpoaKjWF0CJiYn47rvvYGxsrPEFZ3oaNWqksR9Qf6BWfxBM/TlDvd9Oaz/w8uVL/PLLL7h27ZrGUXBAdcTdwMAAM2fO1HqfEkIgJCQk8zuchrReX+fPn9cot53S3r17NY64XrhwAefPn5c+d+hLr1698OLFC6xZs0brttjY2EzPsTcwMED37t2xe/fuNMvqZ2c/W6tWLbi7u2PJkiVa+zz1eBcvXhzNmjXDqlWr0jyaknK7Of35NCuv48x4enqiYsWKWL16tcZrauXKlZDJZBrn9Wfltb5o0SKcPXsWq1evxuzZs9GwYUOMGjVK6zy/YsWKYeXKlRoxrVy5EmZmZlqvtYx07twZRkZGGp+ThRD4+eefUbJkSY3zCLt3744//vhD4zIQx48fx71797I0dhkp8EfA1NeSmDdvHjp06IB27drhypUrOHTokEbpbuDDzgFbvnw5wsPDpW/b9+/fj+fPnwNQna9jbW2NqKgouLi4oHfv3vD09IS5uTmuX7+O9evXw9raOt05wGlJSEhAy5Yt0atXL9y9excrVqxA48aNpQIcDRs2hK2tLQYNGoRPP/0UMpkM/v7+aSZOXl5e2L59OyZNmoQ6derAwsICHTt2xMCBA7Fp0yZMmjQJFy5cQJMmTRAdHY1jx45h9OjR6Ny5c7bGKj0jRozA8uXLMXDgQPzzzz9wdnaGv79/npRz37hxI1asWIGuXbvCw8MD7969w5o1a2BlZaXxrUpOnwMWEREhnaivnna3fPly2NjYwMbGRpqSeuHCBQwcOBB2dnZo2bKl1oe7hg0bSm/+n3/+uXTdqLFjx8LOzg5//PEHDh06hOHDh0vfrL558wajRo1C8+bNpe0sX74cJ0+exODBg/HXX39J31D9+eef0nWfgoODER0djW+//RYA0LRpU+ko62effYZ9+/ahY8eOCA0NxebNmzXiVJ9T4eDggKFDh+KXX35By5Yt0a1bN7x79w4rVqxAbGws/Pz8pGXWr1+PAwcOYMOGDdK0ymXLluGjjz7CypUrpaNWNWvWxNChQ7Fu3TokJSVJRxF27twJPz8/6X6rC1ykpD7aVadOHXTp0kXHR0+bg4MD/Pz8MHPmTLRp0wadOnWSXp916tTROqekVq1aKFu2LKZOnYr4+HitD15WVlZYuXIlBgwYgFq1aqFPnz5wcHDA06dPceDAATRq1CjNL0gy0759eyxatAht2rRBv3798ObNG/z0008oW7Ys/v33X63+VapUga+vr0YZekA1HVZXpqamqFy5MrZv347y5cujWLFiqFKlSobnSFSsWBEeHh6YPHkyXrx4ASsrK+zevTtL55bmti+++AKbN29Gq1atMG7cOKkMfenSpREaGqrxjXxOnwNWr1499OzZE35+fnjz5g3Kli2LjRs34smTJ1pfROhC/ZpWX2PO398ff/31FwDg66+/lvrNmTMHDRs2hLe3N0aOHInnz59j4cKFaN26Ndq0aaOxvqtXr+L48eOwtLREtWrVMG3aNHz99dfo0aOHtG/t0aMH6tevjyFDhuDWrVuwt7fHihUrkJycnKXnGKD7PnXfvn349ttv0aNHD7i7uyM0NBRbt27FjRs3pIJF2VW9enUMGjQIq1evlk4HuHDhAjZu3IguXbpoJXfqa99NnjxZSgZS8vDwwLfffgs/Pz88efIEXbp0gaWlJR4/fozffvsNI0eOxOTJk7McZ4cOHbBnzx507doV7du3x+PHj/Hzzz+jcuXKiIqK0upftmxZNG7cGKNGjUJ8fDyWLFkCOzs7nYtk5ZYBAwZgx44d+OSTT3Dy5Ek0atQIycnJuHPnDnbs2IEjR46kWTI8pe+++w4nT55EvXr1MGLECFSuXBmhoaG4fPkyjh07pnMJeDW5XI6VK1eiY8eOqFGjBoYMGQJnZ2fcuXMHN2/elKbl/vTTT2jcuDGqVq2KESNGoEyZMggKCsK5c+fw/Plz6XpXuVGjQNfXsS7mz5+PTp06oXXr1ujTpw9u3LiB5cuXY/jw4RpHgXV9rd++fRvffPMNBg8ejI4dOwJQXVqpRo0aGD16tHTeoampKWbPno0xY8agZ8+e8PX1xenTp7F582bMmTNHp6ncaqVKlcKECRMwf/58JCYmok6dOti7dy9Onz6NLVu2aHzZ+NVXX2Hnzp1o3rw5xo8fj6ioKMyfPx9Vq1bVqQCOTnSul5iPpC4pnJycLGbOnCmcnZ2FqampaNasmbhx44ZwdXXVKEP/ITIqT66OIz4+XowfP15Uq1ZNWFlZCSMjI+Hq6iqGDRumc+lx9X373//+J0aOHClsbW2FhYWF6N+/vwgJCdHoe+bMGVG/fn1hamoqSpQoIb744gtx5MgRrTKlUVFRol+/fsLGxkYA0ChZGhMTI6ZOnSrc3d2FkZGRcHJyEj169BAPHz4UQvxXGnX+/PlasSKdktMZCQwMFJ06dRJmZmbC3t5ejB8/Xhw+fDjT0qrpxZFeefHUJUEvX74s+vbtK0qXLi0UCoUoXry46NChg0bp7ezKqAx9RqVlU94/9TrS+0ldovr8+fOibdu2wsnJSRgZGYny5cuLOXPmaJRv7datm7C0tBRPnjzRWFZ9CYHvv/9ealOX+E7rJ+VjrC6BnN5PSomJiWLZsmWiRo0awsLCQlhYWIjmzZtLJZqFEOLZs2fC2tpadOzYUWvsunbtKszNzcWjR4+ktoSEBDFjxgzh6uoqjIyMRNmyZcXixYvTelg05FQZerXly5eLihUrCiMjI+Ho6ChGjRqlVYpYberUqQKAKFu2bIbx+fr6Cmtra2FiYiI8PDzE4MGDNZ6fGZXITqsU8dq1a0W5cuWEQqEQFStWFOvXr0+3lPuYMWPE5s2bpf41a9ZM91IcGTl79qzw8vISxsbGGs+djGK/deuW8PHxERYWFsLe3l6MGDFCKiWf8nmfUeyppd73p1eGPnW5ayG0S3kLIcSVK1dEkyZNhEKhEKVKlRLz5s0TS5cuFQDE69evMx6UTGRUhl4IIWJjY8XkyZOFk5OTUCgUok6dOtku267ra1cIIU6fPi0aNmwoTExMhIODgxgzZoyIjIyUbv/nn3+EoaGhGDdunMZySUlJok6dOqJEiRIar4nQ0FAxbNgwYWdnJ8zMzIS3t3eWSjar6bpPvXTpkujYsaMoWbKkMDY2FhYWFqJx48Yal0FJTdcy9EKo9m8zZ86U3jtdXFyEn5+fVnlqtf79+wsAwsfHJ9117t69WzRu3FiYm5sLc3NzUbFiRTFmzBhx9+5dqY+3t7fw9PRMc/nUz12lUinmzp0rXF1dpdf1H3/8keH768KFC4WLi4tQKBSiSZMm4tq1azqNR+p1pS5Dn9a4pvWaTu91mZCQIL7//nvh6ekpFAqFsLW1FV5eXmLmzJkal1VJb58ghOrSKGPGjBEuLi7S552WLVuK1atXS33Se69I73X6119/iVatWglLS0thbm4uqlWrpnEJDCGEePjwoRg4cKD0fl2yZEnRoUMHsWvXrjTjzIr0ytCrZfY6zorffvtN1KhRQ9oPfv311xqXS1LL7LWu3keUKlVK47JCQvxX7n/79u0a7atXrxYVKlQQxsbGwsPDQyxevFij3L+ukpOTpdeEsbGx8PT0FJs3b06z740bN0Tr1q2FmZmZsLGxEf379093f5+dMvQFMgErzLLzIJJ+qR+zy5cvi+Dg4GztFEg/YmNjRXBwsHRtD10TsMIkow8slL7x48cLExMTkZSU9EHrUX+wW7ZsmQgODta6bhPlvqioKBEcHCz69OmjcwJWmGT0RSvlb4MGDRIuLi4iODg43S8BKffEx8eL4OBgsWzZsix/di/w54AR5Re1atWCg4NDtufrU977+eef4eDggPnz5+s7FMrHUlc2DAkJgb+/Pxo3bpxj10gbN24cHBwctK5dRblv6tSpcHBwwLZt2/QdClGWPXv2DA4ODmlOv6fcdfDgQTg4OKR5Pd3MFPhzwEj/EhISMp0/nfo6Z4WJr6+vRtW2lNW/KH/r3r27xjlKfOy06VIeOTcvy5AfNGjQAM2aNUOlSpUQFBSEtWvXIjIyMkvn9aYn9QVYU5fDzowupe9z4jpuuS31tfdSMzAw0LkiZ1aNHj0aHTp0AKB96QdSKerv8/lVyou/Z3UfXFj27bGxsZleMzInLy2VUqNGjTT231m5ri2nIOYzBXEKonrOdEY/6Z3fQET6hUymIGZ0zo36J6vnghY0fn5+oly5csLU1FSYmZmJxo0bi4CAAH2HJYRQTUHK7PEpCDI7vzSj81zow+gyBZHv84VPYdm3Z3YOPYBsndOc22RC5PGVdKnQCQsLwz///JNhH09PT61S+0SU/8XFxUmV8tJTpkyZNK+7Rrnv1q1bUnXe9OR0dbXc8M8//2RY/dLU1DTHq9SS7vg+X/gUln37q1evpMqu6fHy8krz+qn6xASMiIiIiIgoj7AIBxERERERUR5hAkZEhcKGDRsgk8nw5MmTLC976tQpyGQynDp1Ksfjyk/c3NwwePDgPN9us2bN0KxZM+n/J0+eQCaTYcOGDZkuO3jwYLi5ueVoPB/yXCHKrmbNmmV4YXIiKjqYgBFRrmrWrBlkMlmmPzNmzNB3qEXenj17IJPJ8Msvv6TbJyAgADKZDEuXLs3DyLJn7ty52Lt3r77D0ODm5iZV3MvvoqOjMXv2bFSrVg1mZmawtrZGkyZNsGnTJuTHsxcy2tdUrFhR3+EREUlYb5WIctXUqVMxfPhw6f+LFy9i6dKl+Oqrr1CpUiWpPavlt1MbMGAA+vTpA4VCkeVlmzZtitjY2FwpU1uQtG/fHtbW1ti6davGY5bS1q1bYWBggD59+mR7O66uroiNjYWRkVG216GLuXPnokePHujSpYtG+4c8V4qKoKAgtGzZErdv30afPn0wduxYxMXFYffu3Rg0aBAOHjyILVu25Nh10HJKqVKlMG/ePK12XmKCiPITJmBElKtatWql8b+JiQmWLl2KVq1aaUxLSy06Ohrm5uY6b8fAwCDbHwblcjlMTEyytWxholAo0KNHD6xfvx4vX75EiRIlNG6Pi4vDb7/9hlatWqF48eLZ3o5MJtPreH/Ic6WoGDRoEG7fvo3ffvsNnTp1kto//fRTfP7551iwYAFq1qyJKVOm5FlMSqUSCQkJGT53rK2tpesiERHlV5yCSER6N2PGDMhkMty6dQv9+vWDra0tGjduDAD4999/MXjwYJQpUwYmJiZwcnLC0KFDERISorGOtM7rUU/3+uuvv1C3bl2YmJigTJky2LRpk8ayaZ0Dpj5f49atW2jevDnMzMxQsmRJ/PDDD1rxBwYGolOnTjA3N0fx4sUxceJEHDlyRKfzygIDAzF69GhUqFABpqamsLOzQ8+ePbXOT1LfvzNnzmDSpElwcHCAubk5unbtqnUxTSEEvv32W5QqVQpmZmZo3rx5pmV61T766CMolUps27ZN67YDBw4gIiIC/fv3BwCsX78eLVq0QPHixaFQKFC5cmWsXLky022kdw7Y3r17UaVKFZiYmKBKlSr47bff0lx+wYIFaNiwIezs7GBqagovLy/s2rVLo49MJkN0dDQ2btwoTUNTn/+W3jlgK1asgKenJxQKBUqUKIExY8YgPDxco09WnhfZlZSUhNmzZ8PDwwMKhQJubm746quvEB8fr9Hv0qVL8PX1hb29PUxNTeHu7o6hQ4dq9Nm2bRu8vLxgaWkJKysrVK1aFT/++GOG2//7779x5MgRDB48WCP5Ups3bx7KlSuH77//HrGxsUhMTESxYsUwZMgQrb6RkZEwMTHB5MmTpbb4+HhMnz4dZcuWhUKhgIuLC7744gut+yeTyTB27Fhs2bJFelwOHz6c6fhlRr2/uXPnDnr16gUrKyvY2dlh/PjxiIuL0+ir62MBAIcOHYK3t7c01nXq1MHWrVu1+uny3Fm2bBk8PT1hZmYGW1tb1K5dO811EVHBxASMiPKNnj17IiYmBnPnzsWIESMAqM45evToEYYMGYJly5ahT58+2LZtG9q1a6fTeSgPHjxAjx490KpVKyxcuBC2trYYPHiwTglJWFgY2rRpg+rVq2PhwoWoWLEipkyZgkOHDkl9oqOj0aJFCxw7dgyffvoppk6dirNnz+p8ZODixYs4e/Ys+vTpg6VLl+KTTz7B8ePH0axZM8TExGj1HzduHK5du4bp06dj1KhR2L9/P8aOHavRZ9q0afjmm29QvXp1zJ8/H2XKlEHr1q0RHR2daTxNmzZFqVKl0vywt3XrVpiZmUlT+lauXAlXV1d89dVXWLhwIVxcXDB69Gj89NNPOt33lI4ePYru3btDJpNh3rx56NKlC4YMGYJLly5p9f3xxx9Rs2ZNzJo1C3PnzoWhoSF69uyJAwcOSH38/f2hUCjQpEkT+Pv7w9/fHx9//HG6258xYwbGjBmDEiVKYOHChejevTtWrVqF1q1bIzExUaOvLs+LDzF8+HBMmzYNtWrVwuLFi+Ht7Y158+ZpTPt88+YNWrdujSdPnuDLL7/EsmXL0L9/f/z9999Sn4CAAPTt2xe2trb4/vvv8d1336FZs2Y4c+ZMhtvfv38/AGDgwIFp3m5oaIh+/fohLCwMZ86cgZGREbp27Yq9e/ciISFBo+/evXsRHx8vxa5UKtGpUycsWLAAHTt2xLJly9ClSxcsXrwYvXv31trWiRMnMHHiRPTu3Rs//vhjpgVZkpOT8fbtW62ftJ77vXr1QlxcHObNm4d27dph6dKlGDlypEYfXR4LQJXUt2/fHqGhofDz88N3332HGjVqaCWMujx31qxZg08//RSVK1fGkiVLMHPmTNSoUQPnz5/P8L4TUQGix4tAE1ERtHPnTq0r00+fPl0AEH379tXqHxMTo9X266+/CgDizz//lNrWr18vAIjHjx9Lba6urlr93rx5IxQKhfjss8+ktpMnT2rF5O3tLQCITZs2SW3x8fHCyclJdO/eXWpbuHChACD27t0rtcXGxoqKFStqrTMtad2/c+fOaW1bff98fHyEUqmU2idOnCgMDAxEeHi4dP+MjY1F+/btNfp99dVXAoAYNGhQhvEIIcTnn38uAIi7d+9KbREREcLExETjMUordl9fX1GmTBmNNm9vb+Ht7S39//jxYwFArF+/XmqrUaOGcHZ2lu6HEEIcPXpUABCurq4a60u93YSEBFGlShXRokULjXZzc/M072/q54p6zFq3bi2Sk5OlfsuXLxcAxLp16zTuiy7Pi/S4urqK9u3bp3v71atXBQAxfPhwjfbJkycLAOLEiRNCCCF+++03AUBcvHgx3XWNHz9eWFlZiaSkpEzjSqlLly4CgAgLC0u3z549ewQAsXTpUiGEEEeOHBEAxP79+zX6tWvXTuP54O/vL+RyuTh9+rRGv59//lkAEGfOnJHaAAi5XC5u3rypU9zqxyatn48//ljqp97fdOrUSWP50aNHCwDi2rVrQgjdH4vw8HBhaWkp6tWrJ2JjYzX6pnwN6vrc6dy5s/D09NTpPhNRwcQjYESUb3zyySdabaamptLfcXFxePv2LerXrw8AuHz5cqbrrFy5Mpo0aSL97+DggAoVKuDRo0eZLmthYaFxPomxsTHq1q2rsezhw4dRsmRJjalaJiYm0hG8zKS8f4mJiQgJCUHZsmVhY2OT5v0bOXIkZDKZ9H+TJk2QnJyMwMBAAMCxY8eQkJCAcePGafSbMGGCTvEAkO5zyqNgu3fvRlxcnDT9MHXsERERePv2Lby9vfHo0SNERETovL1Xr17h6tWrGDRokEaxhFatWqFy5cpa/VNuNywsDBEREWjSpIlOz4e0qMdswoQJkMv/e1scMWIErKysNI6sAbo9L7Lr4MGDAIBJkyZptH/22WcAIMViY2MDAPjjjz+0jtCp2djYIDo6GgEBAVmK4d27dwAAS0vLdPuob4uMjAQAtGjRAvb29ti+fbvUJywsDAEBARpHtnbu3IlKlSqhYsWKGkeoWrRoAQA4efKkxna8vb3TfA6kx83NDQEBAVo/aT3/x4wZo/H/uHHjAPz3GOj6WAQEBODdu3f48ssvtc5PS/kaBHR77tjY2OD58+e4ePGizvebiAoWJmBElG+4u7trtYWGhmL8+PFwdHSEqakpHBwcpH66fMgvXbq0VputrS3CwsIyXbZUqVJaH6BSLxsYGAgPDw+tfmXLls10/QAQGxuLadOmwcXFBQqFAvb29nBwcEB4eHia9y/1/bG1tQUAKSZ1IlauXDmNfg4ODlLfzFSrVg1VqlTBr7/+KrVt3boV9vb28PX1ldrOnDkDHx8fmJubw8bGBg4ODvjqq68A6PbYqKUXMwBUqFBBq+2PP/5A/fr1YWJigmLFisHBwQErV67M0jbT2n7qbRkbG6NMmTLS7Wq6PC+yKzAwEHK5XOv54+TkBBsbGykWb29vdO/eHTNnzoS9vT06d+6M9evXa5ybNHr0aJQvXx5t27ZFqVKlMHToUJ3OoVInV+pELC2pkzRDQ0N0794dv//+uxTDnj17kJiYqJGA3b9/Hzdv3oSDg4PGT/ny5QGoplamlNY+ISPm5ubw8fHR+kmrDH3q55uHhwfkcrl0bqCuj8XDhw8BQKdrfOny3JkyZQosLCxQt25dlCtXDmPGjMl02igRFSxMwIgo30h5ZEOtV69eWLNmDT755BPs2bMHR48elT5EKpXKTNeZXrU7ocP5Yx+yrK7GjRuHOXPmoFevXtixYweOHj2KgIAA2NnZpXn/8iImQHUU7N69e7h06RJev36NkydPolevXjA0VBXPffjwIVq2bIm3b99i0aJFOHDgAAICAjBx4kQAuj022XH69Gl06tQJJiYmWLFiBQ4ePIiAgAD069cvz65NlRePQeoP6WndvmvXLpw7dw5jx47FixcvMHToUHh5eSEqKgoAULx4cVy9ehX79u1Dp06dcPLkSbRt2xaDBg3KcN3qy0P8+++/6fZR35by6FSfPn3w7t076XymHTt2oGLFiqhevbrUR6lUomrVqmkepQoICMDo0aM1tpPWPiG3pDfmmT0WWaHLc6dSpUq4e/cutm3bhsaNG2P37t1o3Lgxpk+fnmNxEJF+sQw9EeVbYWFhOH78OGbOnIlp06ZJ7ffv39djVJpcXV1x69YtCCE0Pqg9ePBAp+V37dqFQYMGYeHChVJbXFycVvW9rMQDqMaoTJkyUntwcHCWjtD07dsXfn5+2Lp1K1xdXZGcnKwx/XD//v2Ij4/Hvn37NI7KpZ5CltWYU7t7967G/7t374aJiQmOHDmicR2v9evXay2r6wdn9fbv3r2rMWYJCQl4/PgxfHx8dFpPTnB1dYVSqcT9+/c1rpMXFBSE8PBwKVa1+vXro379+pgzZw62bt2K/v37Y9u2bdJ13IyNjdGxY0d07NgRSqUSo0ePxqpVq/DNN9+ke5S2Q4cOmDdvHjZt2oSmTZtq3Z6cnIytW7fC1tYWjRo1ktqbNm0KZ2dnbN++HY0bN8aJEycwdepUjWU9PDxw7do1tGzZMkcTm+y4f/++xhG2Bw8eQKlUSoU+dH0sPDw8AAA3btzQ+ch3ZszNzdG7d2/07t0bCQkJ6NatG+bMmQM/Pz9eMoOoEOARMCLKt9TfFqc+srBkyRI9RJM2X19fvHjxAvv27ZPa4uLisGbNGp2WNzAw0Lp/y5YtQ3Jycrbi8fHxgZGREZYtW6ax3qyOWenSpdGkSRNs374dmzdvhru7Oxo2bKgRN6D52ERERKSZCGXG2dkZNWrUwMaNGzWmEQYEBODWrVsafQ0MDCCTyTTG58mTJ9i7d6/Wes3NzXVKZH18fGBsbIylS5dq3J+1a9ciIiIC7du3z/J9yq527doB0H68Fi1aBABSLGFhYVrPmxo1agCANAUw9aUa5HK5dMHztMqoqzVs2BA+Pj5Yv349/vjjD63bp06dinv37uGLL77QOEIll8vRo0cP7N+/H/7+/khKStKqbNirVy+8ePEizddHbGysTpU6c0rqap3Lli0DALRt2xaA7o9F69atYWlpiXnz5mmVsc/OUdHUj5uxsTEqV64MIUS65/sRUcHCI2BElG9ZWVmhadOm+OGHH5CYmIiSJUvi6NGjePz4sb5Dk3z88cdYvnw5+vbti/Hjx8PZ2RlbtmyRvqXO7Fv+Dh06wN/fH9bW1qhcuTLOnTuHY8eOwc7OLlvxODg4YPLkyZg3bx46dOiAdu3a4cqVKzh06BDs7e2ztK6PPvoII0eOxMuXL7WOZLRu3Vo6uvLxxx8jKioKa9asQfHixfHq1assxz1v3jy0b98ejRs3xtChQxEaGipdC0k9pQ5QfehdtGgR2rRpg379+uHNmzf46aefULZsWa0pc15eXjh27BgWLVqEEiVKwN3dHfXq1dPatoODA/z8/DBz5ky0adMGnTp1wt27d7FixQrUqVMnxy/s++DBA3z77bda7TVr1kT79u0xaNAgrF69GuHh4fD29saFCxewceNGdOnSBc2bNwcAbNy4EStWrEDXrl3h4eGBd+/eYc2aNbCyspISh+HDhyM0NBQtWrRAqVKlEBgYiGXLlqFGjRoaR3TSsmnTJrRs2RKdO3dGv3790KRJE8THx2PPnj04deoUevfujc8//1xrud69e2PZsmWYPn06qlatqrWdAQMGYMeOHfjkk09w8uRJNGrUCMnJybhz5w527NiBI0eOoHbt2tkdWkRERGDz5s1p3pb6cXz8+DE6deqENm3a4Ny5c9i8eTP69esnTZmsXr26To+FlZUVFi9ejOHDh6NOnTrStQyvXbuGmJgYbNy4MUv3oXXr1nByckKjRo3g6OiI27dvY/ny5Wjfvn2GhVGIqADRQ+VFIirCMipDHxwcrNX/+fPnomvXrsLGxkZYW1uLnj17ipcvXwoAYvr06VK/9MrQp1XyO3VZ9PTK0KdVCnrQoEFaZdEfPXok2rdvL0xNTYWDg4P47LPPxO7duwUA8ffff2c4HmFhYWLIkCHC3t5eWFhYCF9fX3Hnzh3h6uqqUUJdff9Slx1PK/bk5GQxc+ZM4ezsLExNTUWzZs3EjRs3tNaZmdDQUKFQKAQAcevWLa3b9+3bJ6pVqyZMTEyEm5ub+P7778W6deu0HgddytALIcTu3btFpUqVhEKhEJUrVxZ79uxJc7zXrl0rypUrJxQKhahYsaJYv3699BxK6c6dO6Jp06bC1NRUowR/Ws8VIVRl5ytWrCiMjIyEo6OjGDVqlFYp9qw8L9KivjRCWj/Dhg0TQgiRmJgoZs6cKdzd3YWRkZFwcXERfn5+Ii4uTlrP5cuXRd++fUXp0qWFQqEQxYsXFx06dBCXLl2S+uzatUu0bt1aFC9eXBgbG4vSpUuLjz/+WLx69SrTOIUQ4t27d2LGjBnC09NTmJqaCktLS9GoUSOxYcMGjfLqKSmVSuHi4iIAiG+//TbNPgkJCeL7778Xnp6eQqFQCFtbW+Hl5SVmzpwpIiIipH4AxJgxY3SKVYiMy9CnfG6onyu3bt0SPXr0EJaWlsLW1laMHTtWq4y8Lo+F2r59+0TDhg2FqampsLKyEnXr1hW//vqrRny6PHdWrVolmjZtKuzs7IRCoRAeHh7i888/1xgbIirYZELk0VnLRERFyJIlSzBx4kQ8f/4cJUuW1Hc4RPTejBkzMHPmTAQHB2f5qDARUU7gOWBERB8oNjZW4/+4uDisWrUK5cqVY/JFREREGngOGBHRB+rWrRtKly6NGjVqSOeg3LlzB1u2bNF3aERERJTPMAEjIvpAvr6++OWXX7BlyxYkJyejcuXK2LZtm1YFOCIiIiKeA0ZERERERJRHeA4YERERERFRHmECRkRERERElEeK3DlgSqUSL1++hKWlZaYXSCUiIiIiosJLCIF3796hRIkSkMvz5thUkUvAXr58CRcXF32HQURERERE+cSzZ89QqlSpPNlWkUvALC0tAagG2crKSs/RAImJiTh69Chat24NIyMjfYdTpHDs9Ydjrz8ce/3h2OsXx19/OPb6w7HPXGRkJFxcXKQcIS8UuQRMPe3Qysoq3yRgZmZmsLKy4gsjj3Hs9Ydjrz8ce/3h2OsXx19/OPb6w7HXXV6emsQiHERERERERHlErwnYn3/+iY4dO6JEiRKQyWTYu3dvpsucOnUKtWrVgkKhQNmyZbFhw4Zcj5OIiIiIiCgn6DUBi46ORvXq1fHTTz/p1P/x48do3749mjdvjqtXr2LChAkYPnw4jhw5ksuREhERERERfTi9ngPWtm1btG3bVuf+P//8M9zd3bFw4UIAQKVKlfDXX39h8eLF8PX1za0wiYiIiCiXCSGQlJSE5ORkfYdSaCQmJsLQ0BBxcXFFelyNjIxgYGCg7zAkBaoIx7lz5+Dj46PR5uvriwkTJqS7THx8POLj46X/IyMjAaiekImJibkSZ1aoY8gPsRQ1HHv94djrD8defzj2+sXx1x9dxj4xMRFBQUGIjY3Nq7CKBCEEnJyc8PTp0yJ9/VuZTAZnZ2eYm5tr3aaPfUKBSsBev34NR0dHjTZHR0dERkYiNjYWpqamWsvMmzcPM2fO1Go/evQozMzMci3WrAoICNB3CEUWx15/OPb6w7HXH469fnH89SejsXd0dISFhQWKFSsGQ8MC9fGU8jkhBCIjI3H37l0EBQVBCKFxe0xMTJ7HVOif4X5+fpg0aZL0v7rWf+vWrfNNGfqAgAC0atWK5UHzGMdefzj2+sOx1x+OvX5x/PUns7GPj4/H06dPUbp06Xz15XhhIITAu3fvYGlpWaSPgFlYWCAxMRGenp5QKBQat6lnx+WlApWAOTk5ISgoSKMtKCgIVlZWaR79AgCFQqE10IBqLmh+2gHnt3iKEo69/nDs9Ydjrz8ce/3i+OtPemOfnJwMmUwGQ0NDyOW8QlJOUiqVAFRT8Iry2BoYGEjPsdTPQX3sDwrUI9GgQQMcP35coy0gIAANGjTQU0RERERERES602sCFhUVhatXr+Lq1asAVGXmr169iqdPnwJQTR8cOHCg1P+TTz7Bo0eP8MUXX+DOnTtYsWIFduzYgYkTJ+ojfCIiIiIioizRawJ26dIl1KxZEzVr1gQATJo0CTVr1sS0adMAAK9evZKSMQBwd3fHgQMHEBAQgOrVq2PhwoX45ZdfWIKeiIiIiAo8Nzc3LFmyROf+p06dgkwmQ3h4eK7FRDlPr+eANWvWTKsSSUobNmxIc5krV67kYlREREREROnLrKDF9OnTMWPGjCyv9+LFi2mWSk9Pw4YN8erVK1hbW2d5W1lx6tQpNG/eHGFhYbCxscnVbRUFBaoIBxERERGRvr169Ur6e/v27Zg2bRru3r0rtVlYWEh/CyGQnJysU3l9BweHLMVhbGwMJyenLC1D+leginAQERERUeEmhEBMQpJefjKamZWSk5OT9GNtbQ2ZTCb9f+fOHVhaWuLQoUPw8vKCQqHAX3/9hYcPH6Jz587SNc/q1KmDY8eOaaw39RREmUyGX375BV27doWZmRnKlSuHffv2SbennoK4YcMG2NjY4MiRI6hUqRKsrKzQo0cPjYQxKSkJn376KWxsbGBnZ4cpU6Zg0KBB6NKlS7Yfs7CwMAwcOBC2trYwMzND27Ztcf/+fen2wMBAdOzYEba2tjA3N4enpycOHjwoLdu/f384ODjA1NQU5cqVw/r167MdS0HAI2BERERElG/EJiaj8rQjetn2rVm+MDPOmY/HX375JRYsWIAyZcrA1tYWz549Q7t27TBnzhwoFAps2rQJHTt2xN27d1G6dOl01zNz5kz88MMPmD9/PpYtW4b+/fsjMDAQxYoVS7N/TEwMFixYAH9/fwDARx99hM8//xxbt24FAHz//ffYsmUL1q9fj0qVKuHHH3/E3r170bx582zf18GDB+P+/fvYt28frKysMGXKFLRr1w63bt2CkZERxowZg4SEBPz5558wNzfHrVu3pKOE33zzDW7duoVDhw7B3t4eDx48QGxsbLZjKQiYgBERERER5bBZs2ahVatW0v/FihVD9erVpf9nz56N3377Dfv27cPYsWPTXc/gwYPRt29fAMDcuXOxdOlSXLhwAW3atEmzf2JiIn7++Wd4eHhAqVRi+PDhWLBggXT7smXL4Ofnh65duwIAli9fLh2Nyg514nXmzBk0bNgQALBlyxa4uLhg79696NmzJ54+fYru3bujatWqAIAyZcpIyz99+hQ1a9ZE7dq1AaiOAhZ2TMD0KegmZK9uwDwuHBBKfUdDREREpHemRga4NUs/Fa5NjQxybF3qhEItKioKM2bMwIEDB/Dq1SskJSUhNjZWo+J3WqpVqyb9bW5uDisrK7x58ybd/mZmZvDw8JD+d3JykvpHREQgKCgIdevWlW43MDCAl5eXdNHmrLp9+zYMDQ1Rr149qc3Ozg4VKlTA7du3AQCffvopRo0ahaNHj8LHxwfdu3eX7teoUaPQvXt3XL58Ga1bt0aXLl2kRK6wYgKmTzf2wPD0AvgAEI++BZyrAyVqAiVqqH7bugOZVNkhIiIiKkxkMlmOTQPUp9TVDCdPnoyAgAAsWLAAZcuWhampKXr06IGEhIQM12NkZKTxv0wmyzBZSqu/rue25Zbhw4fD19cXBw4cwNGjRzFv3jwsXLgQ48aNQ9u2bREYGIiDBw8iICAALVu2xJgxYzSO2hU2LMKhT5ZOUJaohWSZEWTxkcCT08DZpcCuocDSmsD3rsDGjkDANODGHiD0MaDnFxARERERZd2ZM2cwePBgdO3aFVWrVoWTkxOePHmSpzFYW1vD0dERFy9elNqSk5Nx+fLlbK+zUqVKSEpKwvnz56W2kJAQ3L17F5UrV5baXFxc8Mknn2DPnj347LPPsGbNGuk2BwcHDBo0CJs3b8aSJUuwevXqbMdTEBT8rxcKsrojkFxzMA4d2Ie2Xu4wCr4BvLwKvLwCBN0A4iKAx3+qftRMbFRHyJxr/HekzMaVR8qIiIiI8rFy5cphz5496NixI2QyGb755ptsT/v7EOPGjcO8efNQtmxZVKxYEcuWLUNYWFim1zYDgOvXr8PS0lL6XyaToXr16ujcuTNGjBiBVatWwdLSEl9++SVKliyJzp07AwAmTJiAtm3bonz58ggLC8PJkydRqVIlAMC0adPg5eUFT09PxMfH448//pBuK6yYgOUDQmYIOFUFXGoBtQaqGpMSgODbqoTs1dX3SdlNIC4ceHRK9aNmavs+IasJuDYEStcHFJapN0NEREREerJo0SIMHToUDRs2hL29PaZMmYLIyMg8j2PKlCl4/fo1Bg4cCAMDA4wcORK+vr4wMMj8/LemTZtq/G9gYICkpCSsX78e48ePR4cOHZCQkICmTZvi4MGD0nTI5ORkjBkzBs+fP4eVlRXatGmDxYsXA1Bdy8zPzw9PnjyBqakpmjRpgm3btuX8Hc9HZELfk0LzWGRkJKytrREREQErKyt9h4PExEQcPHgQ7dq105qzqyUpAXhz631CdvW/pEyZqNlPbgiUqAW4NwXcmwAu9QAj09y6CwVWlsaechTHXn849vrDsdcvjr/+ZDb2cXFxePz4Mdzd3WFiYqKHCAsvpVKJyMhIWFlZQS5P+8wjpVKJSpUqoVevXpg9e3YeR5g3MnqO6SM34BGwgsTQ+P20wxqA1/u2pHhVUvbyKvDiEvDkLyDsCfD8gurn9ALAwFiVhLk1USVlJb1U6yIiIiKiIiUwMBBHjx6Ft7c34uPjsXz5cjx+/Bj9+vXTd2hFBhOwgs5Q8b5yYk2g9hBVW1igqqCH+vyxd69U/z85DZyaCxiZqaYpujdV/ThVBwz4VCAiIiIq7ORyOTZs2IDJkydDCIEqVarg2LFjhf68q/yEn7oLI1tX1U/Nj1RVE0MeAo//p0rGnpwGYkKAhydUPwCgsAJcG/03ZbG4J5DOYWoiIiIiKrhcXFxw5swZfYdRpDEBK+xkMsC+rOqnzjBAqVQV93j8J/D4tGrKYnwEcO+Q6gcATIupErHGk1TTHYmIiIiIKEcwAStq5HLA0VP1U38UoEwGXl37b8pi4DkgNhS49Ttw9zDQaSlQvY++oyYiIiIiKhSYgBV1cgOgZC3VT6PxqkqLLy8Dfy0G7h0GfvsYePUv0GoWzxMjIiIiIvpAPNGHNBkaqwp09PkVaPq5qu3vn4At3YGYUP3GRkRERERUwDEBo7TJ5UCLr4GeGwEjc9WFn1c3U113jIiIiIiIsoUJGGXMswswPACwcQXCA4FfWqnODyMiIiIioixjAkaZc/QERp4C3L2BxGhgx0DgxBxVRUUiIiIiypZmzZphwoQJ0v9ubm5YsmRJhsvIZDLs3bv3g7edU+uhrGMCRroxKwZ8tAeoP0b1/58/ANv7A3GR+o2LiIiIKI917NgRbdq0SfO206dPQyaT4d9//83yei9evIiRI0d+aHgavvvuO9SqVUur/dWrV2jbtm2Obiu1DRs2wMbGJle3URAxASPdGRgCbeYCXX4GDBTA3YPALz7A2wf6joyIiIgozwwbNgwBAQF4/vy51m3r169H7dq1Ua1atSyv18HBAWZmZjkRYqacnJygUCjyZFukiQmYHt15HYlDN14jKBZIVgp9h6O7Gn2BoYcAyxLA27vAmhbA/WP6joqIiIgKAyGAhGj9/AjdPo916NABDg4O2LBhg0Z7VFQUdu7ciWHDhiEkJAR9+/ZFyZIlYWZmhqpVq+LXX3/NcL2ppyDev38fTZs2hYmJCSpXroyAgACtZaZMmYLy5cvDzMwMZcqUwTfffIPExEQAqiNQ33//Pa5duwaZTAaZTCbFnHoK4vXr19GiRQuYmprCzs4OI0eORFRUlHT74MGD0aVLFyxYsADOzs6ws7PDmDFjpG1lx9OnT9G5c2dYWFjAysoKvXr1QlBQkHT7tWvX0Lx5c1haWsLKygpeXl64dOkSACAwMBAdO3aEra0tzM3N4enpiYMHD2Y7lrzECzvp0R/XXmH5yQcADLHwxnGUc7RARScrVHSyREUnK1RwsoSDZT79ZqKkl+q8sB0DgGfngS09AJ/pQKMJgEym7+iIiIiooEqMAeaW0M+2v3oJGJtn2s3Q0BADBw7Ehg0bMHXqVMjef/bZuXMnkpOT0bdvX0RFRcHLywtTpkyBlZUVDhw4gAEDBsDDwwN169bNdBtKpRLdunWDo6Mjzp8/j4iICI3zxdQsLS2xYcMGlChRAtevX8eIESNgaWmJL774Ar1798aVK1dw8uRJHDum+rLc2tpaax3R0dHw9fVFgwYNcPHiRbx58wbDhw/H2LFjNZLMkydPwtnZGSdPnsSDBw/Qu3dv1KhRAyNGjMj0/qR1/9TJ1//+9z8kJSVhzJgx6N27N06dOgUA6N+/P2rWrImVK1fCwMAAV69ehZGREQBgzJgxSEhIwJ9//glzc3PcunULFhYWWY5DH5iA6VFxKwWqlbLCnZcRiE9S4saLSNx4oXlOlZ25MSo6W6KC4/vEzNkS5YpbwtTYQE9Rp2DpCAzaDxz8HLi8ETg2A3h9Hei0HDDOm8PnRERERPowdOhQzJ8/H//73//QrFkzAKrph927d4e1tTWsra0xefJkqf+4ceNw5MgR7NixQ6cE7NixY7hz5w6OHDmCEiVUCencuXO1ztv6+uuvpb/d3NwwefJkbNu2DV988QVMTU1hbm4OQ0NDODk5pbutrVu3Ii4uDps2bYK5uSoBXb58OTp27Ijvv/8ejo6OAABbW1ssX74cBgYGqFixItq3b4/jx49nKwE7fvw4rl+/jsePH8PFxQUAsGnTJnh6euLixYuoU6cOnj59is8//xwVK1YEAJQrV05a/unTp+jevTuqVq0KAChTpkyWY9AXJmB6NLCBG/rWLok/DhxE1frN8OBtDO68foe7r9/hzut3eBISjZDoBJx5EIIzD0Kk5WQywM3OHBWdLFHByVI6Yla6mBnk8jw++mSoADr+CDhXAw5NAW7sBt7eA/psBWxK520sREREVPAZmamOROlr2zqqWLEiGjZsiHXr1qFZs2Z48OABTp8+jVmzZgEAkpOTMXfuXOzYsQMvXrxAQkIC4uPjdT7H6/bt23BxcZGSLwBo0KCBVr/t27dj6dKlePjwIaKiopCUlAQrKyud74d6W9WrV5eSLwBo1KgRlEol7t69KyVgnp6eMDD47yCAs7Mzrl+/nqVtpdymi4uLlHwBQOXKlWFjY4Pbt2+jTp06mDRpEoYPHw5/f3/4+PigZ8+e8PDwAAB8+umnGDVqFI4ePQofHx907949W+fd6QMTsHxALgNc7cxQ1skabao4S+2xCcm4/+Yd7rxSJWR3Xkfi7ut3CIlOwOO30Xj8NhqHbryW+psZG6BVZUf0ru2C+mXs8i4Zk8mAOsMBh0qqEvWvr6su2txrE+DWOG9iICIiosJBJtNpGmB+MGzYMIwbNw4//fQT1q9fDw8PD3h7ewMA5s+fjx9//BFLlixB1apVYW5ujgkTJiAhISHHtn/u3Dn0798fM2fOhK+vL6ytrbFt2zYsXLgwx7aRknr6n5pMJoMyFy9LNGPGDPTr1w8HDhzAoUOHMH36dGzbtg1du3bF8OHD4evriwMHDuDo0aOYN28eFi5ciHHjxuVaPDmFCVg+ZmpsgGqlbFCtlI1Ge/C7eCkZUydm94OiEJOQjN+vvsTvV1+idDEz9KpdCj28XOBkbZI3Abs1Up0Xtr0/8OoasKkz0OY7VXLG88KIiIiokOnVqxfGjx+PrVu3YtOmTRg1apR0PtiZM2fQuXNnfPTRRwBU5zzdu3cPlStX1mndlSpVwrNnz/Dq1Ss4O6u+oP/77781+pw9exaurq6YOnWq1BYYGKjRx8jICMnJyZlua8OGDYiOjpaOgp05cwZyuRwVKlTQKd6sUt+/Z8+eSUfBbt26hfDwcI0xKl++PMqXL4+JEyeib9++WL9+Pbp27QoAcHFxwSeffIJPPvkEfn5+WLNmDRMwyh0Olgo4WDqgSTkHqS1ZKfDv83Ds/Oc59l99iaehMVhw9B4WBdxDswrF0au2C1pWKg4jg1wufGnjAgw5DOz/FLi+Ezg4WZWMtV+omq5IREREVEhYWFigd+/e8PPzQ2RkJAYPHizdVq5cOezatQtnz56Fra0tFi1ahKCgIJ0TMB8fH5QvXx6DBg3C/PnzERkZqZFoqbfx9OlTbNu2DXXq1MGBAwfw22+/afQpXbo0Hj9+jKtXr6JUqVKwtLTUKj/fv39/TJ8+HYMGDcKMGTMQHByMcePGYcCAAdL0w+xKTk7G1atXNdoUCgV8fHxQtWpV9O/fH0uWLEFSUhJGjx4Nb29v1K5dG7Gxsfj888/Ro0cPuLu74/nz57h48SK6d+8OAJgwYQLatm2L8uXLIywsDCdPnkSlSpU+KNa8wjL0hYSBXIaapW0xt2tVXJjqg4U9q6OuWzEoBXDizht8svkfNJh3HPMO3sbD4KjMV/ghjM2AbmuAVrMBmRy44g8ETMvdbRIRERHpwbBhwxAWFgZfX1+N87W+/vpr1KpVC76+vmjWrBmcnJzQpUsXndcrl8vx22+/ITY2FnXr1sXw4cMxZ84cjT6dOnXCxIkTMXbsWNSoUQNnz57FN998o9XH19cXzZs3h4ODQ5ql8M3MzHDkyBGEhoaiTp066NGjB1q2bInly5dnbTDSEBUVhZo1a2r8dOzYETKZDL///jtsbW3RtGlT+Pj4oEyZMti+fTsAwMDAACEhIRg4cCDKly+PXr16oW3btpg5cyYAVWI3ZswYVKpUCW3atEH58uWxYsWKD443L8iE0PGCB4VEZGQkrK2tERERkeUTFHNDYmIiDh48iHbt2mnNq80JD4OjsOPSM+z+5wXeRsVL7XXcbNGrtgvaV3OGmXEuHgj9dyewZzhgZg9MvgfI80H1xvdye+wpfRx7/eHY6w/HXr84/vqT2djHxcXh8ePHcHd3h4lJHp02UUQolUpERkbCysoKcnnRPe6S0XNMH7lB0X0kiggPBwv4ta2Ec34tsHqAF1pWLA65DLj4JAyf7/oXdecch9+e67j6LBy5kot7dgEU1kDMW+DFPzm/fiIiIiKiAoTngBURRgZytPZ0QmtPJ7yOiMPuy8+x49IzBIbE4NcLT/Hrhaeo6GSJXrVd0LVmSdiaG+fMhg2MgHI+qvL0dw8CLplf94KIiIiIqLDiEbAiyMnaBGOal8XJz5rh1xH10aVGCSgM5bjz+h1m/XEL9eYex9itl3E/6F3ObLBCO9Xvu4dyZn1ERERERAUUj4AVYXK5DA087NDAww4zYxLx+7UX2H7xGW6+jMQf/75CwK0gfNWuEgY2cJVKqmZL2ZaA3BAIvgOEPgKKFZwrlRMRERER5SQeASMAgLWZEQY2cMOBT5vgj3GN4V3eAfFJSkzfdxNDN1xE8Lv4zFeSHlNboPT7K7ffPZwzARMREVGhUsTqwlEeym/PLSZgpKVKSWtsGFIHMzpWhrGhHCfvBqPNkj9x4k5Q9lcqTUM8mDNBEhERUaGgrowYExOj50iosEpISACgKm2fH3AKIqVJJpNhcCN3NPCwx/htV3Dn9TsM3XAJAxu44qt2lWBilMUncIU2wBE/IPAsEBumOipGRERERZ6BgQFsbGzw5s0bAKprUn3QqQ8kUSqVSEhIQFxcXJEtQ69UKhEcHAwzMzMYGuaP1Cd/REH5VgUnS+wd0wg/HL6LdWceY9O5QJx7GIIf+9RE5RJZuFZCsTKAQ0XVeWAPjgNVe+Re0ERERFSgODk5AYCUhFHOEEIgNjYWpqamRTqplcvlKF26dL4ZAyZglCkTIwNM61gZ3hUcMHnnNdx/E4UuP53BF20qYGgjd8jlOj6ZK7RVJWB3DzIBIyIiIolMJoOzszOKFy+OxMREfYdTaCQmJuLPP/9E06ZNi/QFyI2NjfPVEUAmYKQz7/IOODy+Cabsvo5jt4Pw7YHb+N+9YCzoWR2OVjpcub5CO+CvxcD9Y0BSAmCYQ9caIyIiokLBwMAg35ynUxgYGBggKSkJJiYmRToBy2/yTypIBYKdhQJrBnrh2y5VYGIkx+n7b9FmyZ84cvN15guX9ALMHYD4CODp2dwPloiIiIgon2ECRlkmk8nwUX1X/DGuMSo7WyEsJhEf+/8Dvz3XEZOQlP6CcgOgnK/qb5ajJyIiIqIiiAkYZVvZ4pb4bUxDfNxUdWHlXy88RYelf+H684j0F6rQVvX77kEgn12TgYiIiIgotzEBow+iMDSAX7tK2DK8HhytFHj0NhrdVp7Bz/97CKUyjQTLozlgoADCA4E3t/M+YCIiIiIiPWICRjmiUVl7HB7fFG08nZCYLPDdoTvo/8t5vIqI1exobA6Uaab6mxdlJiIiIqIihgkY5Rhbc2Os/KgWvu9eFaZGBjj3KARtlpzGweuvNDtWaKP6fY/ngRERERFR0cIEjHKUTCZD7zqlceDTxqhWyhoRsYkYveUy1v31+L9O5d8nYM8vAe+C9BMoEREREZEeMAGjXFHGwQK7RzXEkEZuAIC1fz2GUBfdsCoBlKgJQAD3j+gtRiIiIiKivMYEjHKNkYEcX/hWhKmRAV6Ex+L6ixTVEcurqyFyGiIRERERFR1MwChXmRoboEXF4gCAg9dTXKxZXY7+4QkgMTaNJYmIiIiICh8mYJTr2lZ1AgAcuvHqv2mITlUBq1JAUizw6H96jI6IiIiIKO8wAaNc17xCcSgM5QgMicGtV5GqRplM86LMRERERERFABMwynXmCkM0q+AAADikMQ1RXY7+CKBU6iEyIiIiIqK8xQSM8kS7qs4AgIPXU0xDdGsCGFsAUa+BV1f0GB0RERERUd5gAkZ5okXF4jA2lOPR22jcDXqnajRUAGVbqv6+e0h/wRERERER5REmYJQnLE2M0LScahqiZjXEdqrfTMCIiIiIqAhgAkZ5pp26GuL1V/81lmsNyORA0A0g/KmeIiMiIiIiyhtMwCjPtKzkCCMDGe6/icJ99TREs2KAS33V37woMxEREREVckzAKM9YmxqhcVl7AMChG2lclJnl6ImIiIiokGMCRnmqbYpqiBJ1AvbkLyAuUg9RERERERHlDSZglKdaV3aEoVyGO6/f4VFwlKrRvhxgVxZQJgIPj+s3QCIiIiKiXMQEjPKUjZkxGnjYAUhvGiKrIRIRERFR4cUEjPKc+qLMh26knIb4vhz9vSNAcpIeoiIiIiIiyn1MwCjPta7sCLkMuPEiEk9DYlSNpeoCprZAXDjw7Lxe4yMiIiIiyi1MwCjP2VkoUL+Mehri+6NgBoZAOV/V36yGSERERESFFBMw0gupGmJ65eiF0ENURERERES5iwkY6YWvpyNkMuDas3A8D3s/DbFsS8DAGAh9BLy9r98AiYiIiIhyARMw0oviliao41YMAHBYfRRMYQm4NVb9fY/VEImIiIio8GECRnrTrooTgNTl6N9XQ2Q5eiIiIiIqhJiAkd60qaI6D+yfwDC8johTNZZvo/r97DwQHaKnyIiIiIiIcgcTMNIbJ2sT1Ha1BZCiGqKNC+BUFRBK4P5RPUZHRERERJTzmICRXqmrIR66nmIaYvkU1RCJiIiIiAoRJmCkV23enwd2MTAUbyLfT0NUl6N/cBxIjNNTZEREREREOY8JGOlVSRtT1HCxgRDAkZvvj4I51wAsnYHEaODJX3qNj4iIiIgoJzEBI71rV1V1FOygehqiXA6U91X9zXL0RERERFSIMAEjvWv7vhri+ccheBsVr2pMWY5eCD1FRkRERESUs5iAkd65FDND1ZLWUArg6M0gVaN7U8DIDIh8Abz+V78BEhERERHlEL0nYD/99BPc3NxgYmKCevXq4cKFCxn2X7JkCSpUqABTU1O4uLhg4sSJiItjoYaCrm1V9UWZ35ejNzIFPFqo/uZFmYmIiIiokNBrArZ9+3ZMmjQJ06dPx+XLl1G9enX4+vrizZs3afbfunUrvvzyS0yfPh23b9/G2rVrsX37dnz11Vd5HDnlNPU0xLMPQxAWnaBqVF+UmQkYERERERUSek3AFi1ahBEjRmDIkCGoXLkyfv75Z5iZmWHdunVp9j979iwaNWqEfv36wc3NDa1bt0bfvn0zPWpG+Z+7vTkqOVshWSkQcOv9NMTyvgBkwKurQMQLfYZHRERERJQjDPW14YSEBPzzzz/w8/OT2uRyOXx8fHDu3Lk0l2nYsCE2b96MCxcuoG7dunj06BEOHjyIAQMGpLud+Ph4xMfHS/9HRkYCABITE5GYmJhD9yb71DHkh1j0zbdycdx+FYkD/75E1xpOgMIWBiVrQ/7iIpJvH4DSa0iObo9jrz8ce/3h2OsPx16/OP76w7HXH4595vQxNjIh9FNi7uXLlyhZsiTOnj2LBg0aSO1ffPEF/ve//+H8+fNpLrd06VJMnjwZQggkJSXhk08+wcqVK9PdzowZMzBz5kyt9q1bt8LMzOzD7wjlmKBYYO5VQxjIBL6tnQwzQ6Dc6/2o/GonXltVx3mPz/QdIhEREREVIjExMejXrx8iIiJgZWWVJ9vU2xGw7Dh16hTmzp2LFStWoF69enjw4AHGjx+P2bNn45tvvklzGT8/P0yaNEn6PzIyEi4uLmjdunWeDXJGEhMTERAQgFatWsHIyEjf4ejdzpdncP9NNAxcaqBdzRJAcBlg9U44Rt9BO5+mgLFFjm2LY68/HHv94djrD8devzj++sOx1x+OfebUs+Pykt4SMHt7exgYGCAoKEijPSgoCE5OTmku880332DAgAEYPnw4AKBq1aqIjo7GyJEjMXXqVMjl2qe0KRQKKBQKrXYjI6N89UTMb/HoS7uqJfDj8fs4evsNetV1BZyrALZukIU9gdHTv4BKHXN8mxx7/eHY6w/HXn849vrF8dcfjr3+cOzTp49x0VsRDmNjY3h5eeH48eNSm1KpxPHjxzWmJKYUExOjlWQZGBgAAPQ0k5JyWLuqqmqIf957i3dxiYBMpnlRZiIiIiKiAkyvVRAnTZqENWvWYOPGjbh9+zZGjRqF6OhoDBmiKrYwcOBAjSIdHTt2xMqVK7Ft2zY8fvwYAQEB+Oabb9CxY0cpEaOCrbyjBTwczJGQrMSJO+8vR6AuR3/vCKBM1l9wREREREQfSK/ngPXu3RvBwcGYNm0aXr9+jRo1auDw4cNwdHQEADx9+lTjiNfXX38NmUyGr7/+Gi9evICDgwM6duyIOXPm6OsuUA6TyWRoV9UZy048wMHrr9C5RknAtSGgsAZi3gLPLwGl6+k7TCIiIiKibNF7EY6xY8di7Nixad526tQpjf8NDQ0xffp0TJ8+PQ8iI31pW0WVgJ26G4zo+CSYK4yAcq2AG7uAuweZgBERERFRgaXXKYhEaankbAk3OzPEJ6WYhlihrer3vcP6C4yIiIiI6AMxAaN8RyaToe37YhyHbrxSNZb1AeSGQPAdIOShHqMjIiIiIso+JmCUL7WrokrATt4JRkxCEmBqozoXDOBRMCIiIiIqsJiAUb5UpaQVStmaIjYxGf+7G6xqLP9+GuLDE/oLjIiIiIjoAzABo3xJXQ0RAA7eeK1qLFFD9Tv4nn6CIiIiIiL6QEzAKN9qW8UJAHDidhDiEpMB+/KqGyKeAgkxeoyMiIiIiCh7mIBRvlXDxQYlrE0QnZCMP+8FA2Z2gKmt6saQB/oNjoiIiIgoG5iAUb4lk8nQpoq6GuJrQCb77yjYW05DJCIiIqKChwkY5WvtqqqmIR67FYT4pGTAvpzqhrf39RgVEREREVH2MAGjfK1WaVsUt1TgXXwSzjx4yyNgRERERFSgMQGjfE0ul0nFOA5ef50iAeMRMCIiIiIqeJiAUb7X9n05+qM3XyPBtqyqMeQ+oFTqMSoiIiIioqxjAkb5Xh23YrC3UCAyLgnnQi0AuRGQFAdEPNN3aEREREREWcIEjPI9A7kMbao4AgAO3QwG7DxUN3AaIhEREREVMEzAqEBo974c/ZGbr6G0U1dCZCEOIiIiIipYmIBRgVDXvRiKmRsjLCYRLwxLqRpDeASMiIiIiAoWJmBUIBgayOHrqZqG+HeEnaqRUxCJiIiIqIBhAkYFRtNyDgCAc5HqBIxTEImIiIioYGECRgVGGQcLAMDZCFtVQ1QQEBuuv4CIiIiIiLKICRgVGK52ZgCA13HGUFqoLs6MkAd6jIiIiIiIKGuYgFGBYWJkgBLWJgCAKAt3VSOnIRIRERFRAcIEjAoUN3tzAECworSqgQkYERERERUgTMCoQFEnYE9k70vRsxIiERERERUgTMCoQHG3UyVgtxJVJel5BIyIiIiIChImYFSgqI+A/RNlr2oIfQQkJ+oxIiIiIiIi3TEBowLF3V5VCfFSmCmEkRmgTALCnug3KCIiIiIiHTEBowLFpZgZ5DIgKkEgydZD1chpiERERERUQDABowJFYWiAEjamAIBIc5aiJyIiIqKChQkYFTju788De2XkompgJUQiIiIiKiCYgFGB4/a+EuJDUULVwCNgRERERFRAMAGjAkddCfF6XHFVw9v7gBB6jIiIiIiISDdMwKjAUVdCvBBpC0AGxIUD0W/1GhMRERERkS6YgFGBo56CeD80GcKmtKqR0xCJiIiIqABgAkYFjksxMxjIZYhNTEaCTVlVIxMwIiIiIioAmIBRgWNkIEcpW1Up+hBTV1UjKyESERERUQHABIwKJPU0xOcGpVQNPAJGRERERAUAEzAqkNTXAruX5KxqYAJGRERERAUAEzAqkNzsVJUQr8Q6qBrCnwKJsXqMiIiIiIgoc0zAqECSrgUWZgSYWAMQQMhD/QZFRERERJQJJmBUIKmnIAaGxkLYlVc1choiEREREeVzTMCoQCppYwpDuQzxSUrEWJdRNbISIhERERHlc0zAqEAyNJCjdDHVeWDBCnUpeh4BIyIiIqL8jQkYFVjq88CeoKSqgQkYEREREeVzTMCowFJfC+xWoqOqIeQBoFTqMSIiIiIioowxAaMCy91eNQXxWpQNIDcEEmOAyBf6DYqIiIiIKANMwKjAUk9BfBASDxR7X4gjhIU4iIiIiCj/YgJGBZZ6CuKz0FgIu3KqRlZCJCIiIqJ8jAkYFVglbExhbCBHQrIS7yzcVY0sxEFERERE+RgTMCqwDOQylLZTnQf2yqi0qpEJGBERERHlY0zAqEBTT0N8KJxVDZyCSERERET5GBMwKtDUlRD/jSuuanj3CoiL1GNERERERETpYwJGBZq6EuLdcDlgob4eGI+CEREREVH+xASMCjT391MQn4TEAPblVY2chkhERERE+RQTMCrQ1EfAnoXGQGlXVtXIQhxERERElE8xAaMCzcnKBApDOZKUAmFmLEVPRERERPkbEzAq0ORymVQJ8blBKVUjpyASERERUT7FBIwKPLf3lRDvJamLcDwEkpP0GBERERERUdqYgFGBpz4P7GaUFWBoAigTgfBAPUdFRERERKSNCRgVeOpKiI9C4wC7cqpGngdGRERERPkQEzAq8NRHwJ68jQbsmYARERERUf7FBIwKPPf3CdjzsBgkF2MCRkRERET5FxMwKvCKWypgZmwApQDemrqqGt8+0G9QRERERERpYAJGBZ5MJoPr+/PAnqCEqpFHwIiIiIgoH2ICRoWC+/tS9LcSiqsaYkOB6BA9RkREREREpI0JGBUK6osxPwxXAtalVY08CkZERERE+QwTMCoU/quEGMNKiERERESUbzEBo0JBXQnx8dtowL68qpEJGBERERHlM0zAqFBQT0F8GRGLRNuyqsa39/UYERERERGRNiZgVCjYWxjDQmEIIYA3ChdVI4+AEREREVE+wwSMCgWZTAa395UQHyhLqhrDA4HEOD1GRURERESkiQkYFRrqaYj3okwBhTUglEDoIz1HRURERET0HyZgVGhIhThCWQmRiIiIiPInJmBUaKiPgD3RqITIQhxERERElH8wAaNC479rgUXzCBgRERER5UtMwKjQUE9BfBkRhwQbD1UjEzAiIiIiykeYgFGhYWtmBCsTQwDAC8P3pehDHgBC6DEqIiIiIqL/MAGjQkMmk0lHwe4l2gMyAyAhCnj3Ss+RERERERGpMAGjQkV9HtjjsESgmLuqkdMQiYiIiCifYAJGhQorIRIRERFRfqb3BOynn36Cm5sbTExMUK9ePVy4cCHD/uHh4RgzZgycnZ2hUChQvnx5HDx4MI+ipfxOuhYYKyESERERUT5kqM+Nb9++HZMmTcLPP/+MevXqYcmSJfD19cXdu3dRvHhxrf4JCQlo1aoVihcvjl27dqFkyZIIDAyEjY1N3gdP+ZJUij4kGqirPgLGBIyIiIiI8ge9JmCLFi3CiBEjMGTIEADAzz//jAMHDmDdunX48ssvtfqvW7cOoaGhOHv2LIyMjAAAbm5ueRky5XPu76cgBkXGI87aAyYApyASERERUb6htwQsISEB//zzD/z8/KQ2uVwOHx8fnDt3Ls1l9u3bhwYNGmDMmDH4/fff4eDggH79+mHKlCkwMDBIc5n4+HjEx8dL/0dGRgIAEhMTkZiYmIP3KHvUMeSHWAoDMyNVOfqwmEQ8THaEJwBEvkBiVCigsNToy7HXH469/nDs9Ydjr18cf/3h2OsPxz5z+hgbvSVgb9++RXJyMhwdHTXaHR0dcefOnTSXefToEU6cOIH+/fvj4MGDePDgAUaPHo3ExERMnz49zWXmzZuHmTNnarUfPXoUZmZmH35HckhAQIC+Qyg0rOUGCIMMu07fgIehFUySInFm/yZEmLmn2Z9jrz8ce/3h2OsPx16/OP76w7HXH459+mJiYvJ8m3qdgphVSqUSxYsXx+rVq2FgYAAvLy+8ePEC8+fPTzcB8/Pzw6RJk6T/IyMj4eLigtatW8PKyiqvQk9XYmIiAgIC0KpVK2laJX2YkzHX8eTaKxQrXQHG8ASenkPjisUhqrTT6Mex1x+Ovf5w7PWHY69fHH/94djrD8c+c+rZcXlJbwmYvb09DAwMEBQUpNEeFBQEJyenNJdxdnaGkZGRxnTDSpUq4fXr10hISICxsbHWMgqFAgqFQqvdyMgoXz0R81s8BVmZ4pYAXuFpWBzkDhWAp+dgGPYISGd8Ofb6w7HXH469/nDs9Yvjrz8ce/3h2KdPH+OitzL0xsbG8PLywvHjx6U2pVKJ48ePo0GDBmku06hRIzx48ABKpVJqu3fvHpydndNMvqho0qiEaM9KiERERESUf+j1OmCTJk3CmjVrsHHjRty+fRujRo1CdHS0VBVx4MCBGkU6Ro0ahdDQUIwfPx737t3DgQMHMHfuXIwZM0Zfd4HyIXUlxMdvY3gxZiIiIiLKV/R6Dljv3r0RHByMadOm4fXr16hRowYOHz4sFeZ4+vQp5PL/ckQXFxccOXIEEydORLVq1VCyZEmMHz8eU6ZM0dddoHzIzV5VXOVtVDyiLCvAAgBCHgDKZECedrVMIiIiIqK8oPciHGPHjsXYsWPTvO3UqVNabQ0aNMDff/+dy1FRQWZpYgR7C2O8jUrAk8RiqGJoAiTFAeGBQLEy+g6PiIiIiIowvU5BJMotbuppiKFxgF1ZVSOnIRIRERGRnjEBo0JJKsTxNhqwL6dqZCEOIiIiItIzJmBUKLm/T8Aeh0QDduoEjEfAiIiIiEi/mIBRoaSegqg6AsZKiERERESUPzABo0JJXQnxSUgMpyASERERUb7BBIwKJfURsNDoBESYu6oaY94CMaF6jIqIiIiIijomYFQomSsMUdxSAQB4EikDrEqpbuA0RCIiIiLSIyZgVGhJlRBDWAmRiIiIiPIHJmBUaLmrrwWmUYiDCRgRERER6Q8TMCq00r4WGKcgEhEREZH+MAGjQsv9fSXExyExPAJGRERERPkCEzAqtDSPgL1PwMKeAEnx+guKiIiIiIo0JmBUaLkWUyVgEbGJCJMXA4wtAZEMhD7Wc2REREREVFQxAaNCy9TYAM7WJgCAx6G8IDMRERER6R8TMCrU1BdkfsJKiERERESUDzABo0KNlRCJiIiIKD9hAkaFGishEhEREVF+wgSMCrU0pyCGPACE0GNURERERFRUMQGjQs09xRREYesGyAyA+EggKki/gRERERFRkcQEjAo1l2JmkMmAd/FJCImXAbZuqhs4DZGIiIiI9IAJGBVqJkYGKGFtCiB1IQ4mYERERESU95iAUaGnnob4mJUQiYiIiEjPmIBRoef2vhLikxBeC4yIiIiI9IsJGBV6/1VCTFmKnkfAiIiIiCjvMQGjQk9zCuL7BCziGZAQrceoiIiIiKgoYgJGhZ6buhR9SDSEqS1gZqe6IfShHqMiIiIioqIoWwnYs2fP8Pz5c+n/CxcuYMKECVi9enWOBUaUU1xszSCXATEJyQh+Fy8dBZOFcBoiEREREeWtbCVg/fr1w8mTJwEAr1+/RqtWrXDhwgVMnToVs2bNytEAiT6UsaEcpWxVhThSVkKU8TwwIiIiIspj2UrAbty4gbp16wIAduzYgSpVquDs2bPYsmULNmzYkJPxEeWIlNMQeQSMiIiIiPQlWwlYYmIiFAoFAODYsWPo1KkTAKBixYp49epVzkVHlEPc7dRHwGJSJGAP9BkSERERERVB2UrAPD098fPPP+P06dMICAhAmzZtAAAvX76EnZ1djgZIlBOkI2ApL8Yc+hAQSj1GRURERERFTbYSsO+//x6rVq1Cs2bN0LdvX1SvXh0AsG/fPmlqIlF+ojEF0cYVMDCGLCkOZgkheo6MiIiIiIoSw+ws1KxZM7x9+xaRkZGwtbWV2keOHAkzM7McC44op7jb/ZeAKSGH3K4s8OYWLOJe6jkyIiIiIipKsnUELDY2FvHx8VLyFRgYiCVLluDu3bsoXrx4jgZIlBNK2ZrCUC5DXKISQe/ipGmIFvE8Z5GIiIiI8k62ErDOnTtj06ZNAIDw8HDUq1cPCxcuRJcuXbBy5cocDZAoJxgayOFSLGUpelUhDos4JmBERERElHeylYBdvnwZTZo0AQDs2rULjo6OCAwMxKZNm7B06dIcDZAop7i9r4T45G0MYF8BAFAs+gEghD7DIiIiIqIiJFsJWExMDCwtLQEAR48eRbdu3SCXy1G/fn0EBgbmaIBEOUWjEEfZlhAGCljHPYPs5RU9R0ZERERERUW2ErCyZcti7969ePbsGY4cOYLWrVsDAN68eQMrK6scDZAop7i/T8Aev40GzIpBVO4CAJBf3qC/oIiIiIioSMlWAjZt2jRMnjwZbm5uqFu3Lho0aABAdTSsZs2aORogUU5xs0txLTAAylqDAQCyW78BsWH6CouIiIiIipBsJWA9evTA06dPcenSJRw5ckRqb9myJRYvXpxjwRHlJPURsMDQGCiVAqJkbUSYloYsKRa4+queoyMiIiKioiBbCRgAODk5oWbNmnj58iWeP38OAKhbty4qVqyYY8ER5aQSNqYwNpAjIUmJlxGxgEyGJ/YtVDdeWsdiHERERESU67KVgCmVSsyaNQvW1tZwdXWFq6srbGxsMHv2bCiVypyOkShHGMhlcClmCuB9JUQAz20bQBibAyH3gSen9RkeERERERUB2UrApk6diuXLl+O7777DlStXcOXKFcydOxfLli3DN998k9MxEuUYqRBHiOo8sCQDUyir9FTdeGmdvsIiIiIioiLCMDsLbdy4Eb/88gs6deoktVWrVg0lS5bE6NGjMWfOnBwLkCgnpS7EAQDKWkNgcHkDcHs/8C4IsHTUU3REREREVNhl6whYaGhomud6VaxYEaGhoR8cFFFuka4FliIBg6Mn4FIPUCYBVzbpKTIiIiIiKgqylYBVr14dy5cv12pfvnw5qlWr9sFBEeWW1FMQJbWHqn7/sxFQJudxVERERERUVGRrCuIPP/yA9u3b49ixY9I1wM6dO4dnz57h4MGDORogUU5SHwF7FhqDpOQUBWMqdwEOfwlEPAPuBwAV2ugnQCIiIiIq1LJ1BMzb2xv37t1D165dER4ejvDwcHTr1g03b96Ev79/TsdIlGOcrUygMJQjMVngZUTcfzcYmQA1+qv+ZjEOIiIiIsol2ToCBgAlSpTQKrZx7do1rF27FqtXr/7gwIhyg1wug6udGe4FRSEwJEbzxtpDgXPLgftHgbBAwNZVP0ESERERUaGV7QsxExVUUiXE1AmYnQdQphkAAfyzIa/DIiIiIqIigAkYFTnqQhxaCRgA1B6m+n3FH0hKyMOoiIiIiKgoYAJGRY66EIfWFEQAqNAWsHQGooOBO/vzODIiIiIiKuyydA5Yt27dMrw9PDz8Q2IhyhMaUxDtU91oYATUGgj873vg0nqgSve8D5CIiIiICq0sJWDW1taZ3j5w4MAPCogot6mnID4Pj0XKSvSSWoOAP+cDT04DwXcBhwp5GyARERERFVpZSsDWr1+fW3EQ5RlHKwVMjQwQm5iMkPg0OliXBMq3Be4eUJWkb/t9nsdIRERERIUTzwGjIkcmU5WiB4DgOFnaneoMVf2++iuQkMa5YkRERERE2cAEjIok9TTE4Lh0OpRpAdi6AfERwI3deRYXERERERVuTMCoSFJXQnwbm84RMLkc8Bqi+vvSujyKioiIiIgKOyZgVCS5v6+E+Do2g041PwIMjIGXl4GXV/ImMCIiIiIq1JiAUZHk5WYLAHj0ToaI2MS0O5nbA5U7q/6+uDaPIiMiIiKiwowJGBVJHg4WqOBogWQhw7Hbb9LvWHuY6veN3UBseJ7ERkRERESFFxMwKrLaeDoCAA7dDEq/U+n6gEMlIDEG+Hd7HkVGRERERIUVEzAqstQJ2NmHIelPQ5TJgDrvj4JdWgcIkUfR5YCkeOD3McCFNfqOhIiIiIjeYwJGRVbZ4hZwMhVITBYIuJXBUbBqvQAjMyD4DhB4Nu8C/FC3fgeubAYO+wHvMrh/RERERJRnmIBRkVbDTgkAOHT9VfqdTKyBqj1Vf18qQMU4/t2h+q1MBC5v0m8sRERERASACRgVcTXtVFMKT99/i8i4dKYhAv9NQ7y1D4gKzoPIPlD0W+Dhif/+/2c9kJykv3iIiIiICAATMCrinMyAsg7mSEhW4lhG0xCdqwMlvVRHk674512A2XXzN0AkA45VADM7IPIFcO+QvqMiIiIiKvKYgFGR17aKqhjHwYymIQL/laT/Zz2gVOZyVB/o+i7V7+p9gVoDVX9f/EV/8RARERERACZgRFI1xD/vZTIN0bOr6nyw8KfAw+N5FF02hAUCz/4GIAOqdAe8hqj+fnQKeHtfz8ERERERFW1MwKjIK1fcAh7vpyEev53BNERjM6BGf9Xfl9blTXDZceP90S/3JoCVM2DrCpT3VbVdLEBFRIiIiIgKISZgVOTJZDK0r+oMADh4/XXGnWsPVf2+dxgIf5bLkWWTevqhunIjANQZofp9dSuQEJ33MRERERERACZgRACAdtVUCdj/7gXjXUbTEO3LAW5NAKEELm/Mo+iyIOgm8OYWYGAMVOr0X7tHC8DWHYiPAK7v1F98REREREUcEzAiABUcLVHGwRwJSUqcuPMm487qkvSXNwHJGSRr+qC+9le51oCpzX/tcvl/cV/8BRAiz0MjIiIiIiZgRAA0pyEe+DeTaogV2gPmxYGoIODOgTyITkdKJXBjt+rvlNMP1Wr0BwxNgNfXgecX8zY2IiIiIgLABIxI0raKKgE7dS8YUfEZXLTY0Pi/0u75qRjHs/NAxDPA2PK/ohspmRVTVUUEgAtr8jY2IiIiIgLABIxIUsnZEu72qmmIGVZDBACvQQBkwOP/AW8f5El8mVKf21WpI2BkmnafOsNVv2/tBaKC8yQsIiIiIvoPEzCi92QyGdpVdQIAHMqsGqJN6f+OMuWHo2DJicDN31R/V0tj+qFayVpAiVpAcgJwxT9vYiMiIiIiCRMwohTavT8P7OTdN4jOaBoiANR+X9Ti6hYgMTaXI8vEwxNAbKjq3DS3phn3VR8Fu7QeUCbnfmxEREREJMkXCdhPP/0ENzc3mJiYoF69erhw4YJOy23btg0ymQxdunTJ3QCpyKjsbAU3OzPE61INsWxLwLo0EBf+39EnfVFPP6zSDTAwzLhvlW6AqS0Q8RS4fzT3YyMiIiIiid4TsO3bt2PSpEmYPn06Ll++jOrVq8PX1xdv3mT84ffJkyeYPHkymjRpkkeRUlGgmoaovihzJtUQ5QZA7cGqv/U5DTE+6r9qjGlVP0zNyBSo+ZHq74u/5F5cRERERKRF7wnYokWLMGLECAwZMgSVK1fGzz//DDMzM6xbl/4H2uTkZPTv3x8zZ85EmTJl8jBaKgpSTkOMSchkGmLNAYDcSFXW/dW/eRBdGu4eAhJjVBdaLuml2zK1hwKQAQ+OASEPczU8IiIiIvpPJnOVcldCQgL++ecf+Pn5SW1yuRw+Pj44d+5cusvNmjULxYsXx7Bhw3D69OkMtxEfH4/4+Hjp/8jISABAYmIiEhP1fxFddQz5IZaiJr2xL+9gChdbUzwLi0XAjVdSYY40KWxhULE95Lf2Qnl+NZLbL87NkNNk8O92yAEke3aDMimThFHN0gUGHi0hf3gMyRd+gdJnVq7GmBqf9/rDsdcfjr1+cfz1h2OvPxz7zOljbPSagL19+xbJyclwdHTUaHd0dMSdO3fSXOavv/7C2rVrcfXqVZ22MW/ePMycOVOr/ejRozAzM8tyzLklICBA3yEUWWmNfXlTOZ6FybH+2FXgmTLD5e0SK6Mx9gLXtuJUQnXEKIrnTqBpME56B98HJwAAp0IcEHXwoM7LOoqqqI9jSL60EUfiakEpN86tMNPF573+cOz1h2OvXxx//eHY6w/HPn0xMTF5vk29JmBZ9e7dOwwYMABr1qyBvb29Tsv4+flh0qRJ0v+RkZFwcXFB69atYWVllVuh6iwxMREBAQFo1aoVjIyM9B1OkZLR2Jd+EYnjP/+Nu+8M0cynGcyMM3qptINy61nIH59CS9lZJLdbnbuBpyD/Zx3k15MhnKqhabfhWVtY6QuxYheMI56hrUsMRPUuuRJjWvi81x+Ovf5w7PWL468/HHv94dhnTj07Li/pNQGzt7eHgYEBgoI0L3obFBSE/7d33/FtVecfxz+SvPceGXb23nuwEzKAQJhhh5RCGaHQtIUfFAiUtoxSymzYm7BJWiCEDEiA7L33chLHdhzvPaTfH8cjJtuWdT2+79dLL11dXek+Prl29Oic85y4uGOHfe3atYu9e/cybty4qn1Op+md8PLyYtu2bbRv377Ga3x9ffH19T3mvby9vRvUhdjQ4mlOjtf2fRIjaB3hz/6MQhbtzqqaF3ZCo/4Kr52DfdNX2If/Hlr0rceIj7LZVF+09bqmFtePNwy8FeY9htfqd2DARPfHd6oIdN1bRm1vHbW9tdT+1lHbW0dtf2JWtIulRTh8fHzo378/8+fPr9rndDqZP38+Q4cOPeb4Ll26sGHDBtauXVt1u/TSSzn//PNZu3YtrVu39mT40oQdXQ3x21NVQwSI7w09rzHbc6eCy1WP0VXISoKkJYANul9Ru/foexM4fCB5DRxY5dbwRERERORYlldBnDJlCm+88QbvvfceW7Zs4c477yQ/P59JkyYBcPPNN1cV6fDz86NHjx41bmFhYQQHB9OjRw98fDw/h0Warot6mATshy1pFJacxoLFFzxskpk9C2HX/FMfX1cbvzT3bc6C0Ja1e4/AKOh+udlWSXoRERGRemd5AjZhwgSeffZZHn30Ufr06cPatWuZPXt2VWGOpKQkDh06jR4IETfr1SqUVuH+FJaWs2DbKRZlBghPhIG3me25U8F5GklbXayvWHz5dNb+OpnKmDd+CQUZdXsvERERETkpyxMwgMmTJ7Nv3z6Ki4tZtmwZgwcPrnpuwYIFvPvuuyd87bvvvsvMmTPrP0hpds54GCLAOX8C31BI3QjrP6u/4FI3QdomswZZt0vr9l6tBkBcLygvhjUfuCc+ERERETmuBpGAiTRUlQnYD1vTKCo9jR6tgAg46z6z/ePfobSofgLb8IW57zgK/MPr9l42GwysqKC44i1wnrzsvoiIiIjUnhIwkZPo3SqUlmH+FJSUs2Db4dN70ZA7IbgFZO+H5fVQkt7prE7Ael7lnvfsebXpucva55n5ayIiIiLNlBIwkZMwwxDNkgizTncYorc/nP+Q2f75X1CY6d6gDiyH7CTwCYLOY93znj4B0PcGs738Dfe8p4iIiIgcQwmYyCmMrRiGOH9L6ukNQwTocz1Ed4WiLPj5OfcGtKGi+EbXcSbZc5cBt5r7HXMgc6/73ldEREREqigBEzmFvq3DaBHqR35JOQu3n+YwRLsDLnzcbC97DbL2uyeY8lLYZBZfdtvww0pRHaDd+YALVr7j3vcWEREREUAJmMgp2Wy2ql6w0x6GCKZARuJZprrgj/9wTzC7foSCIxAYDW3Pc897Hq2yGMfq9+uvgIiIiIhIM6YETOQ0XFQ1DPE0qyGCqS544V/N9rqPIWVj3QOpHH7Y/QpweNX9/X6t0xgIaQWFGbB5pvvfX0RERKSZUwImchr6tg4jPtSPvOIyft6RfvovbNUfuo0HXDDvsboFUZIPW78123VdfPlEHF4w4BazrWIcIiIiIm6nBEzkNNjtNsb2qMUwRIARj4LdC3bOhT0/1T6Ibd9BaT6EtzGLJ9eXvjebBZ4ProTkNfV3HhEREZFmSAmYyGmqLEc/b3MqxWWnOQwRILI99J9ktuc+WvuFjiuHH/a82gxvrC/BsdDtUrO94q36O4+IiIhIM6QETOQ09UsIJy7Ej9ziMn7efgbDEAHOfcCs25W8BjbPOPOTF2TAznlmu76GHx5t4G3mfsMX7l/HTERERKQZUwImcprsdhtjepzhosyVgqJh2O/N9vy/QlnJmb1+80xwlkFcT4jufGavrY2EIRDTHcoKYe30+j+fiIiISDOhBEzkDFzcy8wDm3umwxABht4NgTFmkeNVZ7jO1oYvzL0ner/ADHEcWLEw84q3aj9sUkRERERqUAImcgb6J4QTG+JLbnEZi3ae4TBE3yA47//M9sKnoSjn9F6XtR/2LQJs0MPNiy+fTK8J4BMMGbtg94+eO6+IiIhIE6YETOQMHF0N8dv1KWf+Bv1uhsgOZjHlxS+e3ms2fmnuE4dDaMszP2dt+QZBn+vMtopxiIiIiLiFEjCRMzS2Yh7Y3M0plJSd4dA8hzeMmGq2l7wCuaeRxFUNP/Rg71elARXDELd/Z3riRERERKROlICJnKEBbSKIDvYlp6gWwxABuo6DVoOgtAAWPHnyY9O2QOoGsy5Xt8tqF3BdxHSBNmeDy3nm89ZERERE5BhKwETOkMNuq+oF+/ZMqyGCKXBx4V/N9uoP4PD2Ex9bufZXxwshIOLMz+UOA39r7le/D2XF1sQgIiIi0kQoAROphYt6mnlgczbVYhgiQOJQ6HwxuMph/uPHP8blOmrxZQuGH1bqcjEEx0P+YdjytXVxiIiIiDQBSsBEamFgmwiigiqGIe6qxTBEgJFTwWaHrd9A0tJjnz+wArKSwDsQOo2tW8B14fCG/reY7eWvWxeHiIiISBOgBEykFo4ehvhdbYYhgllQue9NZnvOI6bH62jrPzP3XS8Bn4BaRuom/SaaeWj7l0HSMmtjEREREWnElICJ1NLYniYBm7M5ldLyWi5UfN6D4OUPB5abnrBK5aWwaYbZ7nlNHSN1g5B46D3BbC963tJQRERERBozJWAitTS4bSRRQT5kFZSyeNeR2r1JSDwMvdtsz3scysvM9u6FUJAOAVHQ7lz3BFxXw+4FbLBtFqRttToaERERkUZJCZhILTnsNkZ3N71gs9bXchgiwPB7ISASjuyANe+bfZXFN7pfbuZgNQTRnUxBDoBFL1gbi4iIiEgjpQRMpA4urqiG+P3mlNoPQ/QLgXPuN9sLnoL89OrhiD2vdkOUbnTWH8z9hs+0MLOIiIhILSgBE6mDQW0jiAw0wxCX1HYYIsCA30B4G8hLhenXQEkehCVA60Fui9UtWg0wCzM7y2Dpf6yORkRERKTRUQImUgdeDjujKxdlrsswRC8fuOARs31wlbnvebVZtLmhOes+c7/qXSjIsDISERERkUZHCZhIHY3r1QKAGWsPknSkoPZv1P0KiO9T/bihDT+s1H4ExPWE0gKtCyYiIiJyhpSAidTRkHYRnNUhipIyJ/+YtaX2b2S3w6i/mcWZWw2CmK7uC9KdbDYYfp/ZXvYalORbGo6IiIhIY6IETKSObDYbj1zSDbsNZm9KYfGu9Nq/Wduz4a5lcMNn7guwPnQbb+asFWbA6g+sjkZERESk0VACJuIGneOCuXFIIgB//Xoz5U5X7d8suhP4h7spsnri8IJhvzfbS142C0eLiIiIyCkpARNxkz+M7ESovzdbU3L5ZEWS1eHUvz7XQ2A0ZO+HjV9aHY2IiIhIo6AETMRNwgN9uG9kRwD+NWc72YVNvFfI2x+G3Gm2f3kenLVcB01ERESkGVECJuJGNw5JpENMEBn5Jbw4f4fV4dS/AbeCTzAc3gI75lgdjYiIiEiDpwRMxI28HXYeuaQbAO8t3suuw3kWR1TP/MNgwCSz/cu/LQ1FLLDrB1j0ApSXWR2JiIhIo6EETMTNzu0UzQVdYihzuvj7t3UoS99YDLkLHD6wfynsW+L58+ccgrzDnj9vc5dzCD65AeY+CnMetjoaERGRRkMJmEg9ePjirnjZbfywNY0F29KsDqd+hcRD72vN9qLnPXvulA3wUn+YNgwKMz177uZu/l/NYtwAy6bBmg+tjUdERKSRUAImUg/aRQdxy7A2ADzxzWZKy5t4gYph9wI22D4bUjd75pwFGaYHpjQf8tPg53955rwCyWtg3XSz3eMqc//NH2D/cutiEhERaSSUgInUk3tGdCQy0Iddh/P5YMk+q8OpX1EdoNulZnvRC/V/Pmc5fHUbZO2rXjNt2WuQubf+z93cuVww+0Gz3etauOIN6HoplJeYhDj7oLXxiYiINHBKwETqSai/N38c1RmA5+dtJyO/xOKI6tnw+8z9hs8hq57XQfvxH7BzHnj5w8Svoe25JgGY/0T9nldg838haYlp+xGPgt0O46dBbA/TE/nJ9VBaaHWUIiIiDZYSMJF6NGFga7rGh5BTVMa/5263Opz61bKfSYRc5bD45fo7z5Zv4OdnzfalL0JcTxj1BGCDjV/AwdX1d+7mrrQI5j5its+6D0Jbmm3fILj2I/CPgENr4X/3mJ4yEREROYYSMJF65LDbeLSiLP1Hy/axNSXH4ojq2Vl/MPer34f8I+5//8PbYcYdZnvwndDrGrMd3xt6TTDbcx7Rh//6svQ/pnczuAUMu6fmc+Ft4Jr3we5lekE9MRRVRESkEVICJlLPhraPZGyPOJwu+OvXm3E15eSg3XkmGSorhOWvufe9i3Lg0xugJBcSz6ro9TrKBQ+Dlx/s+8UUAxH3yk2Fn58z2yMfA5/AY49pezaMecpsz3sMtmtxbhERkV9TAibiAQ9d1BUfLzuLdx1hzuZUq8OpPzZbdS/Y8teh2E0LUTudMPNOSN9uel+ufgcc3jWPCWsNQ+4023Mf1eLA7vbj30zy26If9Lz6xMcN/C30vwVwwZe3ml5LERERqaIETMQDWkcEcNvZbQH4x6wtFJeVWxxRPep6KUS0M+tyrX7fPe+56N+w9Ruz4POEDyAo5vjHnfUHCIg0idrq99xzboFD62H1B2Z7zFOm8MaJ2Gww9p+QMAyKc+Dja7VGm4iIyFGUgIl4yF3ndSAm2Jd9Rwp4Z9Feq8OpP3YHDPu92V7yMpTVsfrjznnV1Q0v+ie0GnDiY/1C4dwHzPaCJ6E4t27nFjOf7vuHABd0vwISBp/6NV4+Zj5YaGvI2AVf3GqWDhARERElYCKeEujrxQNjugDw0vwdpOUWWRxRPep9HQTFQs5BU5mwtjL2mA/vuKDfxIqhbafQf5Lpgcs/DIterP25xdj6Lez9GRy+cOHjp/+6oGi4dropV79rvhkWKiIiIkrARDzp8r4t6d06jPyScp79fpvV4dQfbz8YcpfZ/uV5M4frTJUUwKc3QVEWtOxver9Oh5cPjKxIFBa/BDnJZ35ud3A6G/88tLJimPOw2R42GcISzuz18b1g/H/M9pKXYe3H7o1PRESkEVICJuJB9qPK0n++6gAbDmRbHFE9GjAJfEMgfRts/+7MXutywde/h9QNEBgN13wAXr6n//qu46D1EFON8ce/n9m53SE/HV47B14eANkHPH9+d1n+OmTuMb2ZlcVVzlSPK+DsP5ntr++FA6vcF5+IiEgjpARMxMP6J4Yzvk8LXC54/OtNTbcsvV8oDLzVbP/y7zNbm2vZq2YtKZsDrn63esHf02WzVZepX/MRpG46s9fXRWkRfHKDSR4z98CnN0JpoefO7y756bCwotfxgkfAN7j273X+X6DzRVBeDJ9cDzmH3BOjiIhII6QETMQCD4ztgr+3g5X7MvlmfRP+MDr4TjN36MAK2Lf49F6z9xf4/i9me/Tfoc1ZtTt360HQ7TLA5bn5Ry4X/G8y7F8KvqHgHwHJa+CbKY1vcegf/wHF2RDXC/pcX7f3stvh8tcgugvkpZj13Eqb8BxIERGRk1ACJmKB+FB/7jyvPQBPztpCYUkTrRAXHFv94X3R86c+PvsgfH4LuMqh5zUw+I66nX/EVLB7m0qKu36s23udjoVPm547uxdMeN+sV2azw7rpsPyN+j+/u6RuhlXvmO0xT5rKlnXlFwLXfQx+YXBwFXxzX+NLSkVERNxACZiIRW4/px0tw/xJzi7i9Z92Wx1O/Rl2j0lCdsyBlI0nPq6sGD672VQvjO0J414wQwnrIrJ99TDIuY/UrhjI6Vr/uSl9D3Dxv6DdeeZ2YcVQyO8fhL2L6u/87lJZdt7lNHPpatsDeTwR7cyQUpsD1n0MS15x33uLiIg0EkrARCzi5+3g/8aasvTTFu4kOasRzhM6HZHtodt4s32yXrBZf4aDK00PyYQPwCfAPec/534zHDBlA6z/1D3v+WtJS+G/FVUfh/2+Zrn8oXdDz6vBWWYSzIZelGPHHNj9o1n0+sK/uv/9258Po/9htuc+YnonRUREmhElYCIWuqRXPAPbhFNU6uTp2VutDqf+nHWfud/4FWTuPfb5Ve/C6vcAG1z1FkS0dd+5AyPh7Clm+4cn3F8QI2OPKSxRXgJdLqkugV/JZoNxL0JcTyhIryjK0UDnP5WXVs+/G3yH6bGqD4N/B31vNL1sn/8G0nfWz3lEREQaICVgIhay2WxMHdcdmw3+uzaZVfsyrQ6pfsT3hvYXmLldi1+u+dyBlab3C2DEI9BhpPvPP/gOCG1tFoZe+h/3vW9hFky/BgqOQHwfuOJ1U3Di13wCYMJH4B9uinJ820CLcqx4C47sgIAoOOdP9Xcemw0ufg5aDzaFPj6+Foqa8JIMIiIiR1ECJmKxHi1Dubp/KwD++vUmnM4G+MHcHSrXkVrzgZnnBZCXZhZbruw9OmtK/Zzb28+UUgf4+d+mxHpdlZeaIYXp2yGkJVz3CfgEnvj48ES4qqIox9qPYMWbdY/BnQoyquewXfAXs4xAffLyNeu7hbQ0Sd+XvwVnEy1GIyIichQlYCINwJ9GdybI14t1B7L5as1Bq8OpH23Ohhb9oKwI+4o3sbnKcMy4FXKTIaoTjJ9W96IbJ9PzatMTV5JrqhXWhctlerH2LASfILj+UwiJP/Xr2p9fPa9q9v81rKIcC5+GoiyI6Q59b/bMOYNj4dqPwMvPzD2bXw9zzkRERBoYJWAiDUBMsB+TL+gAwDOzt5JXXGZxRPXAZqvqBbOvepNe+z/AnrQEfILh2ummTHl9sturKxKufLtu844WvwSr3ze9WVe9beZ3na6hk6HHVaYox+cTG0ZRjsPbq8vkj/47OLw8d+4WfeGyimqIi543wyBFRESaMA/+LysiJzNpeBs+Xp7EviMF3PDGUlqFB+DjZce34ma2Hb/a5zDb3nZ8HHZ8vR1HHWuObxcViN1ejz1LZ6LLJRDZEduRHbQpqliX6/JXIaqjZ87f7lzoOBp2fA/zpprelzO15ZvqhZ1HPwmdRp/Z6202uPQlOLwNUjeYIZiTvjPDJK0y52EzP6/TWNNL52k9r4LDW+Gnf8K3fwTfYOh1jefjkIajtAg2zYD9y6DLxdDxQqsjEhFxGyVgIg2Er5eDhy/uxm3vr2TdgWzWHXBPUYJerUL58LeDCfHzdsv71YndDsN/D/+7B4Dy4VNwdL3EszFc+FfYORe2fgP7lkDi0NN/bfIa+Oo2wAUDf2uq+dWGTwBc+yG8fh4krzZJx2Uv1+8QzBPZOd8kpHYvGPU3z5+/0vl/MUVNVrwBM+4w8+m6XGxdPGKN7AOmh3rVu6a4DZhFwTuNMcsXRLa3NDwREXdQAibSgFzYLZaPfjuYA5kFFJc5KS51UlLupLi03DyuupnHJZWPS8srjjPPVW5nFZSy/kA2d3+0mrdvGYi3owGMOu41Aee22SSl59PynAdwePr8MV2g702m7P3cR+DWuaeX+GQfhOnXQmmBqdQ45um6JUzhbUxRjg+vgLUfQos+MOi22r9fbZSXVZedH3Q7RHXw7PmPZrPB2GegJM8s0vz5LXD9Z9b0yIlnuVywbxEsew22fmt6YwFCWkHiMNj0FWyfDbt+MAu7n/3Hkxe8ERFp4JSAiTQwwztEue29NhzI5prXlvDzjnQe+moDz1zVC5sVvSxH8/Kl/Kr3WDdrFi3tHk+/jPMfgg1fwIEVsHkmdL/85McX58H0CZCXAjHdTOLkjnlS7c8364bNfcQU5Yjtbj5wesrqd+HwFlMe/9z7PXfeE7Hb4dKXTRK25WuzvtpNMyFhsNWRSX0oyYf1n5n5h2mbqve3Odt8IdD5IvN7ds6f4LsHzALhP/8L1n4Mo56AHlda02ssIlJHDeDrcBGpLz1bhfLy9X2x2+DzVQd46QcteAtAcJz5Jh1g3mNQVnLiY53l8OWtZr5WYLSpeOjOgiHD7jEfJJ1lpqx9toeqYBZlw4//MNvnPWSSsIbA4QVXvmXWjSstgI+uhkPrrItn/3L46VlTpl/cI2OP6Xl9rit8c59JvrwDoP8kuHMx3PINdLu0+kuO6M5w0wyzll5Ygqmc+uWt8O4lkLLR0h9FRKQ2lICJNHEjusby18t6APDc3O18tboBVN1rCIbdA0GxkLn35GtyzXnYDH/y8jNrfYUluDeOyqIcsT3M+mif3mgKENQz+y//MnNsojrBgEn1fr4z4uULEz6EhKFmoeYPLjdFSzzJ6TSJ19uj4Ycn4D9DYfscz8bQlDidsHMefHQNvNgXlrxsvgQIbwOj/g5TNsO4500v8PHYbND1Erh7ufnCwMsP9v0Cr51tFnJXgixSvwqz4Ns/wdf3QW6K1dE0ekrARJqBG4ck8rtz2wHwwJfrWbzTDQsRN3a+QWYoIsBPz5j/XH5t+Ruw9D9m+/JXodWA+onFJ9BUZPQPry7K4aq/BbkDi1Kwr6gsO/8PcDSAAi2/5hNoehvje5tE8f3xJln2hIIMmH6NSbxcTvCPMMNPp18N//s9FOd6Jo6moCgHlr4KrwyED680BV9wmXmU138G96yBYZNPvwfW2x/OewAmr4Bul5l/n+Wvw0v9YeU7nlvMuzjPM+cRaQj2LoJXzzJFkla9Ay8NgCX/MfOIpVaUgIk0Ew+M7sIlveIpLXfxuw9XsT1VHyLpcyNEd4HCTDO35Gg75pl5JwAjHj31PLG6Cm9j1hSz2U1RjpP1ytVR9+RPsTlLof2Ihl3e2y8Ubpxh/o1yk+H9yyDnUP2ec/9yePVsUynTy8/0Tk7ZDEPuMs+vfg+mDW9Yi2g3RIe3m2/Ln+sKsx+AIzvNmn+D74DJq+DGL80SDvZafgwJS4Br3oeb/1vxO5xhhjO+cb75N3QnlwsydsOaD2HmXfBCb3iypanm6qmET8QKZSVmmP67F0P2fvP/VIt+UJIL3z8Ir50D+xZbHWWjpARMpJmw2208e3VvBrYJJ7eojEnvrCAtp/6HujVoDi9Tlh5MBbasJLOdutlU4XOVQ58b4Kwpnomn/QUw8jGzPfv/TJl8N7Pt/Zn47FW4bA6z6HJDFxhpCnGEtzE9YB+Mh/wj7j+PywVLXoF3xkLOAYhoD7+dB/1uNr0uY56Eid9AaAJk7TMfSL7/i0eGizYqKRtNovzKQPNteUkeRHWGi56FP26BsU+7t9pmu/Pgjl/Mmny+IWa+4FsXmqUMajtMyuk0fwOWvwFf/MYkkS/2hf/eDWs/qu6JXf2+WZaivNRdP400dy6XWe5k4T9h1Xsnn59c3w5vh7dGwi//BlzmC8s7foHfzodxL5he67RN5m/mjDsgL826WBshVUEUaUb8vB28ftMArpy2mN3p+fzmvRV8evtQAn2b8Z+CjqNM1bW9P8P8J0xSMn2C+YYv8Sy45HnPVlob9ntIXmtKb392M9y+AEJb1u69ykrMf5DJa81/6ofW4kjdDICz3y04Yrq6K+r6FRJvejreHmMWbP7wCpj4P9ND5g6FWebD9dZvzONu403P16+LrbQ9G+5cBN8/BGs+MPOYdsw1w1Nb9nNPLI3Zphmmh6i0wPTkdhoLg2+HtufW7++QwxuG3gU9r4b5j5meqnUfm0XTz73f9Lp5+Zz49eVlkLLefJO/bzEkLTa94keze0PL/qZKaeJw8/zMO2Hjl1BWbHqvvXzr72eUpqu81CzDsPVbc8s5qhDTz/+qGIFxRe17i8+Uy2XW4vv+L1BWCH5hJuHqPr76mP63QNdLYf7jJlFc97GJ/YKHYcCt7qkS3MSphUSamfBAH96dNIjL/7OIjQdzmDx9NW/cPACvhrBGmBVsNrMA8evnwobPzDfo2UmmB2TCByf/4FZf8Vz2MqRvh9SN8NlNcMss8PY7+evKSkxJ+eQ11QlX2mYor/kNqg3I9k8g4Jz7Pb8GW12EtzFJ2Dtj4dBakyTf+JVZ1LouktfAZxNNr5bd2/R0DfztiRMGvxDz79PlEvj695C+Dd4cCef82ZRLb4jz6eqb0wkLn4KFT5vH7c43H9jCEz0bR1A0XPYK9P8NzPqTmU859xHTUzX2KUg81xxXVgzJK82H3n2LYf8y01N3NO8AaD3IJFuJw0zy5e1f8xi/EPj0JpO4f3KD+Xvx62NEjqckH3bON0nL9tlQlFX9nHegWaLkwArzd+nLW2HRC3Dh42aURH3KOwz/m2xiAvPlyeWvQkiLY48NiDC/531vhm+nmL/L391vvpy66F9aPuQUlICJNEMJkQG8OXEA172xlB+3HWbq/zbxt/E9rF8jzCot+kCvCbD+U/OB2j8cbvjc/AdjBZ9AUwXw9fPg4CqY9UezPlblv095qekJSl5TnXClbjwm2QLMt5ct+pqfsUVfSqN7sGDRBi4KiPTcz+MuleXI3x0HSUtMxcjrPq5dz4PLBSvfgtkPmnYLS4Cr3zUftE9H5zHQeqn54LFphklAts+Gy18zi303F8V5MON31b2HQ+42w3qt/Aa8VX8zTGrtR2b+ypEd8OGVONqdz/C0Q3itvw3Ki2u+xjcUEodW93DF9z51Mt1ptCkU8/F1Zs7g9GtMpVQtEl3N5YKt3+JY+Q69ssohvSPEd7M6KmvkH4Ht35mka9cPUHbU8OWAKOg81nyx0+4884VbST4snWaSr5T1phpsu/PMMPUWfd0f3/Y58N+7TDVeh485z+A7T93z1qo/3PYDrHoX5v8VUjbA26PMkMWRj5kvRuQYSsBEmqm+CeG8cG1f7vhwFR8tS6J1RAB3nNve6rCsc8EjsPl/Zj2uCR9CpMVtEdEWrn7HVI5b8yH4BJnYktea/+B+/QESzJC8+D41Ei7CEmv25pSWgq0Rr50U3xtu+Mx8GNk1H7787ZkvjF2ca0opb/zCPO58EYz/z5mvhRYQYZK2LpeYypWH1ppJ6SMeNUU7PDVkyCqZe+Hj680wV4ePGa7b9warozLsduh3E3QdZ3rmlr2GffePVC1zHxhdnWwlDjMLrNdmYfj258NNX5n16vb8BB9cYb68cedagY3V7gXmA/nBVdiBtgCvDYVOY8wyIInDm/5C2pn7qocWJi02VTsrhSWa67PLxdB68LHXn0+g6VXvPwl+ftbMSdy9wHwx1+MqM9wvom3dYywpMD3FlYWfYrrBFW9AXI/Tfw+7AwbeaiqTznvM9IKt/RC2fm3+HvafVLvfrybM5nLVY63jBignJ4fQ0FCys7MJCbH+D2RpaSmzZs3ioosuwtu7GQ5dsZDa3nj7lz389RszL+il6/oyrvdxhhq4WYNt+7St5j/I2Ab0De0vz8O8qcfu9w2F+F4VyVZFwhXe9pQfaBps25+pXT+YYYjlJdD7ejP07HQSntRNZm7dkZ1gc5hhPUMn1/2DYM4hUxVv51zzOHG4SerC21Qd4ra2z0k2PZ/56eYDnBU9tXt+MkM3CzPMenoTPjRD9hqqtC2Ur/uMDUmZdL/4drxju7r3w/+BlWZuYlG2qRJ345fW9aBb7cBKk3jtWWgeewdS3n8SqVuWEJ+9GhsVHztb9DWJWNfLms6cIZfL/I3Z+o25pWyo+XxcT+hSkXTFdj+zazBzL/z4D1j/GeAyQ6YH/MYMfz5JL9NJ/+4cWme+xErfbh4PvtP0Wp1qyPup7F9uvpRKWW8ex/eGi5+rv6Vc6siK3KCJXPEiUlu/OastBzILeXvRHv742TpiQ/wY1LaZfnBoiEPHht9rPuQmr4G4oxKu8LZNv4flZNpfYHq+PrsZ1k0367qNfebkH2jWfGQ+FJQVQnAL08OYMMQ98YTEm56P1e+Zyev7Fply9aP/Dv0m1v7Dfv6RimGmq839wdVmTbJKcx+Bs/8Ig35X9w9Np8PlMt+Uf/eAqRIa3weunV77QjGeEtMV53kPsW/WLLpHdnR/z0urATDxa7NeXfJqeO9SuHkmBEad6pVNR+pm+PHv1cNRHT4mQTj7jzh9w1lRPIuLhnTGe8XrZnho8hpTZTI0AYbcaXosfYOt/RlqK22L+Zk2/8/M26pks0PCMLOIeOeL6jYvMrwNXPG6+cJo3mNmBMDy18x5h/0eht5t/g6eDmc5LH4Rfvg7OEshKM58YdRhRO3jO1rrQaaA1Mq3TXGrQ+vgzRGmquyIx0x122ZOCZiI8JeLu3Iwq4DvN6Vy2/sr+equYbSPPs0/5FK/bLbqUvlSU9dLYPw0Mwdp+evmw9uIR489rqQAZv3ZDIkBs/7ZFa+7/8OxzWaqg7U911QDTFoMX99rhh9d+hL4neJDR1GO+aCSvNokWslran6YqzqPHaK7miGp6dtg7qNmeNIFj5hKgPWVmJeVwHd/NnM9wJzr0pdUeKJSfG+YNMskX6kb4J2LTLXO4DjPnL84z/TABsdDq4GeG/KVsQcWPGXm0OIy12fv682C2WEJ5pjSilL9Ee3hkufg/IdgxVvm9zY7yawpteApGHCLqVp5vKIPDU1RNmz8ygy3O7iqer+Xn/mCqMslZrilu5ON+F5m2OvuBTB3qhn6vOAf5ouRc+83f4NONn8xa78pG7/vF/O4yyUw7kX3x2l3wKDbTFXZeVNNorj6fdjyNYyYar6YasZfIioBExEcdhvPT+jLdW8sZe3+LG55Zzkz7hpOVJDKKksD13uCqWD37RRTstknCM4+at229B2mlyxts/lgeN5DpseoPv/jj2gLt3wDS/9jvv3dMQdeGYxtzDNARS9VaaEZnlSZaCWvNrFynFkBkR3MsLYWfU25+7hepvqjs9x86P3hb2aR1Bm3w5KX4MInzNwkd8o7bNoxaTFgM8OUht/b9OfwnKmYrjDpO3j/UpMcvzMWbv4fhLWuv3OWFJiCMr88DwXpZp9/OHS40BQKaX9B/QyHzE2Bhc+YXl9nmdnX7TI4/y+mYM7JBEaZBG347801vPhlUyxl0QtmPb6eV5uenjOZh+QJTqfp3V7zIWz+r+lNB7B7mWSr1zXQYaRnCrG0Ow9u+xE2zzRDPjP3mOqfS/9jvojqNv7Y388NX8A3U6A421RbHPs09L2xfn+Pg6JN71q/m80IhNSNZtH01e/Dxf9qtkt4aA6YxZrMfIxGSG1/rPS8Yq74z2KSMgro3TqMT24bgr+P+79FVdtbp8m2/aIXTE8QmEV/B91mPmx8fa9J0AJj4Mo3od25no0rbatJjA6tA+BwUFeiAr2wHd5S/aH1aKGtqxOtFn3NED//sJOfo7QQlr0KPz8HxTlmX/sRZn5bXM+6/wyH1sMn15skzzcErnwLOo2q+/t6mEev/cy98N44s7h7aILpCXNHwYSjlRbBqnfMv3t+xSK4oa3NNVCUXX2czW6KPHQaDR1HmySxLh+4CzLM79uy16oTkPYjYMQjJ6zOd8q2dzphx/ew+CWT4FRqd76ZJ9b+AmuT/eyDZqjzmg+rF+IGs8h4v5tMFd2gGMvCo6zEJMILnzZVDMF8aXPh45S2Gsqcr79grHMe9srCQy0HmFEAni42VV5meup+/HvF3yqbGbrb9mzPxvErVuQGSsAs1mQ/DDUCavvj2304jyumLSaroJRR3WKZdmN/HHb3/sentrdOk277+U+YamFgvv2vKohxFlz1lueGgv1aeSn89Cyun/6JzVVevT8w2nxIatmvuoerLiWb849UV0tzlgI26H0dXPAXCG1Vu/c8enHliPamzHp0p9rHaCGPX/vZB01P2JGdZljgxK8hqmPd37es2PQe/PwvyD1k9oUlwDn3Q+9rAZtZ22zH96a0+OEtNV8f2tosQN9pjPnge7pDSIvzYNk0WPSS6UEBaDUIRk6FNmed9KVn1PYHV5kesc0zq6sGxnQ3iViPKz23NmNZMWybZZKuXT9Ux+ITDD2uMD06Lfs3rF7g4lzTg7joRSjNB8DZ7nyKDmwgoCTdJOPn3G/9moW5qeYLs8NbTC+exRUSlYB5gBIwqaS2P7EVezO44c1llJQ5mTS8DVPHdXfr+6vtrdOk297lMsUhlr9Wve/sP8F5DzaIKmul+1ez/btpdB56EV4JAyGkZf18eMvYAz88ARu/NI8dvqbIwVl/OHVvWiWnExY8CT89Yx63H2GS2DMt1d+AWHLt56bC+5eZD5qB0WYx8dha/j0tKzHzaH56FnIOmH0hrcyH6T43nDgxydxnhsFu/x72/lxz/Skvf2h7junR7Dj6+EMly4ph5Tsmua/sXYntYeYcdhp9Wtdwrdo+cy8sfdUkmxXJBMHxMPh30Gms+ULFL9T9v0MpG828rvWfQmFm9f7Es8xwvW6XNvy13vLS4Kd/miIYFT3trrA22K58o2FVKy0pMMOpLdZsqyC+8sor/POf/yQlJYXevXvz0ksvMWjQ8S+QN954g/fff5+NG806Nv379+cf//jHCY8XkTM3sE0E/76mD3dPX807i/bSOjyA35zl5uEzIu5ms8GYp8y3qTvnweh/QMcLrY6qWlxPdsZeQqcuF0F9JgARbeGqt01VtDmPmsn2i543Q5TOud+s13OyxauLc80k/cpqdkMnw8jHG0QS2+gEx8It38IHl5k5f+9eDDfNNMtGnK7yUlj3iUmGs5Iq3jfezGXsd/OpFyIPTzRDcgfdZj7w7vmpuncs54DZ3vE98EfT01SZjLXsZ0qeL3zaDD8FU331goeh+xX1X0AhvA2MfcrMFVv1rknGcg+ZCoDzHjPHePmbNg6ONwlZUJy5D46vud835OSJWmGmGbK85kNT1KJScAvoc725Wb025JkIioGL/gmD78D507/Yk3yYhImv4R3UwCocN4DkyyqW/zX99NNPmTJlCq+++iqDBw/m+eefZ/To0Wzbto2YmGPH0y5YsIDrrruOYcOG4efnx9NPP82oUaPYtGkTLVs28DK4Io3Ixb3iOZDZhSe/28oT326mRZg/Y3pYNIRL5HTZ7TDmSeBJqyOxXsv+phjI9u9NFbLDW021uWWvmkn6x/sQnbHHzPdK22zKiI97EfpcZ038TUVgpBl++OFVcHClqZJ44xen7okoL4MNn5sEKHNPxXvFmMSr/y21W3bAJwA6jzE3l8v8O2+fbZKxA8vNotppm+CXf5vCEpXzFIPj4dwHTA+Qp4eu+Yeb3tshd5vF01e8ZYZ1FmWZOWiZe2vOyzoeL//jJ2YBkaaa4Javq3sG7d5mna6+N5liNo15AeHI9pRf8gIbZ80iobGW+G+iLE/AnnvuOW677TYmTZoEwKuvvsq3337L22+/zf/93/8dc/xHH31U4/Gbb77Jl19+yfz587n55ps9ErNIc3H7Oe3Yn1nAh0uTuPeTNXx8+xD6JTTeIUgizY7NZj5sdxhphq/9+A9T2v7LW81ckQv/Wj0Bfs9PptJhYWbF4sofQeuB1sbfVPiHm3XBPrrGVJL84HK4/tPjz51ylpvy5gufMokGQECUSUIG/MZ9vQY2mxkOGdvdJHUFGbBzvknIds4zCY5/hKkqOvC31i834OVT3RsFpvhMbgrkpZqesdyUo24Vj/NSTEGSskKTxFYmsscT090U1Oh5jdapknpnaQJWUlLCqlWrePDBB6v22e12Ro4cyZIlS07rPQoKCigtLSUi4vjdqsXFxRQXF1c9zskxFaJKS0sprVyXwkKVMTSEWJobtf3p+cuYThzIKGDB9nQmvr2cadf3YXAdF2pW21tHbW8dy9u+1/XQ5TLsy1/FvuRFbMmr4b1LcHa4EFfLgdh/ehqbqxxnfF/Kr3rfLC7dhK4Ty9vf7gcTPsbxxc3Y9yzE9eFVlF/9Pq52FUsGuJzYtn6N46dnsKVvM7v8I3AOnYyz/63V847qK37vYOg63tycZSb5C21llnao43nrp+29ILiVucWf7OQFkJeKLS8VclOw5ZmkzZabAvlpuCLa4+x9PcT1rh6mqOu+WbGibSwtwpGcnEzLli1ZvHgxQ4cOrdp///33s3DhQpYtW3bK97jrrrv4/vvv2bRpE35+x3bHP/bYYzz++OPH7J8+fToBAc137KnImSguh1e3ONida8Nhc3FjByf9oppV/R6RJsWnNIfOKTNpk/4jdqorM+4PH8bahN/gtHuo0lwzZHeWMHDPS8TlrKPc5sXKtpOxuZx0PjSD0CIz16rEEcjOmLHsib6QMocWuhapTwUFBVx//fXNrwhHbT311FN88sknLFiw4LjJF8CDDz7IlCnVi3Lm5OTQunVrRo0a1WCqIM6dO5cLL7yw6VUka+DU9mdm7Jhy/vjFBuZsTuO9HQ5atO/EpGGJ2GpRgUptbx21vXUaXttfS/mRnbDg79h2/YDz7D8RN2QyYxpSWW03alDtX34Rzpm/w7H1awbvfr5qt8s3GOegO7ENuoOOfiG4oWh9g9Cg2r6ZUdufWuXoOE+yNAGLiorC4XCQmppaY39qaipxcSef7P/ss8/y1FNPMW/ePHr16nXC43x9ffH1PbZCkLe3d4O6EBtaPM2J2v70eHt7M+3GATzxzWbeXbyXJ2dvJyW3hIcv7lbrdcLU9tZR21unQbV9XFe49kNwuXDYbDTicgOnrUG0v7c3XP0uzLzDFNrwCYIhd2IbejcO//Am++/QINq+mVLbn5gV7VLPNURPzsfHh/79+zN//vyqfU6nk/nz59cYkvhrzzzzDE888QSzZ89mwIABnghVRACH3cbUcd34y0VdAXhn0V4mT19NUWn5KV4pIg1aE+31atAcXnD566ZM/X0bTHn3RrzOmoicPksTMIApU6bwxhtv8N5777FlyxbuvPNO8vPzq6oi3nzzzTWKdDz99NM88sgjvP3227Rp04aUlBRSUlLIy8uz6kcQaVZsNhu3ndOOF6/ri4/DzncbU7jxzWVkFZRYHZqISONit5tKiAENbH0mEalXlidgEyZM4Nlnn+XRRx+lT58+rF27ltmzZxMbGwtAUlIShw4dqjp+2rRplJSUcNVVVxEfH191e/bZZ636EUSapUt7t+C93wwi2M+LlfsyuXLaYvZnFFgdloiIiEiD1iCKcEyePJnJkycf97kFCxbUeLx37976D0hETsvQ9pF8eecwbnl7ObsO53PFtMW8c8tAerQMtTo0ERERkQbJ8h4wEWncOsUG89Vdw+kSF8zh3GImvLaEhdsPWx2WiIiISIOkBExE6iwu1I/P7hjK8A6R5JeU85t3V/DZyv1WhyUiIiLS4CgBExG3CPHz5p1bBnF535aUO13c/8V6Xpi3AwvXehcRERFpcJSAiYjb+HjZee6a3tx1XnsA/j1vOw9+tYGycqfFkYmIiIg0DErARMStbDYb94/pwhPje2C3wScr9nPb+yvJLy6zOjQRERERyykBE5F6cdOQRF67aQB+3nZ+3HaY695YyuHcYqvDEhEREbGUEjARqTcXdotl+m1DiAj0Yf2BbK6Ytojdh7VouoiIiDRfSsBEpF71SwjnyzuHkRgZwP6MQq6ctpg1SVlWhyUiIiJiCSVgIlLv2kYF8uWdw+jdKpTMglJuemclPx2ysT01VwU6REREpFnxsjoAEWkeooJ8+fj2IdwzfQ3zt6bx5V4HX768BD9vO91bhNKzZSg9Wpr79tGBeDn0/ZCIiIg0PUrARMRjAny8eO2m/kz7cQczl+0gpdiL/JJyVu3LZNW+zKrj/LztdIsPqU7KWoXSITpISZmIiIg0ekrARMSjvBx27ji3HQn5Wxk9ZhQHskvYeDCb9Qey2Xgwm03J2eSXlLM6KYvVR80VU1ImIiIiTYESMBGxjMNuo0NMEB1ighjftyUATqeL3en5bDyYzYaK26aDJ07KusaHcHbHaG4Z1oaIQB+LfhIRERGR06METEQaFPsZJmVrkrJYk5TFmz/v5qYhifz27HZEB/ta/FOIiIiIHJ8SMBFp8E6UlO05ks/qfZm8u3gvm5JzeO2n3by3ZC/XDUrgd+e0Jy7Uz+LIRURERGpSAiYijZLdbqN9dBDto4O4qn8rftyWxgvzd7JufxbvLNrLR0uTuGZgK+44tz2twgOsDldEREQEUAImIk2AzWbjgi6xnN85hp93pPPSDztYsTeTD5cm8cny/VzZrxV3nd+exMhAq0MVERGRZk4JmIg0GTabjXM6RXN2xyiW7s7gpR92sHjXET5duZ8vVh/gsj4tuPv8DrSPDrI6VBEREWmmlICJSJNjs9kY2j6Soe0jWbk3gxd/2MlP2w/z1eqDzFhzkEt6teCeCzrQKTbY6lBFRESkmdECOiLSpA1oE8H7vxnEzLuHM7JrDC4XfL0umVH//ok7P1zFpuRsq0MUERGRZkQJmIg0C31ah/HmxIF8+/uzGNsjDoDvNqZw8Yu/8Nv3VrBuf5a1AYqIiEizoCGIItKsdG8RyrQb+7MtJZeXf9zJN+uTmbcljXlb0jinUzQ3D0mke8sQ4kL8sNlsVocrIiIiTYwSMBFpljrHBfPSdX25b2RHXvlxJ/9dm8xP2w/z0/bDAIT4edE5LphOscF0qbjvHBdMWICPxZGLiIhIY6YETESatfbRQTx3TR/uHdGRN37ezdLdGexJzyenqIwVezNZsTezxvGxIb4mGYsNplOcSc46xgTj7+Ow6CcQERGRxkQJmIgIkBgZyN/G9wSguKyc3Yfz2ZaSy7bUXLZX3B/ILCQ1p5jUnGJ+3pFe9VqbDRIiAuhc0UtW2WuWEBmAr5cSMxEREammBExE5Fd8vRx0jQ+ha3xIjf25RaXsSMtje0ouW1Ny2Z5qbul5Jew7UsC+IwXM2Zxa4zV+3nbC/H0I9fc2twBzH/arx5W3sABzbIifF14O1UkSERFpapSAiYicpmA/b/olhNMvIbzG/vS84qpesqN7zfJLyikqdZJSWkRKTtGZn8/XixB/b8ICvIkP9advQhj9EsLp3TqUAB/9+RYREWmM9D+4iEgdRQX5EtXBl2Edoqr2OZ0ucovKyC4sJauwhOzC0qpbVkEpOUdtH/1cdmEpecVlAOQWl5FbXMbBrEI2Jecwb4vpXXPYbXSND65KBvsnhtMq3F9VG0VERBoBJWAiIvXAbreZ4YUB3iQQcEavLSt3klNURlZBSUUCV8ruw/msTspk9b5MDmUXsfFgDhsP5vD+kn2ASQL7JYTRL9EkZD1bhuLnrflnIiIiDY0SMBGRBsbLYSci0IeIwOqS9+d3hltpC8Ch7EJW78ti1b5MVidlsik5m/S8YuZsTq2ag+Zlt9G9RQh9K3rI+iWG0yLUz5KfR0RERKopARMRaWTiQ/25uJc/F/eKB6CotJyNB7MresiyWJWUyeHcYtYdyGbdgWzeXbwXMCX0+7QKJbDARq/MQtrGeFv4U4iIiDRPSsBERBo5P28HA9pEMKBNBAAul4sDmYVVQxZXJ2Wx+VAOqTnFfL85DXDw1XM/0y0+hFHdYxndPY4uccGaQyYiIuIBSsBERJoYm81G64gAWkcEcFmflgAUlJSx/kA2K3an89/lO9ida2PzoRw2H8rh+Xk7aB3hz6hucYzqFsuANhE47ErGRERE6oMSMBGRZiDAx4sh7SLp3zqEhPytDDl3JD/tzOD7Tan8vOMw+zMKeeuXPbz1yx4iAn0Y2TWGUd3iOKtjlIp5iIiIuJESMBGRZigi0IerB7Tm6gGtKSgp46ft6czZlML8rWlk5Jfw2coDfLbyAAE+Ds7tFM2o7rFc0DmW0IDazxtzOl2k5RZzILOA/ZkFHMgoNPeZ5r6o1Em7qEA6xQbTMTaIDjFBdIoNJjLQR8MjRUSkyVACJiLSzAX4eDGmRxxjesRRWu5kxZ4MU1FxUwrJ2UV8tzGF7zam4GW3MaRdJKO6x3Jht1jiQ/1rvI/L5eJIfgn7MwrYn1loEq0Mc38gs5CDmYWUlDtPGsvh3GKW7cmosS88wJuOMcF0iA2iU0wQHWOD6RgTRHSwrxIzERFpdJSAiYhIFW+HnWEdohjWIYqp47qx8WAOczanMGdTKttSc/llZzq/7Ezn0f9uonerULq3DOVQViEHMs2tsLT8pO/vsNuID/WjdXgArcL9aR1Rfe/jsLPrcB470vLYkZrHjrRckjIKyCwoZfneDJbvrZmYhfp70zEmqKK3zCRlnWKDiQ1RYiYiIg2XEjARETkum81Gz1ah9GwVyh9HdWZPej5zK5KxVUmZVWXua74G4kKqE6xWlQlWxeP4UD+8HPYTnrN367Aaj4tKy01SVpGQmfs89h3JJ7uwlJX7Mlm5L7PGa4J9vegUF0y/hDCzBlpCODEhWgNNREQaBiVgIiJyWtpGBXL7Oe25/Zz2HM4tZv6WVA5mFdIirDrBahHmj4/XiROsM+Xn7aB7i1C6twitsb+otJw96flsT81l51E9ZnuPFJBbXMaqfZms2pfJGz/vAaB1hD/9j1qUunNs8EkTQRERkfqiBExERM5YdLAv1w5KsOz8ft4OusaH0DU+pMb+4rJy9qYXVC1MvWpfJttSc9mfUcj+jEJmrk0GINDHQZ+EMPonmISsb0I4of5amFpEROqfEjAREWkyfL0cdI4LpnNcMFf2bwVAblEpa/dnVfWKrU3KIre4jEU7j7Bo55Gq13aKDaoastg/MZy2UYGaSyYiIm6nBExERJq0YD9vzu4YzdkdowEod7rYkZZblZCt3pfJ3iMFbE/NY3tqHh8v3w+Y6ov9E8Pp1SqMuFA/ooN9iQ7yJSbYl8ggXy1WLSIitaIETEREmhWH3UaXuBC6xIVww+BEANLzilm9L5NVSSYhW3cgm8yCUuZtSWPelrRj3sNug4hAX6KDTUIWHfyr7aCKxyF+BPo41JMmIiJVlICJiEizFxXky6jucYzqHgdASZmTTcnZrE7KYuuhHA7nFZOWU8zhvGKO5BXjdJmkLT2vmC2HTv7e/t6OiqTMh5JcO7Nz1uHt5cDLYcPLbsNht+Nlt53ZY4eNIF8vhneI0tw1EZFGRgmYiIjIr/h42embYIpz/Fq508WR/GIO55pbWm71dtUtz9znFZdRWFpOUkYBSRkFgJ0NmalujXNk1xgu79uKcztFu7UCpYiI1A8lYCIiImfAYbcRE+xHTPCp1xbLLy4jvSIZS87M5+fla+jarTsum51yp5PSchflThdlThdl5c6q7XKni9KjHpeVO6v2Vz5Oyihg1+F8Zm1IYdaGFMICvLmkVzyX921Jv4RwDXsUEWmglICJiIjUk0BfLwJ9vUiMDKS0ZTCuJBcXDUnA27vuwwZdLhebknOYueYg/12XzOHcYj5cmsSHS5NIiAhgfN+WjO/TgnbRQW74SURExF2UgImIiDRCNpuNHi1D6dEylAcv6sriXenMWHOQ2RtTSMoo4MX5O3hx/g56tw7jir4tuaRXPJFBvlaHfUpFpeUs3H6Yb9YfYvW+THq2DGVU91gu6BJDWICP1eGJiNSZEjAREZFGzmG3VZXa/9v4MuZuTmXGmoP8vCOddfuzWLc/i79+s5lzO0Uzvm9LLuwai7+Pw+qwq5SUOVm0M52v1yUzd3MqucVlVc8dzCpk9qYUHHYbg9tGMKpbLBd2j6NlmL+FEYuI1J4SMBERkSYkwMeLy/q05LI+LTmcW8w365OZseYg6w9k88PWNH7YmkaQrxdjesRxed+WDGkXacmaZmXlTpbsPsI36w4xe1MK2YWlVc/Fh/pxcc94hneMYs2+TOZsTmVrSi6Ldx1h8a4jPPb1Znq0DGFUtzhGdY+lc2yw5ryJSKOhBExERKSJig72ZdLwtkwa3padaXn8d+1BZqw5yIHMQr5YdYAvVh0gLsSPS3rF06t1GB2ig2gXHYifd/30jpU7XazYm8E365P5bkMKR/JLqp6LCvLlkl7xXNIrnn4J4dgrksLzO8cwZVRn9h3JZ+7mVOZsSmXFvgw2Hsxh48Ecnpu7nYSIAEZ1i2VU9zj6J4a7PaF0uVxk5JewP7OQpIwCDmUV4nSBl92G3W6rce+w23DYzLIBdpvt+MfYKo6z2/B22OkQE0Sgrz6SiTQX+m0XERFpBjrEBPHHUZ2ZcmEnVu7LZMaag3y7/hApOUW8+cuequNsNmgdHkCHmCBziw6ifcV2bdYcc7lcrE7K4pv1yXy7/hBpucVVz0UE+jCmRxyX9IpncNuT98QlRgby27Pb8duz25GeV8wPW9KYszmFn3akk5RRwJu/7OHNX/YQGejDiK4xjOoWx1kdo047mSwsKedAplkuYH9GAUkZhezPNNv7MwrILyk/45/9dHnZbfRqFcrQ9pEMaRfJgMSIBjVEVETcSwmYiIhIM2Kz2RjYJoKBbSKYOq4bC7YdZsG2NHak5rHzcB5ZBaVV65b9sDWtxmtjgn2rE7OK5KxDTBDRwb41hgC6XC42Hszh64qk62BWYdVzIX5ejO4ex7jeLRjaPhJvx5mvXRYV5Ms1A1tzzcDW5BeX8fOOw8zZlMq8LakcyS/hs5UH+GzlAfy9HZzbKZpR3WMZ3i6cjGJYtieD5OwS9v8q2UrPKz7leWNDfEmICKBFmD9edjtOl1kWwOl0UeZ0Uu6EcqeTclfFfcXSAVU3l4uycleN1+WXlHM4t5jVSVmsTsrilR934e2w0ad1GEPbmYSsX2J4vfVKiojnKQETERFppny9HIzuHsfo7nGASZyO5JewMy2PHWl57ErLY2fFLSWniLSKhacX7zpS432C/byqErIQf2/mbUll35GCqucDfRyM6m56us7u6N4FowN9vRjTI54xPeIpLXeyYk8GczanMmdTCsnZRczelMLsTSkVR3vB6pUnfK9gXy9aRwTQOsKfhIiAiu0AWocH0Crcv96SoP0ZBSzZfYSlu46wZPcRDmUXsWJvJiv2ZvLiDzvNwuCtwxjaPpKh7SLpkxCGr5cSMpHGSgmYiIiIAKZ3LCrIl6ggX4a0i6zxXG5RKbsO51clZDvTctmZlkdSRgG5RWWsScpiTVJW1fF+3nZGdI1lXK94zusc45EeHG+HnWEdohjWIYqp47qxKTmHOZtSqop4OGwuWoUHkhBpEquEiuSqMuEK9fe2pJhHZaJ3zYDWuFwukjIKWFKRjC3ZdYS03GKW7clg2Z4MnmcHvl52+ieGM7RdJEPbR9KrVZhbk1oRqV9KwEREROSUgv286dM6jD6tw2rsLyotZ++R6sQsLbeYIe0iGdElxtLCEkevkzZlVGeO5BTw0w9zueTis9yyEHZ9sdlsJEYGkhgZyLWDEnC5XOxOz2dpRTK2dPcR0vNKqipCMhf8vR0MaBPOkHaR9GwZSqfYYGJDfFUZUqSBUgImIiIitebn7aBLXAhd4kKsDuWkQvy9saDafp3ZbDbaRwfRPjqIGwYn4nK52JmWZ4Ys7j7C0t0ZZOSX8POOdH7ekV71umA/LzrFBtMpNoiOMcF0ig2mY2wQMcFKzESspgRMREREpJGw2Wx0jA2mY2wwNw9tg9PpYntaLkt2HWH5ngy2peSy90g+uUVlrNqXyap9mTVeH+rvTceYIDpWJGeViVl0kBIzEU9RAiYiIiLSSNnttqoeyEnD2wJQXFbO7sP5bE/NZUdqnrlPy2PfkXyyC0tZuS+Tlb9KzMICvOkUE0yH2CA6xZjErHNcMJFBvlb8WCJNmhIwERERkSbE18tB1/gQusbXHBZaVFrOrsNmrt721Fy2p+axIzWXfRkFZBWUsnxvBsv3ZtR4TVSQL13jg+kcG0yX+BC6xAXTISao3ouquFwu0vNK2FFR7GVHah470nLZfTgffx+HmScXEUBiZABtIgNJrCisonL90hgoARMRERFpBvy8HXRvEUr3FqE19heVllcsPVCZlJkELSmjgPS8Yn7eUVxjfpnDbqNNZIBJyCp6yrrGh9AyzB/7GU60c7lcpOYUsyMttyLJMhU2d6SZNelO5OhlDirZbBAf4kdCVVJmEjNzCyTIwqIwIkfTlSgiIiLSjPl5O6oqRh4tv7iM7am5bEvJZWtKLltTctiakktWgVmSYNfhfL7lUNXxgT4OOscF0zkupLrXLC6EAG9wuuBgViF7MjLZWdGbtSMtj52peeQWlx03LpsNEiMCKhb+DqZjTBDtY4IoKi1n35F89h0pYN+RAvZWbOcVl5GcXURydhFLd2cc835RQT7VSVlEIG2izKLascF+xIT4qvdMPEYJmIiIiIgcI9DXi74J4fRNCK/a53K5SMstZsuhnKMSs1x2peWRX1LO6qQsVh+1HhxAbLAvmfkOSpb+fNzzVPaodYwxBUE6xJjKje2iA0+YFP16nTqXy0VGfgn7MgrYdySfvekFJGWY5CzpSAFH8ktIzzO3XxcmqRTi50VsiB+xISYhiwn2IzbE1zwONvfRwUrUpO6UgImIiIjIabHZbFVJynmdY6r2l5Y72ZOebxKyo5Kzg1mFpOYWAza8HTbaRQXRITbIVGKsSLgSIwPw9apbUmOz2YgM8iUyyJd+RyWMlXKKSkk6qscsqeI+JaeI1Jwiikqd5BSVkVNkhkGeTFiAd1VCFlPRexYb7EvAUUMcKwdiVlaWrH78q/uKZ35dgNLXy06AjxeBvl4E+joIrNgO8HHg62VXxcpGTgmYiIiIiNSJt8Nese5YMJf2blG1P6eolK3JWaxdvpgbx48hwM+aqoohft7HHWYJpvcst7iMtJwiUnOKSc0pIi234v5Xj4vLnGQVlJJVUMr21JMnavXFy24jwMdRkZx5EejjqJGsBfh4EVRx7+9tIynNRuiuI7SODCI+1I8AH338t5r+BURERESkXoT4edO3dRiHNpgkrSGy2WyE+HkT4udNh5jgEx7ncrnIKSwjNbc6MTt6u7jMicvlqj6+6nW/fuz61ftWPu+qelxS5iSvuIyCknIKSsrIKy6jqNQJQJnTVdFbd/y5c8dyMH3XqqpHIX5etAjzJy7Uj/hQP+JC/IkPM9vm5k+gGwqWlJQ5ySkqJbeojJzCivuiUnKLSskpNNv3XNARH6+GeV3UJyVgIiIiIiKnYLPZCA3wJjTAm06xJ07U6ku500VBSRn5xeXkl5SRX2y2KxO0gpLyY/blFpayfd9BynyCSckpJq/YJG45FUNETyTYz8skZ6H+tAj1q0rWfL0cNZKqnKqkqjLJqthXWEpxmfOUP9PNQ9sQHdz81ppTAiYiIiIi0sA57DaC/bwJ9vM+7deUlpYya9Z+LrpoON7e3uQWlZJSUSkyJbuQQ9lFHMoq4lBOxeOsInKLy8gtKiO3KM8twyyDfb0I9vMixN/b3Pt5V217neGyBU2FEjARERERkWagMoHreJIevNyiUlJziqqTs+wiUnIKSc4qoszpJMSvOpEK9vMmxN+ret/RSZafN0F+XjiaaZJ1MkrAREREREQEqE7STjYfTuqm+c16ExERERERsYgSMBEREREREQ9RAiYiIiIiIuIhSsBEREREREQ8RAmYiIiIiIiIhygBExERERER8RAlYCIiIiIiIh6iBExERERERMRDGkQC9sorr9CmTRv8/PwYPHgwy5cvP+nxn3/+OV26dMHPz4+ePXsya9YsD0UqIiIiIiJSe5YnYJ9++ilTpkxh6tSprF69mt69ezN69GjS0tKOe/zixYu57rrruPXWW1mzZg3jx49n/PjxbNy40cORi4iIiIiInBnLE7DnnnuO2267jUmTJtGtWzdeffVVAgICePvtt497/AsvvMCYMWP485//TNeuXXniiSfo168fL7/8socjFxEREREROTNeVp68pKSEVatW8eCDD1bts9vtjBw5kiVLlhz3NUuWLGHKlCk19o0ePZqZM2ce9/ji4mKKi4urHufk5ABQWlpKaWlpHX+CuquMoSHE0tyo7a2jtreO2t46antrqf2to7a3jtr+1KxoG0sTsPT0dMrLy4mNja2xPzY2lq1btx73NSkpKcc9PiUl5bjHP/nkkzz++OPH7J8zZw4BAQG1jNz95s6da3UIzZba3jpqe+uo7a2jtreW2t86anvrqO1PrKCgwOPntDQB84QHH3ywRo9ZTk4OrVu3ZtSoUYSEhFgYmVFaWsrcuXO58MIL8fb2tjqcZkVtbx21vXXU9tZR21tL7W8dtb111PanVjk6zpMsTcCioqJwOBykpqbW2J+amkpcXNxxXxMXF3dGx/v6+uLr63vMfm9v7wZ1ITa0eJoTtb111PbWUdtbR21vLbW/ddT21lHbn5gV7WJpEQ4fHx/69+/P/Pnzq/Y5nU7mz5/P0KFDj/uaoUOH1jgeTLfqiY4XERERERFpKCwfgjhlyhQmTpzIgAEDGDRoEM8//zz5+flMmjQJgJtvvpmWLVvy5JNPAnDvvfdy7rnn8q9//YuLL76YTz75hJUrV/L6669b+WOIiIiIiIickuUJ2IQJEzh8+DCPPvooKSkp9OnTh9mzZ1cV2khKSsJur+6oGzZsGNOnT+fhhx/moYceomPHjsycOZMePXpY9SOIiIiIiIicFssTMIDJkyczefLk4z63YMGCY/ZdffXVXH311bU6l8vlAqyZcHc8paWlFBQUkJOTo7G5Hqa2t47a3jpqe+uo7a2l9reO2t46avtTq8wJKnMET2gQCZgn5ebmAtC6dWuLIxERERERkYYgNzeX0NBQj5zL5vJkutcAOJ1OkpOTCQ4OxmazWR1OVVn8/fv3N4iy+M2J2t46anvrqO2to7a3ltrfOmp766jtT83lcpGbm0uLFi1qTHuqT82uB8xut9OqVSurwzhGSEiIfjEsora3jtreOmp766jtraX2t47a3jpq+5PzVM9XJUvL0IuIiIiIiDQnSsBEREREREQ8RAmYxXx9fZk6dSq+vr5Wh9LsqO2to7a3jtreOmp7a6n9raO2t47avmFqdkU4RERERERErKIeMBEREREREQ9RAiYiIiIiIuIhSsBEREREREQ8RAmYiIiIiIiIhygBs9Arr7xCmzZt8PPzY/DgwSxfvtzqkJq8xx57DJvNVuPWpUsXq8Nqsn766SfGjRtHixYtsNlszJw5s8bzLpeLRx99lPj4ePz9/Rk5ciQ7duywJtgm5lRtf8sttxzzuzBmzBhrgm1innzySQYOHEhwcDAxMTGMHz+ebdu21TimqKiIu+++m8jISIKCgrjyyitJTU21KOKm43Ta/rzzzjvm2r/jjjssirjpmDZtGr169apa8Hfo0KF89913Vc/rmq8/p2p7XfMNjxIwi3z66adMmTKFqVOnsnr1anr37s3o0aNJS0uzOrQmr3v37hw6dKjq9ssvv1gdUpOVn59P7969eeWVV477/DPPPMOLL77Iq6++yrJlywgMDGT06NEUFRV5ONKm51RtDzBmzJgavwsff/yxByNsuhYuXMjdd9/N0qVLmTt3LqWlpYwaNYr8/PyqY/7whz/w9ddf8/nnn7Nw4UKSk5O54oorLIy6aTidtge47bbbalz7zzzzjEURNx2tWrXiqaeeYtWqVaxcuZILLriAyy67jE2bNgG65uvTqdoedM03OC6xxKBBg1x333131ePy8nJXixYtXE8++aSFUTV9U6dOdfXu3dvqMJolwDVjxoyqx06n0xUXF+f65z//WbUvKyvL5evr6/r4448tiLDp+nXbu1wu18SJE12XXXaZJfE0N2lpaS7AtXDhQpfLZa5zb29v1+eff151zJYtW1yAa8mSJVaF2ST9uu1dLpfr3HPPdd17773WBdWMhIeHu958801d8xaobHuXS9d8Q6QeMAuUlJSwatUqRo4cWbXPbrczcuRIlixZYmFkzcOOHTto0aIF7dq144YbbiApKcnqkJqlPXv2kJKSUuP3IDQ0lMGDB+v3wEMWLFhATEwMnTt35s477+TIkSNWh9QkZWdnAxAREQHAqlWrKC0trXHtd+nShYSEBF37bvbrtq/00UcfERUVRY8ePXjwwQcpKCiwIrwmq7y8nE8++YT8/HyGDh2qa96Dft32lXTNNyxeVgfQHKWnp1NeXk5sbGyN/bGxsWzdutWiqJqHwYMH8+6779K5c2cOHTrE448/ztlnn83GjRsJDg62OrxmJSUlBeC4vweVz0n9GTNmDFdccQVt27Zl165dPPTQQ4wdO5YlS5bgcDisDq/JcDqd3HfffQwfPpwePXoA5tr38fEhLCysxrG69t3reG0PcP3115OYmEiLFi1Yv349DzzwANu2beOrr76yMNqmYcOGDQwdOpSioiKCgoKYMWMG3bp1Y+3atbrm69mJ2h50zTdESsCkWRk7dmzVdq9evRg8eDCJiYl89tln3HrrrRZGJuJZ1157bdV2z5496dWrF+3bt2fBggWMGDHCwsialrvvvpuNGzdqrqkFTtT2t99+e9V2z549iY+PZ8SIEezatYv27dt7OswmpXPnzqxdu5bs7Gy++OILJk6cyMKFC60Oq1k4Udt369ZN13wDpCGIFoiKisLhcBxT/Sc1NZW4uDiLomqewsLC6NSpEzt37rQ6lGan8lrX70HD0K5dO6KiovS74EaTJ0/mm2++4ccff6RVq1ZV++Pi4igpKSErK6vG8br23edEbX88gwcPBtC17wY+Pj506NCB/v378+STT9K7d29eeOEFXfMecKK2Px5d89ZTAmYBHx8f+vfvz/z586v2OZ1O5s+fX2O8rtS/vLw8du3aRXx8vNWhNDtt27YlLi6uxu9BTk4Oy5Yt0++BBQ4cOMCRI0f0u+AGLpeLyZMnM2PGDH744Qfatm1b4/n+/fvj7e1d49rftm0bSUlJuvbr6FRtfzxr164F0LVfD5xOJ8XFxbrmLVDZ9seja956GoJokSlTpjBx4kQGDBjAoEGDeP7558nPz2fSpElWh9ak/elPf2LcuHEkJiaSnJzM1KlTcTgcXHfddVaH1iTl5eXV+IZtz549rF27loiICBISErjvvvv429/+RseOHWnbti2PPPIILVq0YPz48dYF3UScrO0jIiJ4/PHHufLKK4mLi2PXrl3cf//9dOjQgdGjR1sYddNw9913M336dP773/8SHBxcNcclNDQUf39/QkNDufXWW5kyZQoRERGEhIRwzz33MHToUIYMGWJx9I3bqdp+165dTJ8+nYsuuojIyEjWr1/PH/7wB8455xx69eplcfSN24MPPsjYsWNJSEggNzeX6dOns2DBAr7//ntd8/XsZG2va76BsroMY3P20ksvuRISElw+Pj6uQYMGuZYuXWp1SE3ehAkTXPHx8S4fHx9Xy5YtXRMmTHDt3LnT6rCarB9//NEFHHObOHGiy+UypegfeeQRV2xsrMvX19c1YsQI17Zt26wNuok4WdsXFBS4Ro0a5YqOjnZ5e3u7EhMTXbfddpsrJSXF6rCbhOO1O+B65513qo4pLCx03XXXXa7w8HBXQECA6/LLL3cdOnTIuqCbiFO1fVJSkuucc85xRUREuHx9fV0dOnRw/fnPf3ZlZ2dbG3gT8Jvf/MaVmJjo8vHxcUVHR7tGjBjhmjNnTtXzuubrz8naXtd8w2RzuVwuTyZ8IiIiIiIizZXmgImIiIiIiHiIEjAREREREREPUQImIiIiIiLiIUrAREREREREPEQJmIiIiIiIiIcoARMREREREfEQJWAiIiIiIiIeogRMRERERETEQ5SAiYiInAGbzcbMmTOtDkNERBopJWAiItJo3HLLLdhstmNuY8aMsTo0ERGR0+JldQAiIiJnYsyYMbzzzjs19vn6+loUjYiIyJlRD5iIiDQqvr6+xMXF1biFh4cDZnjgtGnTGDt2LP7+/rRr144vvviixus3bNjABRdcgL+/P5GRkdx+++3k5eXVOObtt9+me/fu+Pr6Eh8fz+TJk2s8n56ezuWXX05AQAAdO3bkf//7X/3+0CIi0mQoARMRkSblkUce4corr2TdunXccMMNXHvttWzZsgWA/Px8Ro8eTXh4OCtWrODzzz9n3rx5NRKsadOmcffdd3P77bezYcMG/ve//9GhQ4ca53j88ce55pprWL9+PRdddBE33HADGRkZHv05RUSkcbK5XC6X1UGIiIicjltuuYUPP/wQPz+/GvsfeughHnroIWw2G3fccQfTpk2rem7IkCH069eP//znP7zxxhs88MAD7N+/n8DAQABmzZrFuHHjSE5OJjY2lpYtWzJp0iT+9re/HTcGm83Gww8/zBNPPAGYpC4oKIjvvvtOc9FEROSUNAdMREQalfPPP79GggUQERFRtT106NAazw0dOpS1a9cCsGXLFnr37l2VfAEMHz4cp9PJtm3bsNlsJCcnM2LEiJPG0KtXr6rtwMBAQkJCSEtLq+2PJCIizYgSMBERaVQCAwOPGRLoLv7+/qd1nLe3d43HNpsNp9NZHyGJiEgTozlgIiLSpCxduvSYx127dgWga9eurFu3jvz8/KrnFy1ahN1up3PnzgQHB9OmTRvmz5/v0ZhFRKT5UA+YiIg0KsXFxaSkpNTY5+XlRVRUFACff/45AwYM4KyzzuKjjz5i+fLlvPXWWwDccMMNTJ06lYkTJ/LYY49x+PBh7rnnHm666SZiY2MBeOyxx7jjjjuIiYlh7Nix5ObmsmjRIu655x7P/qAiItIkKQETEZFGZfbs2cTHx9fY17lzZ7Zu3QqYCoWffPIJd911F/Hx8Xz88cd069YNgICAAL7//nvuvfdeBg4cSEBAAFdeeSXPPfdc1XtNnDiRoqIi/v3vf/OnP/2JqKgorrrqKs/9gCIi0qSpCqKIiDQZNpuNGTNmMH78eKtDEREROS7NARMREREREfEQJWAiIiIiIiIeojlgIiLSZGhUvYiINHTqARMREREREfEQJWAiIiIiIiIeogRMRERERETEQ5SAiYiIiIiIeIgSMBEREREREQ9RAiYiIiIiIuIhSsBEREREREQ8RAmYiIiIiIiIh/w/HC1f5V8Z3PgAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best checkpoint path: /home/adrian/AIRT_Segmentation_Project/experimenting/training_validation/id=13-patch_dims=[128x128x64]-overlap_training=[0_10x0_15]-overlap_inference=[0_00x0_00]/best-checkpoint-epoch=27-val_loss=0.19.ckpt\n",
      "Best epoch: 27.0\n",
      "All dimensions are divisible by 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_714218/3083895650.py:171: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
      "  colormap = plt.cm.get_cmap(\"viridis\", num_classes)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined inference plot saved at: training_validation/id=13-patch_dims=[128x128x64]-overlap_training=[0_10x0_15]-overlap_inference=[0_00x0_00]/results_best-checkpoint-epoch=27-val_loss=0.19.ckpt.png\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "####################################################################################\n",
    "#############################TRAINING###############################################\n",
    "####################################################################################\n",
    "# Measure start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Train the model\n",
    "model = SwinUNETR_AIRT_LightningModel(patch_dimensions=patch_dims)\n",
    "# Fit the model\n",
    "trainer.fit(model, train_loader, val_loader)\n",
    "\n",
    "# Measure end time\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate and print the elapsed time\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Training completed in {elapsed_time // 3600:.0f}h {elapsed_time % 3600 // 60:.0f}m {elapsed_time % 60:.0f}s\")\n",
    "\n",
    "# When you call trainer.fit(), by default, PyTorch Lightning performs a \"sanity check\" on the validation data\n",
    "# before the first training epoch starts. This sanity check typically involves running a few batches\n",
    "# (usually two by default) of the validation data to ensure that:\n",
    "#\n",
    "# 1. There are no issues with the validation loop or dataset.\n",
    "# 2. The model can perform a forward and backward pass without errors.\n",
    "####################################################################################\n",
    "#######################PLOTTING TRAINING LOSSES#####################################\n",
    "####################################################################################\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Base directory for the logs\n",
    "log_dir = execution_dir\n",
    "\n",
    "# Path to the latest metrics file\n",
    "metrics_file = os.path.join(log_dir, 'metrics.csv')\n",
    "\n",
    "# Load the logged metrics\n",
    "metrics_df = pd.read_csv(metrics_file)\n",
    "\n",
    "# Ensure metrics_df is loaded\n",
    "# Filter rows where train_loss_epoch and val_loss_epoch are not NaN\n",
    "train_loss = metrics_df['train_loss_epoch'].dropna()\n",
    "val_loss = metrics_df['val_loss_epoch'].dropna()\n",
    "\n",
    "# Use the 'epoch' column as x-axis\n",
    "epochs_train = metrics_df.loc[metrics_df['train_loss_epoch'].notna(), 'epoch']\n",
    "epochs_val = metrics_df.loc[metrics_df['val_loss_epoch'].notna(), 'epoch']\n",
    "\n",
    "# Plot Training and Validation Loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(epochs_train, train_loss, label=\"Training Loss\")\n",
    "plt.plot(epochs_val, val_loss, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(experiment_config_id + \"\\n Training and Validation Loss Over Epochs\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Save the plot to the folder\n",
    "save_path = os.path.join(log_dir, f\"loss-evol_{experiment_config_id}.png\")\n",
    "plt.savefig(save_path, dpi=300, bbox_inches='tight')  # Save as PNG with high resolution\n",
    "print(f\"Losses Plot saved at: {save_path}\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "########################################################################################################\n",
    "#############################PLOTTING BEST_CHECKPOINT RESULTS###########################################\n",
    "########################################################################################################\n",
    "\n",
    "\n",
    "# Path to the best checkpoint\n",
    "best_checkpoint_path = checkpoint_callback.best_model_path\n",
    "print(f\"Best checkpoint path: {best_checkpoint_path}\")\n",
    "\n",
    "# Extract the best epoch number from the checkpoint filename\n",
    "best_epoch = float(best_checkpoint_path.split(\"best-checkpoint-\")[1].split(\"epoch=\")[1].split(\"-\")[0]) # best-checkpoint-epoch=02-val_loss=0.85\n",
    "print(f\"Best epoch: {best_epoch}\")\n",
    "\n",
    "# Path to metrics.csv\n",
    "metrics_file = os.path.join(execution_dir, \"metrics.csv\")\n",
    "\n",
    "# Load metrics to extract val_loss_epoch, val_mean_iou_epoch, val_dice_epoch for the best epoch\n",
    "metrics_df = pd.read_csv(metrics_file)\n",
    "\n",
    "# Get metrics for the best epoch\n",
    "metrics_for_best_epoch = metrics_df.loc[metrics_df['epoch'] == best_epoch]\n",
    "val_loss_epoch = metrics_for_best_epoch.loc[metrics_for_best_epoch['val_loss_epoch'].notna(), 'val_loss_epoch'].values[0]\n",
    "val_mean_iou_epoch = metrics_for_best_epoch.loc[metrics_for_best_epoch['val_mean_iou_epoch'].notna(), 'val_mean_iou_epoch'].values[0]\n",
    "val_dice_epoch = metrics_for_best_epoch.loc[metrics_for_best_epoch['val_dice_epoch'].notna(), 'val_dice_epoch'].values[0]\n",
    "\n",
    "# Titles for the plots\n",
    "plot_title_template = (\n",
    "    f\"Epoch: {best_epoch:.1f}, \"\n",
    "    f\"val_loss_epoch: {val_loss_epoch:.4f}, \"\n",
    "    f\"val_mean_iou_epoch: {val_mean_iou_epoch:.4f}, \"\n",
    "    f\"val_dice_epoch: {val_dice_epoch:.4f}\"\n",
    ")\n",
    "\n",
    "# Load the best checkpoint\n",
    "best_model = SwinUNETR_AIRT_LightningModel.load_from_checkpoint(\n",
    "    checkpoint_path=best_checkpoint_path,\n",
    "    patch_dimensions=patch_dims  # Manually pass required arguments\n",
    ")\n",
    "best_model.eval()  # Set model to evaluation mode\n",
    "best_model.freeze()  # Freeze the model\n",
    "\n",
    "# Initialize lists to collect all results\n",
    "all_ground_truth = []\n",
    "all_predictions = []\n",
    "\n",
    "# Inference on validation set\n",
    "for batch_idx, batch in enumerate(val_loader):\n",
    "    x, y = batch  # x: list of dict_patches, y: list of label_paths\n",
    "\n",
    "    for sample_x, sample_y in zip(x, y):\n",
    "        # Load ground truth\n",
    "        y = torch.load(sample_y, weights_only=True).to(best_model.device)  # Move to the correct device\n",
    "        y = y.unsqueeze(0)  # Add 1 dimension to match model output format\n",
    "\n",
    "        # Create a tensor filled with NaN values\n",
    "        y_hat = torch.empty_like(y).fill_(float('nan')).to(best_model.device)\n",
    "        patches_dict_info = sample_x\n",
    "\n",
    "        for patch_key in list(patches_dict_info.keys()):\n",
    "            patch_coordinates = patches_dict_info[patch_key][\"patch_coord\"]\n",
    "            patch_path = patches_dict_info[patch_key][\"patch_path\"]\n",
    "            patch_tensor = torch.load(patch_path, weights_only=True).to(best_model.device)\n",
    "            patch_tensor = patch_tensor.unsqueeze(0)  # Match model input format\n",
    "            patch_prediction = best_model(patch_tensor)\n",
    "\n",
    "            # Fill y_hat with patch predictions\n",
    "            for i in range(patch_coordinates[0], patch_coordinates[0] + best_model.patch_dimensions[0]):\n",
    "                for j in range(patch_coordinates[1], patch_coordinates[1] + best_model.patch_dimensions[1]):\n",
    "                    is_pixel_without_prediction = torch.isnan(y_hat[:, :, i, j]).any()\n",
    "                    if is_pixel_without_prediction:\n",
    "                        y_hat[:, :, i, j] = patch_prediction[:, :, i - patch_coordinates[0], j - patch_coordinates[1]]\n",
    "\n",
    "        # Apply softmax to probabilities (POST-PROCESSING)\n",
    "        y_hat_probabilities = F.softmax(y_hat, dim=1)\n",
    "\n",
    "        # Classify each pixel accoring to probabilties assigning the class with higher probabilty\n",
    "        y_hat_categorized = torch.argmax(y_hat_probabilities.squeeze(), dim=0)\n",
    "        # Convert y one_encoded into categorized tensor\n",
    "        y = torch.argmax(y.squeeze(), dim=0)\n",
    "\n",
    "        # Append to the global list\n",
    "        all_ground_truth.append(y)\n",
    "        all_predictions.append(y_hat_categorized)\n",
    "\n",
    "# Find unique classes dynamically\n",
    "all_classes = set()\n",
    "for gt, pred in zip(all_ground_truth, all_predictions):\n",
    "    all_classes.update(torch.unique(gt).tolist())  # Add classes from ground truth\n",
    "    all_classes.update(torch.unique(pred).tolist())  # Add classes from predictions\n",
    "\n",
    "# Sort the classes to ensure order\n",
    "all_classes = sorted(all_classes)\n",
    "\n",
    "# Define class labels dynamically (for simplicity, use numeric labels for now)\n",
    "class_labels = {cls: f\"Class {cls}\" for cls in all_classes}\n",
    "num_classes = len(class_labels)\n",
    "\n",
    "# Create a discrete colormap with exactly `num_classes` colors\n",
    "colormap = plt.cm.get_cmap(\"viridis\", num_classes)\n",
    "\n",
    "# Create legend patches using discrete colors from the colormap\n",
    "legend_patches = [\n",
    "    mpatches.Patch(color=colormap(i), label=f\"{i}: {label}\")\n",
    "    for i, label in class_labels.items()\n",
    "]\n",
    "\n",
    "# Plot all results together\n",
    "num_samples = len(all_ground_truth)\n",
    "\n",
    "# Create the figure\n",
    "fig, axes = plt.subplots(num_samples, 2, figsize=(10, 5 * num_samples))\n",
    "\n",
    "# Ensure axes is always 2D\n",
    "if num_samples == 1:\n",
    "    axes = axes[None, :]  # Ensure axes is 2D when there's only one sample\n",
    "\n",
    "# Plot ground truth and predictions\n",
    "for sample_idx, (ground_truth, prediction) in enumerate(zip(all_ground_truth, all_predictions)):\n",
    "    # Ground truth\n",
    "    im_gt = axes[sample_idx, 0].imshow(ground_truth.cpu().numpy(), cmap=\"viridis\", interpolation=\"none\", vmin=min(all_classes), vmax=max(all_classes))\n",
    "    axes[sample_idx, 0].set_title(f\"Sample {sample_idx} - Ground Truth\")\n",
    "    axes[sample_idx, 0].axis(\"off\")\n",
    "\n",
    "    # Prediction\n",
    "    im_pred = axes[sample_idx, 1].imshow(prediction.cpu().numpy(), cmap=\"viridis\", interpolation=\"none\", vmin=min(all_classes), vmax=max(all_classes))\n",
    "    axes[sample_idx, 1].set_title(f\"Sample {sample_idx} - Prediction\")\n",
    "    axes[sample_idx, 1].axis(\"off\")\n",
    "\n",
    "# Add a single legend for the entire figure\n",
    "fig.legend(\n",
    "    handles=legend_patches,\n",
    "    loc=\"upper center\",\n",
    "    ncol=num_classes,\n",
    "    bbox_to_anchor=(0.5, 1.02),\n",
    "    fontsize=12\n",
    ")\n",
    "\n",
    "# Add a title for the entire figure\n",
    "fig.suptitle(\"RESULTS BEST CHECKPOINT\\n\" + plot_title_template, fontsize=16, y=1.15)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the combined plot\n",
    "combined_plot_path = os.path.join(execution_dir, f\"results_{best_checkpoint_path.split('/')[-1]}.png\")\n",
    "plt.savefig(combined_plot_path, dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "plt.close(fig)\n",
    "\n",
    "print(f\"RESULTS BEST CHECKPOINT plot saved at: {combined_plot_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "01a404fef4cb481ba0eb2affe9c9f0a3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "02996bb2eaac478fb1f8c39104591d9e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "04a768248f854759a430185e0ecc760f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "04d29d904f2e49759fe4590a893604de": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c8790884269a4ffdbe155525796bf676",
      "placeholder": "​",
      "style": "IPY_MODEL_762eb6299af34b079b2b7d1404f9d8ab",
      "value": " 2/2 [00:08&lt;00:00,  0.23it/s]"
     }
    },
    "07e78019d0fe490bacfa494fed3c0639": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_27b9a12b35814c6896695b933b42d203",
      "placeholder": "​",
      "style": "IPY_MODEL_57354716ebe845398c024c78dcc23412",
      "value": " 2/2 [00:09&lt;00:00,  0.20it/s]"
     }
    },
    "09b1ad60c74f46a2b6bb7a55cc9076a6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ed210ac45b50410b8406a03920ca70aa",
      "max": 32,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c74a4aff322d4c1baaea6319c68fc013",
      "value": 32
     }
    },
    "0b80357856dd497b977f1c62b4dd54d5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": "2",
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0ba419f910fd451e8d20d7ee21ac229a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bba67b7b4b0c409686babdc2cb7455bd",
      "placeholder": "​",
      "style": "IPY_MODEL_a678d62c00414bb783d8663d5d8b8a9a",
      "value": " 2/2 [00:10&lt;00:00,  0.20it/s]"
     }
    },
    "0d1043b563a2421e93da1b9dde15b83e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "0fd8a83a43d441e182dd14fd46942786": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "13296b0129ed4d04aa301f2b07662d4a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "147a9512d7e14fc1bb5df198c8645072": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": "2",
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "14e7eba41ad54db3b9e81442a0f3fcb5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "156907f41ad54247b0faeebaf54db22d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "16c67939d1b3438d9e4a577ed12e2ba1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "19269604ec5f47a39c76a3442b98f36f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1944dda3e99a4b73bab485d1ebcece46": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1dad2ea4a59a44158e4232c43205d677": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e3e6e22c70de42b1be6d703f07f57ed9",
      "placeholder": "​",
      "style": "IPY_MODEL_19269604ec5f47a39c76a3442b98f36f",
      "value": " 2/2 [00:09&lt;00:00,  0.22it/s]"
     }
    },
    "1f891dfa066e4eb08a7730dd770611f6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "inline-flex",
      "flex": null,
      "flex_flow": "row wrap",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": "hidden",
      "width": "100%"
     }
    },
    "238467bc46154011b1ae98d030ac9fd4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_aba2b3234543468192b95a151139570f",
      "placeholder": "​",
      "style": "IPY_MODEL_cc4a23eaf9d3404f8aebc62549a21851",
      "value": "Epoch 9: 100%"
     }
    },
    "23cafe3cfb504ab78b65feb113802c75": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": "2",
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "25c749615a744af69d020c21077b063f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2623d6ae12a144c5bbf067210b00ea22": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_aa3065868cc24baeabfe417f6521c35e",
       "IPY_MODEL_77c4c406303d4574becdd7e61e58094c",
       "IPY_MODEL_8d14f4ab98d14e2fad1274fe5c772c69"
      ],
      "layout": "IPY_MODEL_6631195574944c0184cbeeb57d09dc62"
     }
    },
    "2631c25082444c3baeebddf48da847fc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a883fddfa800437789bc05acafbe921e",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c803d63da62f496ab6f8ccf8dfb313c0",
      "value": 2
     }
    },
    "269c42edb5e24fada8cbfbbff3d6a766": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4ee3a8a753cb4c28a2f04e14fec40b4c",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_df570219617f436d8d0432bf23047a5a",
      "value": 2
     }
    },
    "27b9a12b35814c6896695b933b42d203": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2c715b96110849c482142eec37bdf4ef": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "32afa45db0c443c288be348601cace6d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0b80357856dd497b977f1c62b4dd54d5",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_13296b0129ed4d04aa301f2b07662d4a",
      "value": 2
     }
    },
    "359daf43863c4a33bd4569de5210f46d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_790968fc3c26487dada3256743598c46",
      "placeholder": "​",
      "style": "IPY_MODEL_d2e5b4dbfedd4cfd9a7faa638d159a69",
      "value": " 2/2 [00:09&lt;00:00,  0.20it/s]"
     }
    },
    "361ba0ee48264d1d932edc7b0f95fd88": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3af8446178204d4a8ce6d014e8d0d646": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_4219267821184a5088c0c88a34d2f4c9",
       "IPY_MODEL_76e033fd323048699969847140de85a4",
       "IPY_MODEL_f3001fa8500d4a22bce7661eb0ae204a"
      ],
      "layout": "IPY_MODEL_1f891dfa066e4eb08a7730dd770611f6"
     }
    },
    "3ce6ffb88d8549bbb053ff4146beb070": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3fb12ce622d44f518cddc8cfabcf1877": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4219267821184a5088c0c88a34d2f4c9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9eb5072d61774bd18d894ecf77e81e12",
      "placeholder": "​",
      "style": "IPY_MODEL_c2fdd7e266254984bb2e2b54ef9d7f2d",
      "value": "Validation DataLoader 0: 100%"
     }
    },
    "45e456e35e98476cbb0c05b6438f987c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": "2",
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "462e6c8fc880423592439e448a60895c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "48f9dd734e1747d1836b789279d7c289": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ae47ae991a6b4371b3ec9aa489d2ffcc",
       "IPY_MODEL_32afa45db0c443c288be348601cace6d",
       "IPY_MODEL_359daf43863c4a33bd4569de5210f46d"
      ],
      "layout": "IPY_MODEL_a4f0addb2ce84e29bc6db32fceee5968"
     }
    },
    "49ae2980e69142e394b308d863f4060a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_60a46c23903e46c49e75afe50c9a6c04",
      "placeholder": "​",
      "style": "IPY_MODEL_b0ef657450784488aa2b046bc5ee577e",
      "value": "Validation DataLoader 0: 100%"
     }
    },
    "4c4468dccb4e4857a4f59b352f6e2d65": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4e1afd85300c4533a5ac8c929f63a4c6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fc916d29fc52475c953a3e22781ff313",
      "placeholder": "​",
      "style": "IPY_MODEL_1944dda3e99a4b73bab485d1ebcece46",
      "value": "Validation DataLoader 0: 100%"
     }
    },
    "4ee3a8a753cb4c28a2f04e14fec40b4c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": "2",
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5516845c13664350a6c3ef8e6e90d0a3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "57354716ebe845398c024c78dcc23412": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "57d1b45a22454941ad98dfc2565bd6ba": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "5b229f4741d44a61829647791c4a53cc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_efc5ae14e3894efb851b894a6d8bcd77",
       "IPY_MODEL_adc73e09733a4821a9f6699b1832d7db",
       "IPY_MODEL_fcdddf668b284457ae120ad2d26effda"
      ],
      "layout": "IPY_MODEL_f79040d7c6db49f48963aa24c40a4d8a"
     }
    },
    "5edabcfcaf1c4ef7a8736160884b5502": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": "2",
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5f2899d0eae84a26819267d8381ab98b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "inline-flex",
      "flex": null,
      "flex_flow": "row wrap",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": "hidden",
      "width": "100%"
     }
    },
    "5f8af3e19b414671b202568469256021": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "inline-flex",
      "flex": null,
      "flex_flow": "row wrap",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": "hidden",
      "width": "100%"
     }
    },
    "60a46c23903e46c49e75afe50c9a6c04": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "623aefc5dd734c92b6ac65635fe8f6a7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6584f713501f49cd9e47d028abd8eb49": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ea11c07a8df54393b5fedd153ea790ce",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_57d1b45a22454941ad98dfc2565bd6ba",
      "value": 2
     }
    },
    "6631195574944c0184cbeeb57d09dc62": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "inline-flex",
      "flex": null,
      "flex_flow": "row wrap",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": "hidden",
      "width": "100%"
     }
    },
    "6b7bd8095d2c4ad18c659bea3867c9f9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": "2",
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6c6265ad504a4dd6b29b5b44e9c8e88e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6dc74a03a32243faaedc32ce99ae1186": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6e1a9bdb343f49fb9eb696f75e35551e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a3d67c0e328a40ed9a30df9185cbffbb",
      "placeholder": "​",
      "style": "IPY_MODEL_16c67939d1b3438d9e4a577ed12e2ba1",
      "value": "Validation DataLoader 0: 100%"
     }
    },
    "6eb5de561af54373821567026e5cdf67": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_238467bc46154011b1ae98d030ac9fd4",
       "IPY_MODEL_09b1ad60c74f46a2b6bb7a55cc9076a6",
       "IPY_MODEL_fe0a0addb86649f280e95cd3455bbdd1"
      ],
      "layout": "IPY_MODEL_c39ca81b222d4bd8b562dbc6b3472d47"
     }
    },
    "6f21451a4fb045b4b1643da6f1a482db": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": "2",
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6fd65b20690a4571a0f40daeca6553ea": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8dd441669acd480aa67478ce06c32a54",
       "IPY_MODEL_f6af8c83bd474f8a8b89143f1fd9062d",
       "IPY_MODEL_1dad2ea4a59a44158e4232c43205d677"
      ],
      "layout": "IPY_MODEL_70d1f021f6094454841277b5d31e53f3"
     }
    },
    "70d1f021f6094454841277b5d31e53f3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "inline-flex",
      "flex": null,
      "flex_flow": "row wrap",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": "hidden",
      "width": "100%"
     }
    },
    "73da76a42f87466bb5ad5720acdb594c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "762eb6299af34b079b2b7d1404f9d8ab": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "76e033fd323048699969847140de85a4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5edabcfcaf1c4ef7a8736160884b5502",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_9175ad14ddad400b87d1805a79d9f993",
      "value": 2
     }
    },
    "77c4c406303d4574becdd7e61e58094c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_45e456e35e98476cbb0c05b6438f987c",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b457bb71f5d34ab9af0bdf88a52b8d4f",
      "value": 2
     }
    },
    "790968fc3c26487dada3256743598c46": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7958baf0d57f4a299a532d6d1903848c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_49ae2980e69142e394b308d863f4060a",
       "IPY_MODEL_2631c25082444c3baeebddf48da847fc",
       "IPY_MODEL_04d29d904f2e49759fe4590a893604de"
      ],
      "layout": "IPY_MODEL_e27eca274bb34cc7a065e00530bed0cb"
     }
    },
    "7a371b13ada04a2184458c7f5a1b94ef": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0fd8a83a43d441e182dd14fd46942786",
      "placeholder": "​",
      "style": "IPY_MODEL_885d1a21842f4addb048d45a12840cf9",
      "value": "Validation DataLoader 0: 100%"
     }
    },
    "7bceda8bc2104bf796f28f29e2effcfb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7cf0d78d353e4736b9c26555b4fd5c8d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "7d8588f03d4d4dce82ac7f9dc2408cd6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "inline-flex",
      "flex": null,
      "flex_flow": "row wrap",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": "hidden",
      "width": "100%"
     }
    },
    "7f983c2eb21744a5b8ef53576a2a988b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": "2",
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "885d1a21842f4addb048d45a12840cf9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8d14f4ab98d14e2fad1274fe5c772c69": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_afc5b6291f8a43299cf95408d85301a0",
      "placeholder": "​",
      "style": "IPY_MODEL_3fb12ce622d44f518cddc8cfabcf1877",
      "value": " 2/2 [00:09&lt;00:00,  0.20it/s]"
     }
    },
    "8dd441669acd480aa67478ce06c32a54": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9b3c8c7bad644443a272536a4dd484ec",
      "placeholder": "​",
      "style": "IPY_MODEL_73da76a42f87466bb5ad5720acdb594c",
      "value": "Validation DataLoader 0: 100%"
     }
    },
    "8e1507e834a34d2f95e4d8d237b9d70b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7f983c2eb21744a5b8ef53576a2a988b",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_7cf0d78d353e4736b9c26555b4fd5c8d",
      "value": 2
     }
    },
    "9175ad14ddad400b87d1805a79d9f993": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "9a43948433da47ad8193944880f8581b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6b7bd8095d2c4ad18c659bea3867c9f9",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e5e8f803089547f6aac87be176b6b586",
      "value": 2
     }
    },
    "9ae0bd932f4840f2a11132046dfed2ec": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cf09714929914e15b8ca92ccd79fa9f9",
      "placeholder": "​",
      "style": "IPY_MODEL_ba0c9cb9084940d681b93b4ccf9370a7",
      "value": "Validation DataLoader 0: 100%"
     }
    },
    "9b3c8c7bad644443a272536a4dd484ec": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9d801c19d3a644e2ac1a6d042ece3a37": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_6e1a9bdb343f49fb9eb696f75e35551e",
       "IPY_MODEL_269c42edb5e24fada8cbfbbff3d6a766",
       "IPY_MODEL_e0f001637c7e471586388621b5c63bb7"
      ],
      "layout": "IPY_MODEL_5f8af3e19b414671b202568469256021"
     }
    },
    "9eb5072d61774bd18d894ecf77e81e12": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a21af7833aa74b9daef348fa40e2a769": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a3d67c0e328a40ed9a30df9185cbffbb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a4f0addb2ce84e29bc6db32fceee5968": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "inline-flex",
      "flex": null,
      "flex_flow": "row wrap",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": "hidden",
      "width": "100%"
     }
    },
    "a678d62c00414bb783d8663d5d8b8a9a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a883fddfa800437789bc05acafbe921e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": "2",
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "aa3065868cc24baeabfe417f6521c35e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_462e6c8fc880423592439e448a60895c",
      "placeholder": "​",
      "style": "IPY_MODEL_2c715b96110849c482142eec37bdf4ef",
      "value": "Validation DataLoader 0: 100%"
     }
    },
    "aba2b3234543468192b95a151139570f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "aca1da82bdfb4ed39c2a5e19439b9cf4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "inline-flex",
      "flex": null,
      "flex_flow": "row wrap",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": "hidden",
      "width": "100%"
     }
    },
    "acee2b3e64e04397b19a5980ee9693f4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "inline-flex",
      "flex": null,
      "flex_flow": "row wrap",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": "hidden",
      "width": "100%"
     }
    },
    "adc73e09733a4821a9f6699b1832d7db": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_23cafe3cfb504ab78b65feb113802c75",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_0d1043b563a2421e93da1b9dde15b83e",
      "value": 2
     }
    },
    "addfe20913dc4f86a6fd9226e5328f40": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_7a371b13ada04a2184458c7f5a1b94ef",
       "IPY_MODEL_8e1507e834a34d2f95e4d8d237b9d70b",
       "IPY_MODEL_e4852d0c53db482d91f0b3573e933ef6"
      ],
      "layout": "IPY_MODEL_7d8588f03d4d4dce82ac7f9dc2408cd6"
     }
    },
    "ae47ae991a6b4371b3ec9aa489d2ffcc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e5b81fab1daa433da14aeb73714dc66b",
      "placeholder": "​",
      "style": "IPY_MODEL_623aefc5dd734c92b6ac65635fe8f6a7",
      "value": "Validation DataLoader 0: 100%"
     }
    },
    "afc5b6291f8a43299cf95408d85301a0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "afe77aa595c74197b1ffb77c69d5ae44": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_4e1afd85300c4533a5ac8c929f63a4c6",
       "IPY_MODEL_6584f713501f49cd9e47d028abd8eb49",
       "IPY_MODEL_07e78019d0fe490bacfa494fed3c0639"
      ],
      "layout": "IPY_MODEL_5f2899d0eae84a26819267d8381ab98b"
     }
    },
    "b0ef657450784488aa2b046bc5ee577e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b457bb71f5d34ab9af0bdf88a52b8d4f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "ba0c9cb9084940d681b93b4ccf9370a7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "bba67b7b4b0c409686babdc2cb7455bd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c2fdd7e266254984bb2e2b54ef9d7f2d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c39ca81b222d4bd8b562dbc6b3472d47": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "inline-flex",
      "flex": null,
      "flex_flow": "row wrap",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "100%"
     }
    },
    "c3f90ce9999f478fb032d933f0910376": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_147a9512d7e14fc1bb5df198c8645072",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_04a768248f854759a430185e0ecc760f",
      "value": 2
     }
    },
    "c74a4aff322d4c1baaea6319c68fc013": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "c803d63da62f496ab6f8ccf8dfb313c0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "c8790884269a4ffdbe155525796bf676": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cb4d7afcc95643369cd3095d204080aa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "cc4a23eaf9d3404f8aebc62549a21851": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cf09714929914e15b8ca92ccd79fa9f9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d2e5b4dbfedd4cfd9a7faa638d159a69": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "df390551f9ba405e98c7afb5b337da0c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_25c749615a744af69d020c21077b063f",
      "placeholder": "​",
      "style": "IPY_MODEL_156907f41ad54247b0faeebaf54db22d",
      "value": " 2/2 [00:09&lt;00:00,  0.21it/s]"
     }
    },
    "df570219617f436d8d0432bf23047a5a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "e0f001637c7e471586388621b5c63bb7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3ce6ffb88d8549bbb053ff4146beb070",
      "placeholder": "​",
      "style": "IPY_MODEL_7bceda8bc2104bf796f28f29e2effcfb",
      "value": " 2/2 [00:08&lt;00:00,  0.22it/s]"
     }
    },
    "e27eca274bb34cc7a065e00530bed0cb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "inline-flex",
      "flex": null,
      "flex_flow": "row wrap",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": "hidden",
      "width": "100%"
     }
    },
    "e3e6e22c70de42b1be6d703f07f57ed9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e4852d0c53db482d91f0b3573e933ef6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a21af7833aa74b9daef348fa40e2a769",
      "placeholder": "​",
      "style": "IPY_MODEL_361ba0ee48264d1d932edc7b0f95fd88",
      "value": " 2/2 [00:09&lt;00:00,  0.21it/s]"
     }
    },
    "e5b81fab1daa433da14aeb73714dc66b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e5e8f803089547f6aac87be176b6b586": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "e5f0aedfb973415a89b8efeae1cea88a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e7c3dca7ea534bd19894c0a7d350189c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6dc74a03a32243faaedc32ce99ae1186",
      "placeholder": "​",
      "style": "IPY_MODEL_14e7eba41ad54db3b9e81442a0f3fcb5",
      "value": "Sanity Checking DataLoader 0: 100%"
     }
    },
    "e99501e206864fa6834e3285c7ee0b40": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e7c3dca7ea534bd19894c0a7d350189c",
       "IPY_MODEL_c3f90ce9999f478fb032d933f0910376",
       "IPY_MODEL_df390551f9ba405e98c7afb5b337da0c"
      ],
      "layout": "IPY_MODEL_aca1da82bdfb4ed39c2a5e19439b9cf4"
     }
    },
    "ea11c07a8df54393b5fedd153ea790ce": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": "2",
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "eae96f67f37a4ebabb31f19173e3152b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9ae0bd932f4840f2a11132046dfed2ec",
       "IPY_MODEL_9a43948433da47ad8193944880f8581b",
       "IPY_MODEL_0ba419f910fd451e8d20d7ee21ac229a"
      ],
      "layout": "IPY_MODEL_acee2b3e64e04397b19a5980ee9693f4"
     }
    },
    "ed210ac45b50410b8406a03920ca70aa": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": "2",
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "efc5ae14e3894efb851b894a6d8bcd77": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6c6265ad504a4dd6b29b5b44e9c8e88e",
      "placeholder": "​",
      "style": "IPY_MODEL_e5f0aedfb973415a89b8efeae1cea88a",
      "value": "Validation DataLoader 0: 100%"
     }
    },
    "f3001fa8500d4a22bce7661eb0ae204a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fbc2f79c24ce45e29f64221d00481a83",
      "placeholder": "​",
      "style": "IPY_MODEL_5516845c13664350a6c3ef8e6e90d0a3",
      "value": " 2/2 [00:09&lt;00:00,  0.21it/s]"
     }
    },
    "f6af8c83bd474f8a8b89143f1fd9062d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6f21451a4fb045b4b1643da6f1a482db",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_cb4d7afcc95643369cd3095d204080aa",
      "value": 2
     }
    },
    "f79040d7c6db49f48963aa24c40a4d8a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "inline-flex",
      "flex": null,
      "flex_flow": "row wrap",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": "hidden",
      "width": "100%"
     }
    },
    "fa07b61c9040475aaefe4a3a2eb9797a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fbc2f79c24ce45e29f64221d00481a83": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fc916d29fc52475c953a3e22781ff313": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fcdddf668b284457ae120ad2d26effda": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fa07b61c9040475aaefe4a3a2eb9797a",
      "placeholder": "​",
      "style": "IPY_MODEL_01a404fef4cb481ba0eb2affe9c9f0a3",
      "value": " 2/2 [00:10&lt;00:00,  0.20it/s]"
     }
    },
    "fe0a0addb86649f280e95cd3455bbdd1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4c4468dccb4e4857a4f59b352f6e2d65",
      "placeholder": "​",
      "style": "IPY_MODEL_02996bb2eaac478fb1f8c39104591d9e",
      "value": " 32/32 [00:26&lt;00:00,  1.22it/s, v_num=8, train_loss_step=0.182, val_loss_step=0.453, val_loss_epoch=0.467, avg_val_loss=0.467, train_loss_epoch=0.359]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
